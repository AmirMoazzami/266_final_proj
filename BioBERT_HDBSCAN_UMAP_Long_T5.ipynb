{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdce99a",
   "metadata": {},
   "source": [
    "***Extractive step: BioBERT, HDBSCAN***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb8496",
   "metadata": {},
   "source": [
    "major changes: BioBERT for embedding. Using a different sentence segmentation like spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60386300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import umap\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Check if MPS is available and set the device accordingly\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# Load the dataset and cut down \n",
    "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\n",
    "# Use select to create a subset\n",
    "# dataset = dataset.select(range(20,30))  \n",
    "\n",
    "\n",
    "# Initialize the BioBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1').to(device)\n",
    "\n",
    "def bert_sentence_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # Move inputs to the device\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Move outputs back to CPU\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def reduce_dimensions(embeddings, n_components=2):\n",
    "    \"\"\"\n",
    "    Reduces the dimensions of the given embeddings using UMAP.\n",
    "\n",
    "    :param embeddings: High-dimensional data to be reduced.\n",
    "    :param n_components: The dimension of the space to embed into. Default is 2.\n",
    "    :return: The reduced dimension embeddings.\n",
    "    \"\"\"\n",
    "    # Ensure n_neighbors is at least 2 and less than the number of embeddings\n",
    "    n_neighbors = max(2, min(embeddings.shape[0] - 1, 15))\n",
    "\n",
    "    umap_reducer = umap.UMAP(n_neighbors=n_neighbors, n_components=n_components)\n",
    "    return umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "\n",
    "# def select_top_sentences(sentences, embeddings, n_sentences=5):\n",
    "#     # If there are fewer sentences than the desired number, return them all\n",
    "#     if len(sentences) < n_sentences:\n",
    "#         return ' '.join(sentences)\n",
    "\n",
    "#     # Set the minimum cluster size for HDBSCAN\n",
    "#     # Ensure it is not larger than the number of sentences and at least 2\n",
    "#     cluster_size = max(4, min(len(sentences), len(sentences) // n_sentences))\n",
    "\n",
    "#     # Initialize HDBSCAN with the determined minimum cluster size\n",
    "#     clusterer = hdbscan.HDBSCAN(min_cluster_size=cluster_size, gen_min_span_tree=True)\n",
    "\n",
    "#     # Fit the HDBSCAN clusterer to the embeddings\n",
    "#     cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "#     # Handle cases where meaningful clusters are not found\n",
    "#     if len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0) < 2:\n",
    "#         # If less than two clusters (excluding noise), return the first n_sentences\n",
    "#         return ' '.join(sentences[:n_sentences])\n",
    "\n",
    "#     # Calculate the centroids of the clusters\n",
    "#     unique_labels = set(cluster_labels)\n",
    "#     unique_labels.discard(-1)  # Remove the noise label, if present\n",
    "#     centroids = [embeddings[cluster_labels == label].mean(axis=0) for label in unique_labels]\n",
    "\n",
    "#     # Find the closest sentence to each centroid\n",
    "#     top_sentence_indices = []\n",
    "#     for centroid in centroids:\n",
    "#         distances = np.linalg.norm(embeddings - centroid, axis=1)\n",
    "#         top_sentence_indices.append(np.argmin(distances))\n",
    "\n",
    "#     # Remove duplicate indices while preserving order\n",
    "#     top_sentence_indices = list(dict.fromkeys(top_sentence_indices))\n",
    "\n",
    "#     # Select the sentences corresponding to the top indices\n",
    "#     top_sentences = [sentences[index] for index in top_sentence_indices]\n",
    "\n",
    "#     # Return the top sentences joined into a single string\n",
    "#     return ' '.join(top_sentences)\n",
    "\n",
    "def select_top_sentences(sentences, embeddings, n_sentences=5):\n",
    "    # Return all sentences if there are fewer than the desired number\n",
    "    if len(sentences) <= n_sentences:\n",
    "        return ' '.join(sentences)\n",
    "\n",
    "    # Initialize HDBSCAN with a dynamic minimum cluster size\n",
    "    cluster_size = max(4, min(len(sentences), len(sentences) // n_sentences))\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=cluster_size, gen_min_span_tree=True)\n",
    "\n",
    "    # Fit the HDBSCAN clusterer to the embeddings\n",
    "    cluster_labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "    # Check if meaningful clusters are found\n",
    "    if len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0) < 2:\n",
    "        # If less than two clusters, return the first n_sentences\n",
    "        return ' '.join(sentences[:n_sentences])\n",
    "\n",
    "    # Calculate the centroids of the clusters\n",
    "    unique_labels = set(cluster_labels)\n",
    "    unique_labels.discard(-1)  # Remove the noise label\n",
    "    centroids = [embeddings[cluster_labels == label].mean(axis=0) for label in unique_labels]\n",
    "\n",
    "    # Initialize a set to store indices of selected sentences\n",
    "    selected_indices = set()\n",
    "    top_sentences = []\n",
    "\n",
    "    # Select the closest sentence to each centroid without repeating sentences\n",
    "    for centroid in centroids:\n",
    "        distances = np.linalg.norm(embeddings - centroid, axis=1)\n",
    "        sorted_indices = np.argsort(distances)\n",
    "\n",
    "        for index in sorted_indices:\n",
    "            if index not in selected_indices:\n",
    "                selected_indices.add(index)\n",
    "                top_sentences.append(sentences[index])\n",
    "                break\n",
    "\n",
    "    # Return the top sentences joined into a single string\n",
    "    return ' '.join(top_sentences)\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    review_id = row['review_id']\n",
    "    abstract_list = row['abstract'] \n",
    "\n",
    "    combined_summary = ''\n",
    "\n",
    "    for abstract in abstract_list:\n",
    "        abstract_text = ' '.join(abstract) if isinstance(abstract, list) else abstract\n",
    "\n",
    "        # Use SpaCy for sentence segmentation\n",
    "        doc = nlp(abstract_text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "        embeddings = bert_sentence_embeddings(sentences)\n",
    "        summary = select_top_sentences(sentences, embeddings)\n",
    "        combined_summary += summary + ' '\n",
    "\n",
    "    return {\"review_id\": review_id, \"summary\": combined_summary.strip()}\n",
    "\n",
    "# Apply the function to each element of the dataset\n",
    "summaries_dataset = dataset.map(process_row)\n",
    "\n",
    "\n",
    "# # Saving the dataset\n",
    "# import pickle\n",
    "# with open('summaries_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(summaries_dataset, file)\n",
    "\n",
    "# # Convert to pandas DataFrame\n",
    "# df = pd.DataFrame(summaries_dataset)\n",
    "# df = df[['review_id', 'summary']]\n",
    "# csv_file_path = 'test.csv'  # Update with your desired file path\n",
    "# df.to_csv(csv_file_path, index=True)\n",
    "\n",
    "# print(f\"Saved summaries to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c8017",
   "metadata": {},
   "source": [
    "***abstractive step: Long T5***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Check if MPS is available and set the device accordingly\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "    \n",
    "    \n",
    "# Load LongT5 Model and Tokenizer\n",
    "model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fine-tuned for summarization\n",
    "longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Initialize the results list\n",
    "results = []\n",
    "\n",
    "# Convert the dataset to a list of dictionaries if not already\n",
    "data_list = list(summaries_dataset)\n",
    "\n",
    "# Generate summaries in batches\n",
    "for i in range(0, len(data_list), batch_size):\n",
    "    batch = data_list[i:i + batch_size]\n",
    "    input_texts = [row['summary'] for row in batch]\n",
    "    review_ids_batch = [row['review_id'] for row in batch]\n",
    "\n",
    "    inputs = longt5_tokenizer(\n",
    "        input_texts,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=16384\n",
    "    ).to(device)\n",
    "\n",
    "    try:\n",
    "        summary_ids = longt5_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            num_beams=2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            min_length=10,\n",
    "            max_length=512,\n",
    "            early_stopping=True\n",
    "        ).to('cpu')\n",
    "        \n",
    "        batch_summaries = longt5_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Append each summary with its ReviewID to the results list\n",
    "        for review_id, summary in zip(review_ids_batch, batch_summaries):\n",
    "            results.append({'review_id': review_id, 'Summary': summary})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch starting at index {i}: {e}\")\n",
    "        for review_id in review_ids_batch:\n",
    "            results.append({'review_id': review_id, 'Summary': \"\"})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "summaries_df_val = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file_val = 'Summary_BioBERT_HDBSCAN_UMAP_Long_T5_prediction.csv'\n",
    "summaries_df_val.to_csv(output_file_val, index=True)\n",
    "print(f\"Saved summaries to {output_file_val}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
