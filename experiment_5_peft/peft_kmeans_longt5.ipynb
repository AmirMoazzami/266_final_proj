{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# PEFT on LongT5\n",
        "\n",
        "Extractive-Abstractive Model.\n",
        "- Extractive: KMeans clustering per study with 5 clusters, then choosing the sentence closest to each cluster's centroid (no overlapping).\n",
        "- Abstractive: LongT5 model originally trained for summarization (BookSum). Applied parameter-efficient fine-tuning (LoRA: Low Rank Adaptation) with r = 16.\n",
        "\n",
        "Note: this notebook contains code for using bitsandbytes, a library for 4-bit/8-bit quantization for memory-efficient training. This was ultimately NOT used in the final model, as we learned that bitsandbytes was not working well with LongT5 (and we simply don't have time to debug!)\n",
        "\n",
        "Inspiration code for LoRA and QLoRA (LoRA + bit quantization):\n",
        "- https://www.philschmid.de/fine-tune-flan-t5-peft\n",
        "- https://blog.lancedb.com/optimizing-llms-a-step-by-step-guide-to-fine-tuning-with-peft-and-qlora-22eddd13d25b\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/AmirMoazzami/266_final_proj/blob/mk%2Fkmeans-extraction-conclusion-only/experiment_5_peft/peft_kmeans_longt5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8DvMnJS6T9E",
        "outputId": "b9cc281d-76d8-4efa-81b2-f124dfeaf5cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTTKsqKPlicw",
        "outputId": "18477266-38fe-43b1-f41f-1dbda965f762"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Dec  4 16:45:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    44W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iTzCAkM5_1R",
        "outputId": "a13da915-7fe3-42d1-acfd-c2ec653e253c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade git+https://github.com/huggingface/transformers\n",
        "!pip install -q --upgrade git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes accelerate datasets tensorboardX loralib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNk98aFo5_1T",
        "outputId": "044dd4a1-03bf-4533-9039-daadc47e0481"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA device.\n"
          ]
        }
      ],
      "source": [
        "# Purpose of notebook: fine-tune LongT5 on exctracted sentences from studies, but using LoRA and bitsandbytes quantization\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from pprint import pprint\n",
        "import gc\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    LongT5ForConditionalGeneration,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS device.\")\n",
        "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = \"0.0\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA device.\")\n",
        "    max_split_size_mb = 256  # Set the max_split_size_mb value (e.g., 512 MB)\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"max_split_size_mb:{max_split_size_mb}\"\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"MPS/CUDA not available. Using CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq9cgF-4AobS",
        "outputId": "20f55ed1-7f2e-4ffc-8b97-5c2670a3bbad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([11350, 8192])\n",
            "torch.Size([2838, 8192])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load tokenizer and model\n",
        "model_id = 'pszemraj/long-t5-tglobal-base-16384-book-summary'\n",
        "# output_dir = \"training_history\"\n",
        "output_dir = \"/content/drive/MyDrive/266 final project/notebooks/peft_training_history\"  # Colab\n",
        "\n",
        "# extracted_file_path = '../experiment_1/biobert_extractive_only_training_dataset.csv.gz'\n",
        "extracted_file_path = '/content/drive/MyDrive/266 final project/notebooks/biobert_extractive_only_training_dataset.csv.gz'  # Colab\n",
        "\n",
        "# source_data_path = \"data\"\n",
        "source_data_path = \"/content/drive/MyDrive/266 final project/notebooks/peft_training_history/data\"  # Colab\n",
        "# source_data_path = \"/content/drive/MyDrive/266 final project/notebooks/peft_training_history/data_1024\"  # Colab\n",
        "\n",
        "# longT5 max token length is 16384, let's 1/2 that\n",
        "max_input_token_length = 8192\n",
        "# max_input_token_length = 1024\n",
        "\n",
        "# -------- END CONFIG ----------\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# label_pad_token_id = tokenizer.pad_token_id\n",
        "label_pad_token_id = -100  # special label token that gets ignored in loss calculations\n",
        "\n",
        "train_data_path = os.path.join(source_data_path, 'train_tokenized_dataset')\n",
        "val_data_path = os.path.join(source_data_path, 'val_tokenized_dataset')\n",
        "\n",
        "if os.path.exists(train_data_path) and os.path.exists(val_data_path):\n",
        "    train_dataset = Dataset.load_from_disk(train_data_path)\n",
        "    val_dataset = Dataset.load_from_disk(val_data_path)\n",
        "\n",
        "else:\n",
        "    ms2_dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"train\")\n",
        "\n",
        "    # Load your CSV file\n",
        "    df = pd.read_csv(extracted_file_path, compression='gzip')\n",
        "\n",
        "    # # ---- if full extracted data is not available yet:\n",
        "    # all_extracted_summaries = []\n",
        "    # for fpath in os.listdir('../experiment_1/biobert_extractive_only_training_dataset'):\n",
        "    #     all_extracted_summaries.append(\n",
        "    #         pickle.load(open(os.path.join('../experiment_1/biobert_extractive_only_training_dataset', fpath), 'rb'))\n",
        "    #     )\n",
        "    # df = pd.DataFrame(all_extracted_summaries)\n",
        "    # # ----\n",
        "\n",
        "    target_texts = ms2_dataset['target']\n",
        "    input_texts = [\n",
        "        df[df['review_id'] == int(i)]['summary'].tolist()[0] for i in ms2_dataset['review_id']\n",
        "    ]\n",
        "    dataset = Dataset.from_dict({'input_text': input_texts, 'target_text': target_texts})\n",
        "\n",
        "    # Tokenize data\n",
        "    def tokenize_function(examples):\n",
        "        model_inputs = tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=max_input_token_length)\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(text_target=examples['target_text'], padding='max_length', truncation=True, max_length=256)\n",
        "            labels[\"input_ids\"] = [\n",
        "                [(l if l != tokenizer.pad_token_id else label_pad_token_id) for l in label] for label in labels[\"input_ids\"]\n",
        "            ]\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        return model_inputs\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"input_text\", \"target_text\"])\n",
        "    print(f\"Keys of tokenized dataset: {list(tokenized_datasets.features)}\")\n",
        "\n",
        "    # Split the dataset\n",
        "    shuffle_dataset = tokenized_datasets.shuffle(seed=42)\n",
        "    shuffle_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    train_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10))\n",
        "    val_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10, len(tokenized_datasets)))\n",
        "\n",
        "    # save to disk for easy loading\n",
        "    train_dataset.save_to_disk(train_data_path)\n",
        "    val_dataset.save_to_disk(val_data_path)\n",
        "\n",
        "print(train_dataset[\"input_ids\"].shape)\n",
        "print(val_dataset[\"input_ids\"].shape)\n",
        "type(train_dataset[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "DD-_P8W7bTST",
        "outputId": "955192b1-a095-4da4-f313-1f478dcff286"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    11350.000000\n",
              "mean      3661.722291\n",
              "std       2308.642882\n",
              "min         71.000000\n",
              "25%       1855.000000\n",
              "50%       3050.000000\n",
              "75%       5047.000000\n",
              "max       8192.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95% percentile is 8192.0\n",
            "If we truncated input_ids to 8192, this is the percentile it'll be at (anything at a higher percentile could risk losing information): 0.9473568281938326\n",
            "8192.0\n"
          ]
        }
      ],
      "source": [
        "# ANALYSIS: what's the distribution of non-padding tokens in train_dataset[\"input_ids\"]?\n",
        "all_tokens = train_dataset[\"input_ids\"].numpy()\n",
        "non_pad_token_counts = np.array([len(np.where(tokens != 0)[0]) for tokens in all_tokens])\n",
        "# distribution of non_pad_token_counts\n",
        "display(pd.Series(non_pad_token_counts).describe())\n",
        "\n",
        "# what's the 95% percentile?\n",
        "print(\"95% percentile is\", np.percentile(non_pad_token_counts, 95))\n",
        "\n",
        "# which percentile is \"8192 non-padding tokens\" on?\n",
        "print(\n",
        "    \"If we truncated input_ids to 8192, this is the percentile it'll be at (anything at a higher percentile could risk losing information):\",\n",
        "    (perc_8192 := pd.Series(non_pad_token_counts).rank(pct=True)[np.where(non_pad_token_counts <= 8192)[0]].max())\n",
        ")\n",
        "# confirm\n",
        "print(np.percentile(non_pad_token_counts, perc_8192 * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLJzPFWk5_1T",
        "outputId": "1b5e4d8f-2b62-463e-e82f-2656a9d0286a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,769,472 || all params: 249,356,928 || trainable%: 0.7096141319161583\n",
            "Tensorboard log path: /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/logs\n",
            "run this in terminal: tensorboard --logdir /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/logs\n"
          ]
        }
      ],
      "source": [
        "# bitsandbytes\n",
        "# Source notebooks:\n",
        "# - https://colab.research.google.com/drive/1Vvju5kOyBsDr7RX_YAvp6ZsSOoSMjhKD?usp=sharing#scrollTo=E0Nl5mWL0k2T\n",
        "# - https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing#scrollTo=HOWcL0LU3JYt\n",
        "\n",
        "checkpoint_path = \"longt5-qlora\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    # load_in_8bit=True,\n",
        ")\n",
        "\n",
        "base_model = LongT5ForConditionalGeneration.from_pretrained(model_id)\n",
        "model = LongT5ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=bnb_config,  # enable when in CUDA\n",
        "    # device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# # BUG: `model` has its embeddings reinitiated. Copy over from `base_model` but retain data type\n",
        "# reinited_params = ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
        "# for param_name in reinited_params:\n",
        "#     model_param = model.get_parameter(param_name)\n",
        "#     base_model_param = base_model.get_parameter(param_name)\n",
        "#     model_param.data = (\n",
        "#         base_model_param.data\n",
        "#         .to(model_param.dtype)  # or, comment out to remain in 32-bit for accuracy\n",
        "#         .to(device)\n",
        "#     )\n",
        "\n",
        "# use PEFT LoRA\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    # target_modules=[\"q\", \"v\", \"k\"],\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    # target_modules=[\"q\"],\n",
        "    layers_to_transform=list(range(0, 12)),  # 11 is max layer\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "# model = prepare_model_for_kbit_training(model)  # enable for 4bit or 8bit quantization\n",
        "model.enable_input_require_grads()\n",
        "model = get_peft_model(model, lora_config)\n",
        "# Fix from this GitHub issue: https://github.com/huggingface/peft/issues/522#issuecomment-1705989330\n",
        "model.base_model.model.encoder.enable_input_require_grads()\n",
        "model.base_model.model.decoder.enable_input_require_grads()\n",
        "\n",
        "model.train()\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Training arguments\n",
        "logpath = os.path.join(output_dir, checkpoint_path, \"logs\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=os.path.join(output_dir, checkpoint_path),\n",
        "    evaluation_strategy=\"steps\",  # alternatively, \"epoch\"\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=1e-3,\n",
        "    logging_dir=logpath,\n",
        "    report_to=\"tensorboard\",\n",
        "    save_strategy=\"steps\",\n",
        "    fp16=False,\n",
        "    # predict_with_generate=True,\n",
        "\n",
        "    # FOR REAL TRAINING\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # auto_find_batch_size=True,\n",
        "    eval_steps=200,\n",
        "    logging_steps=100,\n",
        "    save_steps=100,\n",
        "    log_level=\"info\",\n",
        "\n",
        "    # FOR DEBUGGING\n",
        "    # num_train_epochs=1,\n",
        "    # per_device_train_batch_size=1,\n",
        "    # per_device_eval_batch_size=1,\n",
        "    # max_steps=20,\n",
        "    # eval_steps=2,\n",
        "    # logging_steps=2,  # should match eval_steps\n",
        "    # save_steps=4,  # includes train loss metric\n",
        "    # log_level=\"debug\",\n",
        "\n",
        "    # FOR 4BIT OR 8BIT QUANTIZATION\n",
        "    # fp16=True,\n",
        "    # optim=\"paged_adamw_8bit\",  # default: adamw_torch\n",
        ")\n",
        "\n",
        "print(\"Tensorboard log path:\", logpath)\n",
        "print(\"run this in terminal: tensorboard --logdir\", logpath)\n",
        "\n",
        "# Initialize Trainer\n",
        "model.config.use_cache = False\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    # model=model_id,\n",
        "    # label_pad_token_id=label_pad_token_id,\n",
        "    # pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset.shuffle(seed=42).select(range(200)),\n",
        "    # eval_dataset=val_dataset.select(range(10, 20)),  # for debugging\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhkpNOlQZ7WG",
        "outputId": "6cb3b2e1-1c85-4010-deb6-a0df4c186449"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.5561,  0.4233,  0.8544,  ..., -0.9618,  0.6647,  0.9398],\n",
              "        [ 0.4269,  1.6681,  4.5766,  ..., -2.2274, -0.5151,  2.1782],\n",
              "        [-5.4195, -2.4177, -0.8740,  ..., -0.2788, -1.3139, -1.5880],\n",
              "        ...,\n",
              "        [ 1.5533,  0.5635,  1.6218,  ...,  1.9036,  0.7348,  0.1447],\n",
              "        [ 0.2494,  0.8528, -0.6396,  ...,  0.1166, -1.1269,  0.8604],\n",
              "        [ 0.8795, -0.3369, -1.7056,  ...,  0.4987,  1.2487,  0.6472]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.get_parameter(\"encoder.embed_tokens.weight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asbD4STKY8KY",
        "outputId": "6604e99d-c13b-429e-ce30-bfb22590df67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.5561,  0.4233,  0.8544,  ..., -0.9618,  0.6647,  0.9398],\n",
              "        [ 0.4269,  1.6681,  4.5766,  ..., -2.2274, -0.5151,  2.1782],\n",
              "        [-5.4195, -2.4177, -0.8740,  ..., -0.2788, -1.3139, -1.5880],\n",
              "        ...,\n",
              "        [ 1.5533,  0.5635,  1.6218,  ...,  1.9036,  0.7348,  0.1447],\n",
              "        [ 0.2494,  0.8528, -0.6396,  ...,  0.1166, -1.1269,  0.8604],\n",
              "        [ 0.8795, -0.3369, -1.7056,  ...,  0.4987,  1.2487,  0.6472]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_parameter(\"encoder.embed_tokens.weight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2PIzIYwD6Q3",
        "outputId": "844f09c1-b943-40ab-e375-51b18eefa77b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0092,  0.0045,  0.0233,  ..., -0.0029, -0.0240, -0.0316],\n",
            "        [-0.0245,  0.0008, -0.0296,  ...,  0.0352,  0.0221,  0.0048],\n",
            "        [ 0.0348,  0.0140, -0.0356,  ..., -0.0293,  0.0241,  0.0042],\n",
            "        ...,\n",
            "        [-0.0112, -0.0082, -0.0025,  ..., -0.0095, -0.0045,  0.0197],\n",
            "        [-0.0340,  0.0227, -0.0244,  ...,  0.0126, -0.0216, -0.0190],\n",
            "        [-0.0286, -0.0319,  0.0334,  ...,  0.0159,  0.0300,  0.0041]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([16, 768])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c5SdhYUf7fH",
        "outputId": "3ef68696-110b-445b-b2e8-0e23b4fc53ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([768, 16])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJGZK8VgLSOR",
        "outputId": "edb14946-9b58-4ca6-979f-d41d527fca80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(train_dataset[\"labels\"].device)\n",
        "model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5C8feaoL5_1U"
      },
      "outputs": [],
      "source": [
        "# # to reset memory\n",
        "# del train_dataset, val_dataset, tokenizer\n",
        "# del model, base_model, data_collator, trainer\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()  # Colab\n",
        "# # torch.mps.empty_cache()  # MPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQyunNVb5_1U",
        "outputId": "161f142a-fd87-4c07-e9af-d63832e7ddc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [1024], which does not match the required output shape [1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 8, 128, 1], which does not match the required output shape [1, 8, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('In this paper, the authors describe a study of safety and efficacy of two '\n",
            " 'different local anesthesias in patients with hypertension. The aim of the '\n",
            " 'study is to determine the effects of these two different anestheses on blood '\n",
            " 'pressure before and after restorative tooth extraction. A total of sixty-two '\n",
            " 'patients are included in this study. Sixty were assigned to receive either 2 '\n",
            " 'p.Liocaine or 1 % Licocaine for local analization. After a single tooth '\n",
            " 'extraction, both groups showed similar changes in heartbeat and blood '\n",
            " 'pressure.')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "base_model = base_model.to(device)\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = base_model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128, num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSj-bjeBd8K-",
        "outputId": "5e718425-f780-46c7-b5f4-6a87583bd790"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('A prospective open-based, single-exaggerated study evaluating safety and '\n",
            " 'effusiveness of local analin in the treatment of patients with severe '\n",
            " 'hypertension. In this paper, blood pressures were measured after tooth '\n",
            " 'extraction by a method employing two different concentrations of '\n",
            " 'lignocenelilicous drugs: one at a 0.1ug / mL and the other as soon as the '\n",
            " 'patient had been removed from the room. The results showed that there was no '\n",
            " 'significant change in blood pressure or pulse rate between the two control '\n",
            " 'groups.')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128, num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QggGMOa0cxP2",
        "outputId": "db4c4d74-7b93-482b-9c07-5a5dc356d615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without padding tokens\n",
            "tensor(3.7706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-22.0120,  -7.3763,  -6.8542,  ..., -21.5278, -21.9906, -21.9159],\n",
            "         [-28.0730,  -6.6089,  -6.8185,  ..., -27.5369, -28.1858, -27.8626],\n",
            "         [-30.6885,  -6.5421, -10.9930,  ..., -30.0147, -30.4963, -30.3102],\n",
            "         ...,\n",
            "         [-28.4630,  -3.2069,  -7.9613,  ..., -27.7613, -28.3718, -28.3400],\n",
            "         [-29.3955,  -1.9470,  -4.6935,  ..., -28.8900, -29.5536, -29.4921],\n",
            "         [-29.7273,  -2.3630,  -7.3372,  ..., -29.1836, -29.8692, -29.5749]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    labels=train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"Without padding tokens\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZVtxa7FdhXE",
        "outputId": "24d87923-fd9b-42d5-f26b-b62056d34dfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With padding tokens in labels\n",
            "tensor(3.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-17.5048,  -3.3334,  -4.8966,  ..., -17.3239, -17.5362, -17.5236],\n",
            "         [-29.3313,  -7.5968,  -6.0506,  ..., -28.8467, -29.3566, -29.1388],\n",
            "         [-33.3023, -11.5635, -13.7315,  ..., -32.7689, -33.4021, -33.0907],\n",
            "         ...,\n",
            "         [-19.4768,   1.2223,  -2.6142,  ..., -19.1825, -19.5845, -19.5321],\n",
            "         [-20.8250,  -2.5398,  -5.6711,  ..., -20.4851, -20.8054, -20.7826],\n",
            "         [-15.5038,  -1.7199,  -3.8062,  ..., -15.2332, -15.5030, -15.4429]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    labels=train_dataset[id_to_choose]['labels'].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"With padding tokens in labels\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GokqRnCpmCch"
      },
      "source": [
        "### Some diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r4VRnQEG2We",
        "outputId": "85d4a9af-df57-4c38-86cf-c7d2aefa2f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.shared.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.relative_attention_bias.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.global_relative_attention_bias.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.0.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.0.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.1.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.2.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.3.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.4.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.5.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.6.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.7.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.8.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.9.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.10.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.11.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.final_layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\n",
            "base_model.model.decoder.block.0.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.0.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.0.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.1.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.1.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.1.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.2.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.2.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.2.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.3.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.3.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.3.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.4.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.4.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.4.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.5.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.5.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.5.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.6.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.6.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.6.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.7.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.7.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.7.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.8.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.8.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.8.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.9.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.9.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.9.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.10.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.10.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.10.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.11.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.11.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.11.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.final_layer_norm.weight False\n",
            "base_model.model.lm_head.weight False\n"
          ]
        }
      ],
      "source": [
        "for name, param in trainer.model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtX_iCOBIPTp",
        "outputId": "b2ca3fce-bf5d-4a5e-fd3f-474d277a9c2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForSeq2SeqLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LongT5ForConditionalGeneration(\n",
              "      (shared): Embedding(32128, 768)\n",
              "      (encoder): LongT5Stack(\n",
              "        (embed_tokens): Embedding(32128, 768)\n",
              "        (block): ModuleList(\n",
              "          (0): LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerTransientGlobalSelfAttention(\n",
              "                (TransientGlobalSelfAttention): LongT5TransientGlobalAttention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 12)\n",
              "                  (global_relative_attention_bias): Embedding(32, 12)\n",
              "                  (global_input_layer_norm): LongT5LayerNorm()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-11): 11 x LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerTransientGlobalSelfAttention(\n",
              "                (TransientGlobalSelfAttention): LongT5TransientGlobalAttention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (global_input_layer_norm): LongT5LayerNorm()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): LongT5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (decoder): LongT5Stack(\n",
              "        (embed_tokens): Embedding(32128, 768)\n",
              "        (block): ModuleList(\n",
              "          (0): LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerSelfAttention(\n",
              "                (SelfAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 12)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerCrossAttention(\n",
              "                (EncDecAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-11): 11 x LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerSelfAttention(\n",
              "                (SelfAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerCrossAttention(\n",
              "                (EncDecAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): LongT5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqGg5zM_mH3o"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP9yWIPazB0K",
        "outputId": "62cc3a48-4c04-4984-d5a4-772484da28e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from checkpoint: /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-2750\n"
          ]
        }
      ],
      "source": [
        "# (If needed) Load model from checkpoint\n",
        "latest_checkpoint = max([int(f.split('-')[1]) for f in os.listdir(os.path.join(output_dir, checkpoint_path)) if f.startswith('checkpoint')])\n",
        "if latest_checkpoint:\n",
        "    resume_from_checkpoint = os.path.join(output_dir, checkpoint_path, f\"checkpoint-{latest_checkpoint}\")\n",
        "    print(\"Resuming from checkpoint:\", resume_from_checkpoint)\n",
        "else:\n",
        "    resume_from_checkpoint = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HvzKoAjO5_1V",
        "outputId": "db910c9d-1aa0-4cce-b27a-163590017632"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading model from /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-2750.\n",
            "***** Running training *****\n",
            "  Num examples = 11,350\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5,676\n",
            "  Number of trainable parameters = 1,769,472\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 1\n",
            "  Continuing training from global step 2750\n",
            "  Will skip the first 1 epochs then the first 1331 batches in the first epoch.\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5676' max='5676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5676/5676 5:09:12, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.843100</td>\n",
              "      <td>2.524143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.819500</td>\n",
              "      <td>2.502765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.773700</td>\n",
              "      <td>2.488067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [6, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [6, 64, 128, 1], which does not match the required output shape [6, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3000\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3000/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3250\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3250/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3250/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3500\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3500/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3750\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3750/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3750/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4000\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4000/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4250\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4250/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4250/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [6, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [6, 64, 128, 1], which does not match the required output shape [6, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4500\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4500/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4750\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4750/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4750/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5000\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5000/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5250\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5250/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5250/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5500\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5500/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [6, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [6, 64, 128, 1], which does not match the required output shape [6, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5676, training_loss=1.4375211676717896, metrics={'train_runtime': 18559.6412, 'train_samples_per_second': 2.446, 'train_steps_per_second': 0.306, 'total_flos': 4.516809378103296e+17, 'train_loss': 1.4375211676717896, 'epoch': 4.0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "_LQxsV2infsn",
        "outputId": "a0267845-271b-4d4b-9cbf-bf7144b83eac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [2, 64, 128, 1], which does not match the required output shape [2, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 2.6070263385772705,\n",
              " 'eval_runtime': 2.2912,\n",
              " 'eval_samples_per_second': 4.365,\n",
              " 'eval_steps_per_second': 0.873,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate on custom slice of train dataset\n",
        "trainer.evaluate(train_dataset.select(range(0, 10)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "xC6FnvyL5_1V",
        "outputId": "879d4e2b-4253-4fda-f2ce-953d489bdb4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 4.012439727783203,\n",
              " 'eval_runtime': 11.0355,\n",
              " 'eval_samples_per_second': 0.906,\n",
              " 'eval_steps_per_second': 0.906,\n",
              " 'epoch': 0.0}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# view results\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GcN42yAxF-s",
        "outputId": "e5fcd8b1-268e-4b5e-ed66-241708496666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0197,  0.0322, -0.0018,  ...,  0.0370, -0.0552,  0.0419],\n",
            "        [ 0.0526, -0.0724, -0.0003,  ...,  0.0095, -0.0972,  0.0021],\n",
            "        [ 0.0086,  0.0318,  0.0109,  ...,  0.0422,  0.0820,  0.0055],\n",
            "        ...,\n",
            "        [-0.0266,  0.0364,  0.0284,  ...,  0.0088,  0.0610,  0.0288],\n",
            "        [ 0.0816, -0.0670,  0.0038,  ...,  0.0564, -0.0580, -0.0146],\n",
            "        [-0.0146,  0.0368, -0.0205,  ..., -0.0566,  0.0114, -0.0238]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([16, 768])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1YHELDKxF-y",
        "outputId": "0e7b721d-8a9f-4ba8-981d-871466db7d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0539, -0.0352,  0.0256,  ...,  0.0116, -0.0168, -0.0118],\n",
            "        [ 0.0160, -0.0318,  0.0142,  ...,  0.0199, -0.0460, -0.0028],\n",
            "        [ 0.0271, -0.0188,  0.0189,  ...,  0.0334, -0.0603, -0.0007],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0071,  0.0259,  ..., -0.0034,  0.0119,  0.0226],\n",
            "        [-0.0341, -0.0029, -0.0074,  ..., -0.0272,  0.0215,  0.0012],\n",
            "        [-0.0263, -0.0530, -0.0163,  ..., -0.0220,  0.0195,  0.0521]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([768, 16])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPBeNMXpSi5",
        "outputId": "60d81509-3ac0-409b-cffd-aeba914587bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 64, 128, 1], which does not match the required output shape [1, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('There was no significant difference in blood pressure or pulse rate between '\n",
            " 'the 2 groups. Conclusions This meta- analysis shows that there is no '\n",
            " 'evidence to support the use of angiotenain 1:20,000 as a local anesthesia '\n",
            " 'for tooth extraction')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ24qJjg83Vw",
        "outputId": "577d4cba-772d-40c6-f29f-410579ae7199"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 64, 128, 1], which does not match the required output shape [1, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('The results of this meta- analysis showed that the use of a single dose of '\n",
            " 'lidocoine was associated with an increase in heart rate. However, there were '\n",
            " 'no significant differences between these two groups on pulse rates and blood '\n",
            " 'pressure.')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnkEjEgy1f7I",
        "outputId": "6030fc72-4ab0-42b5-b2ff-3ddc17083be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With padding tokens in labels\n",
            "tensor(2.8820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-16.2487,  -0.6006,  -1.9300,  ..., -15.8971, -16.3333, -16.1956],\n",
            "         [-19.4892,  -3.5078,  -2.8381,  ..., -19.1383, -19.5005, -19.3040],\n",
            "         [-16.9491,  -3.9170,  -5.3435,  ..., -16.6350, -16.9932, -16.8125],\n",
            "         ...,\n",
            "         [-15.2111,   0.2993,  -1.5088,  ..., -14.8921, -15.2493, -15.1783],\n",
            "         [ -9.2678,   1.8246,   0.2085,  ...,  -9.0547,  -9.3041,  -9.3049],\n",
            "         [-13.5367,   0.1113,  -1.5535,  ..., -13.3507, -13.5380, -13.5523]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    # labels=train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id].unsqueeze(0),\n",
        "    labels=train_dataset[id_to_choose]['labels'].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"With padding tokens in labels\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtmkk7U85_1V",
        "outputId": "529b88fe-7de9-4feb-92d4-fa363725531f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/tokenizer.json')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save model\n",
        "# trainer.save_model(os.path.join(output_dir, \"longt5-qlora-final\"))\n",
        "final_save_dir = \"longt5-qlora-4-epochs-final\"\n",
        "trainer.model.save_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "tokenizer.save_pretrained(os.path.join(output_dir, final_save_dir))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GokqRnCpmCch"
      ],
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
