{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8DvMnJS6T9E",
        "outputId": "8b29c8f3-c78f-4354-f939-52dac95c03ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTTKsqKPlicw",
        "outputId": "59810438-b4b3-47d0-c325-afe34e48cd09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec  5 08:45:15 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    25W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iTzCAkM5_1R",
        "outputId": "4f214437-a56f-44d0-a8a9-2f696161f773"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade git+https://github.com/huggingface/transformers\n",
        "!pip install -q --upgrade git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes accelerate datasets tensorboardX loralib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNk98aFo5_1T",
        "outputId": "d072c3df-d171-4b0c-a76d-cf5abe24daa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA device.\n"
          ]
        }
      ],
      "source": [
        "# Purpose of notebook: fine-tune LongT5 on exctracted sentences from studies, but using LoRA and bitsandbytes quantization\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from pprint import pprint\n",
        "import gc\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import (\n",
        "    LongT5ForConditionalGeneration,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    BitsAndBytesConfig,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    Seq2SeqTrainer,\n",
        "    Seq2SeqTrainingArguments,\n",
        ")\n",
        "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "import bitsandbytes as bnb\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using MPS device.\")\n",
        "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = \"0.0\"\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA device.\")\n",
        "    max_split_size_mb = 256  # Set the max_split_size_mb value (e.g., 512 MB)\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"max_split_size_mb:{max_split_size_mb}\"\n",
        "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"MPS/CUDA not available. Using CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8uE7Q8OJQzF"
      },
      "outputs": [],
      "source": [
        "# -------- START CONFIG ----------\n",
        "# Load tokenizer and model\n",
        "model_id = 'pszemraj/long-t5-tglobal-base-16384-book-summary'\n",
        "# output_dir = \"training_history\"\n",
        "output_dir = \"/content/drive/MyDrive/266 final project/notebooks/peft_training_history\"  # Colab\n",
        "\n",
        "# extracted_file_path = '../experiment_1/biobert_extractive_only_training_dataset.csv.gz'\n",
        "extracted_file_path = '/content/drive/MyDrive/266 final project/notebooks/biobert_extractive_only_training_dataset.csv.gz'  # Colab\n",
        "\n",
        "# source_data_path = \"data\"\n",
        "source_data_path = \"/content/drive/MyDrive/266 final project/notebooks/peft_training_history/data\"  # Colab\n",
        "# source_data_path = \"/content/drive/MyDrive/266 final project/notebooks/peft_training_history/data_1024\"  # Colab\n",
        "\n",
        "# longT5 max token length is 16384, let's 1/2 that\n",
        "max_input_token_length = 8192\n",
        "# max_input_token_length = 1024\n",
        "\n",
        "# -------- END CONFIG ----------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq9cgF-4AobS",
        "outputId": "69359af9-4478-43a0-aa14-a53cf2df7e06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([11350, 8192])\n",
            "torch.Size([2838, 8192])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# label_pad_token_id = tokenizer.pad_token_id\n",
        "label_pad_token_id = -100  # special label token that gets ignored in loss calculations\n",
        "\n",
        "train_data_path = os.path.join(source_data_path, 'train_tokenized_dataset')\n",
        "val_data_path = os.path.join(source_data_path, 'val_tokenized_dataset')\n",
        "\n",
        "if os.path.exists(train_data_path) and os.path.exists(val_data_path):\n",
        "    train_dataset = Dataset.load_from_disk(train_data_path)\n",
        "    val_dataset = Dataset.load_from_disk(val_data_path)\n",
        "\n",
        "else:\n",
        "    ms2_dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"train\")\n",
        "\n",
        "    # Load your CSV file\n",
        "    df = pd.read_csv(extracted_file_path, compression='gzip')\n",
        "\n",
        "    # # ---- if full extracted data is not available yet:\n",
        "    # all_extracted_summaries = []\n",
        "    # for fpath in os.listdir('../experiment_1/biobert_extractive_only_training_dataset'):\n",
        "    #     all_extracted_summaries.append(\n",
        "    #         pickle.load(open(os.path.join('../experiment_1/biobert_extractive_only_training_dataset', fpath), 'rb'))\n",
        "    #     )\n",
        "    # df = pd.DataFrame(all_extracted_summaries)\n",
        "    # # ----\n",
        "\n",
        "    target_texts = ms2_dataset['target']\n",
        "    input_texts = [\n",
        "        df[df['review_id'] == int(i)]['summary'].tolist()[0] for i in ms2_dataset['review_id']\n",
        "    ]\n",
        "    dataset = Dataset.from_dict({'input_text': input_texts, 'target_text': target_texts})\n",
        "\n",
        "    # Tokenize data\n",
        "    def tokenize_function(examples):\n",
        "        model_inputs = tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=max_input_token_length)\n",
        "        with tokenizer.as_target_tokenizer():\n",
        "            labels = tokenizer(text_target=examples['target_text'], padding='max_length', truncation=True, max_length=256)\n",
        "            labels[\"input_ids\"] = [\n",
        "                [(l if l != tokenizer.pad_token_id else label_pad_token_id) for l in label] for label in labels[\"input_ids\"]\n",
        "            ]\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        return model_inputs\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"input_text\", \"target_text\"])\n",
        "    print(f\"Keys of tokenized dataset: {list(tokenized_datasets.features)}\")\n",
        "\n",
        "    # Split the dataset\n",
        "    shuffle_dataset = tokenized_datasets.shuffle(seed=42)\n",
        "    shuffle_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    train_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10))\n",
        "    val_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10, len(tokenized_datasets)))\n",
        "\n",
        "    # save to disk for easy loading\n",
        "    train_dataset.save_to_disk(train_data_path)\n",
        "    val_dataset.save_to_disk(val_data_path)\n",
        "\n",
        "print(train_dataset[\"input_ids\"].shape)\n",
        "print(val_dataset[\"input_ids\"].shape)\n",
        "type(train_dataset[\"input_ids\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "DD-_P8W7bTST",
        "outputId": "955192b1-a095-4da4-f313-1f478dcff286"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    11350.000000\n",
              "mean      3661.722291\n",
              "std       2308.642882\n",
              "min         71.000000\n",
              "25%       1855.000000\n",
              "50%       3050.000000\n",
              "75%       5047.000000\n",
              "max       8192.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "95% percentile is 8192.0\n",
            "If we truncated input_ids to 8192, this is the percentile it'll be at (anything at a higher percentile could risk losing information): 0.9473568281938326\n",
            "8192.0\n"
          ]
        }
      ],
      "source": [
        "# ANALYSIS: what's the distribution of non-padding tokens in train_dataset[\"input_ids\"]?\n",
        "all_tokens = train_dataset[\"input_ids\"].numpy()\n",
        "non_pad_token_counts = np.array([len(np.where(tokens != 0)[0]) for tokens in all_tokens])\n",
        "# distribution of non_pad_token_counts\n",
        "display(pd.Series(non_pad_token_counts).describe())\n",
        "\n",
        "# what's the 95% percentile?\n",
        "print(\"95% percentile is\", np.percentile(non_pad_token_counts, 95))\n",
        "\n",
        "# which percentile is \"8192 non-padding tokens\" on?\n",
        "print(\n",
        "    \"If we truncated input_ids to 8192, this is the percentile it'll be at (anything at a higher percentile could risk losing information):\",\n",
        "    (perc_8192 := pd.Series(non_pad_token_counts).rank(pct=True)[np.where(non_pad_token_counts <= 8192)[0]].max())\n",
        ")\n",
        "# confirm\n",
        "print(np.percentile(non_pad_token_counts, perc_8192 * 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLJzPFWk5_1T",
        "outputId": "1b5e4d8f-2b62-463e-e82f-2656a9d0286a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1,769,472 || all params: 249,356,928 || trainable%: 0.7096141319161583\n",
            "Tensorboard log path: training_history/longt5-qlora/logs\n",
            "run this in terminal: tensorboard --logdir training_history/longt5-qlora/logs\n"
          ]
        }
      ],
      "source": [
        "# bitsandbytes\n",
        "# Source notebooks:\n",
        "# - https://colab.research.google.com/drive/1Vvju5kOyBsDr7RX_YAvp6ZsSOoSMjhKD?usp=sharing#scrollTo=E0Nl5mWL0k2T\n",
        "# - https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing#scrollTo=HOWcL0LU3JYt\n",
        "# More background info:\n",
        "# - https://huggingface.co/blog/hf-bitsandbytes-integration\n",
        "\n",
        "checkpoint_path = \"longt5-qlora\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    # load_in_8bit=True,\n",
        ")\n",
        "\n",
        "base_model = LongT5ForConditionalGeneration.from_pretrained(model_id)\n",
        "model = LongT5ForConditionalGeneration.from_pretrained(\n",
        "    model_id,\n",
        "    # quantization_config=bnb_config,  # enable when in CUDA\n",
        "    # device_map=\"auto\",\n",
        ")\n",
        "\n",
        "# # BUG: `model` has its embeddings reinitiated. Copy over from `base_model` but retain data type\n",
        "# reinited_params = ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
        "# for param_name in reinited_params:\n",
        "#     model_param = model.get_parameter(param_name)\n",
        "#     base_model_param = base_model.get_parameter(param_name)\n",
        "#     model_param.data = (\n",
        "#         base_model_param.data\n",
        "#         .to(model_param.dtype)  # or, comment out to remain in 32-bit for accuracy\n",
        "#         .to(device)\n",
        "#     )\n",
        "\n",
        "# use PEFT LoRA\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    # target_modules=[\"q\", \"v\", \"k\"],\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    # target_modules=[\"q\"],\n",
        "    layers_to_transform=list(range(0, 12)),  # 11 is max layer\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "model.gradient_checkpointing_enable()\n",
        "# model = prepare_model_for_kbit_training(model)  # enable for 4bit or 8bit quantization\n",
        "model.enable_input_require_grads()\n",
        "model = get_peft_model(model, lora_config)\n",
        "# Fix from this GitHub issue: https://github.com/huggingface/peft/issues/522#issuecomment-1705989330\n",
        "model.base_model.model.encoder.enable_input_require_grads()\n",
        "model.base_model.model.decoder.enable_input_require_grads()\n",
        "\n",
        "model.train()\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Training arguments\n",
        "logpath = os.path.join(output_dir, checkpoint_path, \"logs\")\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=os.path.join(output_dir, checkpoint_path),\n",
        "    evaluation_strategy=\"steps\",  # alternatively, \"epoch\"\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=1e-3,\n",
        "    logging_dir=logpath,\n",
        "    report_to=\"tensorboard\",\n",
        "    save_strategy=\"steps\",\n",
        "    fp16=False,\n",
        "    # predict_with_generate=True,\n",
        "\n",
        "    # FOR REAL TRAINING\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # auto_find_batch_size=True,\n",
        "    eval_steps=200,\n",
        "    logging_steps=100,\n",
        "    save_steps=100,\n",
        "    log_level=\"info\",\n",
        "\n",
        "    # FOR DEBUGGING\n",
        "    # num_train_epochs=1,\n",
        "    # per_device_train_batch_size=1,\n",
        "    # per_device_eval_batch_size=1,\n",
        "    # max_steps=20,\n",
        "    # eval_steps=2,\n",
        "    # logging_steps=2,  # should match eval_steps\n",
        "    # save_steps=4,  # includes train loss metric\n",
        "    # log_level=\"debug\",\n",
        "\n",
        "    # FOR 4BIT OR 8BIT QUANTIZATION\n",
        "    # fp16=True,\n",
        "    # optim=\"paged_adamw_8bit\",  # default: adamw_torch\n",
        ")\n",
        "\n",
        "print(\"Tensorboard log path:\", logpath)\n",
        "print(\"run this in terminal: tensorboard --logdir\", logpath)\n",
        "\n",
        "# Initialize Trainer\n",
        "model.config.use_cache = False\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    # model=model_id,\n",
        "    # label_pad_token_id=label_pad_token_id,\n",
        "    # pad_to_multiple_of=8,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset.shuffle(seed=42).select(range(200)),\n",
        "    # eval_dataset=val_dataset.select(range(10, 20)),  # for debugging\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhkpNOlQZ7WG",
        "outputId": "6cb3b2e1-1c85-4010-deb6-a0df4c186449"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.5561,  0.4233,  0.8544,  ..., -0.9618,  0.6647,  0.9398],\n",
              "        [ 0.4269,  1.6681,  4.5766,  ..., -2.2274, -0.5151,  2.1782],\n",
              "        [-5.4195, -2.4177, -0.8740,  ..., -0.2788, -1.3139, -1.5880],\n",
              "        ...,\n",
              "        [ 1.5533,  0.5635,  1.6218,  ...,  1.9036,  0.7348,  0.1447],\n",
              "        [ 0.2494,  0.8528, -0.6396,  ...,  0.1166, -1.1269,  0.8604],\n",
              "        [ 0.8795, -0.3369, -1.7056,  ...,  0.4987,  1.2487,  0.6472]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "base_model.get_parameter(\"encoder.embed_tokens.weight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asbD4STKY8KY",
        "outputId": "6604e99d-c13b-429e-ce30-bfb22590df67"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-0.5561,  0.4233,  0.8544,  ..., -0.9618,  0.6647,  0.9398],\n",
              "        [ 0.4269,  1.6681,  4.5766,  ..., -2.2274, -0.5151,  2.1782],\n",
              "        [-5.4195, -2.4177, -0.8740,  ..., -0.2788, -1.3139, -1.5880],\n",
              "        ...,\n",
              "        [ 1.5533,  0.5635,  1.6218,  ...,  1.9036,  0.7348,  0.1447],\n",
              "        [ 0.2494,  0.8528, -0.6396,  ...,  0.1166, -1.1269,  0.8604],\n",
              "        [ 0.8795, -0.3369, -1.7056,  ...,  0.4987,  1.2487,  0.6472]],\n",
              "       device='cuda:0')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_parameter(\"encoder.embed_tokens.weight\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2PIzIYwD6Q3",
        "outputId": "844f09c1-b943-40ab-e375-51b18eefa77b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0092,  0.0045,  0.0233,  ..., -0.0029, -0.0240, -0.0316],\n",
            "        [-0.0245,  0.0008, -0.0296,  ...,  0.0352,  0.0221,  0.0048],\n",
            "        [ 0.0348,  0.0140, -0.0356,  ..., -0.0293,  0.0241,  0.0042],\n",
            "        ...,\n",
            "        [-0.0112, -0.0082, -0.0025,  ..., -0.0095, -0.0045,  0.0197],\n",
            "        [-0.0340,  0.0227, -0.0244,  ...,  0.0126, -0.0216, -0.0190],\n",
            "        [-0.0286, -0.0319,  0.0334,  ...,  0.0159,  0.0300,  0.0041]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([16, 768])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_c5SdhYUf7fH",
        "outputId": "3ef68696-110b-445b-b2e8-0e23b4fc53ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([768, 16])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJGZK8VgLSOR",
        "outputId": "edb14946-9b58-4ca6-979f-d41d527fca80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(train_dataset[\"labels\"].device)\n",
        "model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5C8feaoL5_1U"
      },
      "outputs": [],
      "source": [
        "# # to reset memory\n",
        "# del train_dataset, val_dataset, tokenizer\n",
        "# del model, base_model, data_collator, trainer\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()  # Colab\n",
        "# # torch.mps.empty_cache()  # MPS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQyunNVb5_1U",
        "outputId": "161f142a-fd87-4c07-e9af-d63832e7ddc1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [1024], which does not match the required output shape [1, 1024]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 8, 128, 1], which does not match the required output shape [1, 8, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('In this paper, the authors describe a study of safety and efficacy of two '\n",
            " 'different local anesthesias in patients with hypertension. The aim of the '\n",
            " 'study is to determine the effects of these two different anestheses on blood '\n",
            " 'pressure before and after restorative tooth extraction. A total of sixty-two '\n",
            " 'patients are included in this study. Sixty were assigned to receive either 2 '\n",
            " 'p.Liocaine or 1 % Licocaine for local analization. After a single tooth '\n",
            " 'extraction, both groups showed similar changes in heartbeat and blood '\n",
            " 'pressure.')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "base_model = base_model.to(device)\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = base_model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128, num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSj-bjeBd8K-",
        "outputId": "5e718425-f780-46c7-b5f4-6a87583bd790"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('A prospective open-based, single-exaggerated study evaluating safety and '\n",
            " 'effusiveness of local analin in the treatment of patients with severe '\n",
            " 'hypertension. In this paper, blood pressures were measured after tooth '\n",
            " 'extraction by a method employing two different concentrations of '\n",
            " 'lignocenelilicous drugs: one at a 0.1ug / mL and the other as soon as the '\n",
            " 'patient had been removed from the room. The results showed that there was no '\n",
            " 'significant change in blood pressure or pulse rate between the two control '\n",
            " 'groups.')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128, num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QggGMOa0cxP2",
        "outputId": "db4c4d74-7b93-482b-9c07-5a5dc356d615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without padding tokens\n",
            "tensor(3.7706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-22.0120,  -7.3763,  -6.8542,  ..., -21.5278, -21.9906, -21.9159],\n",
            "         [-28.0730,  -6.6089,  -6.8185,  ..., -27.5369, -28.1858, -27.8626],\n",
            "         [-30.6885,  -6.5421, -10.9930,  ..., -30.0147, -30.4963, -30.3102],\n",
            "         ...,\n",
            "         [-28.4630,  -3.2069,  -7.9613,  ..., -27.7613, -28.3718, -28.3400],\n",
            "         [-29.3955,  -1.9470,  -4.6935,  ..., -28.8900, -29.5536, -29.4921],\n",
            "         [-29.7273,  -2.3630,  -7.3372,  ..., -29.1836, -29.8692, -29.5749]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    labels=train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"Without padding tokens\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZVtxa7FdhXE",
        "outputId": "24d87923-fd9b-42d5-f26b-b62056d34dfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With padding tokens in labels\n",
            "tensor(3.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-17.5048,  -3.3334,  -4.8966,  ..., -17.3239, -17.5362, -17.5236],\n",
            "         [-29.3313,  -7.5968,  -6.0506,  ..., -28.8467, -29.3566, -29.1388],\n",
            "         [-33.3023, -11.5635, -13.7315,  ..., -32.7689, -33.4021, -33.0907],\n",
            "         ...,\n",
            "         [-19.4768,   1.2223,  -2.6142,  ..., -19.1825, -19.5845, -19.5321],\n",
            "         [-20.8250,  -2.5398,  -5.6711,  ..., -20.4851, -20.8054, -20.7826],\n",
            "         [-15.5038,  -1.7199,  -3.8062,  ..., -15.2332, -15.5030, -15.4429]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    labels=train_dataset[id_to_choose]['labels'].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"With padding tokens in labels\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GokqRnCpmCch"
      },
      "source": [
        "### Some diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5r4VRnQEG2We",
        "outputId": "85d4a9af-df57-4c38-86cf-c7d2aefa2f2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.shared.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.relative_attention_bias.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.global_relative_attention_bias.weight False\n",
            "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.0.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.0.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.1.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.2.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.3.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.4.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.5.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.6.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.7.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.8.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.8.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.9.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.9.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.10.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.10.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.base_layer.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.k.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.base_layer.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.o.weight False\n",
            "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.0.layer_norm.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight False\n",
            "base_model.model.encoder.block.11.layer.1.DenseReluDense.wo.weight False\n",
            "base_model.model.encoder.block.11.layer.1.layer_norm.weight False\n",
            "base_model.model.encoder.final_layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\n",
            "base_model.model.decoder.block.0.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.0.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.0.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.0.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.1.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.1.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.1.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.1.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.2.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.2.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.2.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.2.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.3.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.3.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.3.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.3.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.4.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.4.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.4.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.4.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.5.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.5.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.5.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.5.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.6.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.6.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.6.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.6.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.7.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.7.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.7.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.7.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.8.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.8.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.8.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.8.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.8.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.9.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.9.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.9.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.9.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.9.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.10.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.10.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.10.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.10.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.10.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.k.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.0.SelfAttention.o.weight False\n",
            "base_model.model.decoder.block.11.layer.0.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.k.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.base_layer.weight False\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_A.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.lora_B.default.weight True\n",
            "base_model.model.decoder.block.11.layer.1.EncDecAttention.o.weight False\n",
            "base_model.model.decoder.block.11.layer.1.layer_norm.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight False\n",
            "base_model.model.decoder.block.11.layer.2.DenseReluDense.wo.weight False\n",
            "base_model.model.decoder.block.11.layer.2.layer_norm.weight False\n",
            "base_model.model.decoder.final_layer_norm.weight False\n",
            "base_model.model.lm_head.weight False\n"
          ]
        }
      ],
      "source": [
        "for name, param in trainer.model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtX_iCOBIPTp",
        "outputId": "b2ca3fce-bf5d-4a5e-fd3f-474d277a9c2f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForSeq2SeqLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LongT5ForConditionalGeneration(\n",
              "      (shared): Embedding(32128, 768)\n",
              "      (encoder): LongT5Stack(\n",
              "        (embed_tokens): Embedding(32128, 768)\n",
              "        (block): ModuleList(\n",
              "          (0): LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerTransientGlobalSelfAttention(\n",
              "                (TransientGlobalSelfAttention): LongT5TransientGlobalAttention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 12)\n",
              "                  (global_relative_attention_bias): Embedding(32, 12)\n",
              "                  (global_input_layer_norm): LongT5LayerNorm()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-11): 11 x LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerTransientGlobalSelfAttention(\n",
              "                (TransientGlobalSelfAttention): LongT5TransientGlobalAttention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (global_input_layer_norm): LongT5LayerNorm()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): LongT5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (decoder): LongT5Stack(\n",
              "        (embed_tokens): Embedding(32128, 768)\n",
              "        (block): ModuleList(\n",
              "          (0): LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerSelfAttention(\n",
              "                (SelfAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (relative_attention_bias): Embedding(32, 12)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerCrossAttention(\n",
              "                (EncDecAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "          (1-11): 11 x LongT5Block(\n",
              "            (layer): ModuleList(\n",
              "              (0): LongT5LayerSelfAttention(\n",
              "                (SelfAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (1): LongT5LayerCrossAttention(\n",
              "                (EncDecAttention): LongT5Attention(\n",
              "                  (q): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "                  (v): lora.Linear(\n",
              "                    (base_layer): Linear(in_features=768, out_features=768, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=768, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=768, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "              (2): LongT5LayerFF(\n",
              "                (DenseReluDense): LongT5DenseGatedActDense(\n",
              "                  (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wi_1): Linear(in_features=768, out_features=2048, bias=False)\n",
              "                  (wo): Linear(in_features=2048, out_features=768, bias=False)\n",
              "                  (dropout): Dropout(p=0.1, inplace=False)\n",
              "                  (act): NewGELUActivation()\n",
              "                )\n",
              "                (layer_norm): LongT5LayerNorm()\n",
              "                (dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (final_layer_norm): LongT5LayerNorm()\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqGg5zM_mH3o"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP9yWIPazB0K",
        "outputId": "62cc3a48-4c04-4984-d5a4-772484da28e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from checkpoint: /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-2750\n"
          ]
        }
      ],
      "source": [
        "# (If needed) Load model from checkpoint\n",
        "latest_checkpoint = max([int(f.split('-')[1]) for f in os.listdir(os.path.join(output_dir, checkpoint_path)) if f.startswith('checkpoint')])\n",
        "if latest_checkpoint:\n",
        "    resume_from_checkpoint = os.path.join(output_dir, checkpoint_path, f\"checkpoint-{latest_checkpoint}\")\n",
        "    print(\"Resuming from checkpoint:\", resume_from_checkpoint)\n",
        "else:\n",
        "    resume_from_checkpoint = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HvzKoAjO5_1V",
        "outputId": "db910c9d-1aa0-4cce-b27a-163590017632"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading model from /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-2750.\n",
            "***** Running training *****\n",
            "  Num examples = 11,350\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 5,676\n",
            "  Number of trainable parameters = 1,769,472\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 1\n",
            "  Continuing training from global step 2750\n",
            "  Will skip the first 1 epochs then the first 1331 batches in the first epoch.\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5676' max='5676' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5676/5676 5:09:12, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.843100</td>\n",
              "      <td>2.524143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.819500</td>\n",
              "      <td>2.502765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.773700</td>\n",
              "      <td>2.488067</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [6, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [6, 64, 128, 1], which does not match the required output shape [6, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3000\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3000/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3250\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3250/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3250/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3500\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3500/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3750\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3750/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-3750/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4000\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4000/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4250\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4250/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4250/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [6, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [6, 64, 128, 1], which does not match the required output shape [6, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4500\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4500/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4750\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4750/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-4750/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 200\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5000\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5000/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5000/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5250\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5250/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5250/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "Saving model checkpoint to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5500\n",
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5500/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora/checkpoint-5500/special_tokens_map.json\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [6, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [6, 64, 128, 1], which does not match the required output shape [6, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5676, training_loss=1.4375211676717896, metrics={'train_runtime': 18559.6412, 'train_samples_per_second': 2.446, 'train_steps_per_second': 0.306, 'total_flos': 4.516809378103296e+17, 'train_loss': 1.4375211676717896, 'epoch': 4.0})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the model\n",
        "trainer.train(resume_from_checkpoint=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "_LQxsV2infsn",
        "outputId": "a0267845-271b-4d4b-9cbf-bf7144b83eac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 8\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [8, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [8, 64, 128, 1], which does not match the required output shape [8, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [2, 64, 128, 1], which does not match the required output shape [2, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 2.6070263385772705,\n",
              " 'eval_runtime': 2.2912,\n",
              " 'eval_samples_per_second': 4.365,\n",
              " 'eval_steps_per_second': 0.873,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# evaluate on custom slice of train dataset\n",
        "trainer.evaluate(train_dataset.select(range(0, 10)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "xC6FnvyL5_1V",
        "outputId": "879d4e2b-4253-4fda-f2ce-953d489bdb4d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 10\n",
            "  Batch size = 1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 00:22]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 4.012439727783203,\n",
              " 'eval_runtime': 11.0355,\n",
              " 'eval_samples_per_second': 0.906,\n",
              " 'eval_steps_per_second': 0.906,\n",
              " 'epoch': 0.0}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# view results\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GcN42yAxF-s",
        "outputId": "e5fcd8b1-268e-4b5e-ed66-241708496666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0197,  0.0322, -0.0018,  ...,  0.0370, -0.0552,  0.0419],\n",
            "        [ 0.0526, -0.0724, -0.0003,  ...,  0.0095, -0.0972,  0.0021],\n",
            "        [ 0.0086,  0.0318,  0.0109,  ...,  0.0422,  0.0820,  0.0055],\n",
            "        ...,\n",
            "        [-0.0266,  0.0364,  0.0284,  ...,  0.0088,  0.0610,  0.0288],\n",
            "        [ 0.0816, -0.0670,  0.0038,  ...,  0.0564, -0.0580, -0.0146],\n",
            "        [-0.0146,  0.0368, -0.0205,  ..., -0.0566,  0.0114, -0.0238]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([16, 768])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1YHELDKxF-y",
        "outputId": "0e7b721d-8a9f-4ba8-981d-871466db7d0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0539, -0.0352,  0.0256,  ...,  0.0116, -0.0168, -0.0118],\n",
            "        [ 0.0160, -0.0318,  0.0142,  ...,  0.0199, -0.0460, -0.0028],\n",
            "        [ 0.0271, -0.0188,  0.0189,  ...,  0.0334, -0.0603, -0.0007],\n",
            "        ...,\n",
            "        [ 0.0061,  0.0071,  0.0259,  ..., -0.0034,  0.0119,  0.0226],\n",
            "        [-0.0341, -0.0029, -0.0074,  ..., -0.0272,  0.0215,  0.0012],\n",
            "        [-0.0263, -0.0530, -0.0163,  ..., -0.0220,  0.0195,  0.0521]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([768, 16])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gPBeNMXpSi5",
        "outputId": "60d81509-3ac0-409b-cffd-aeba914587bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 64, 128, 1], which does not match the required output shape [1, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('There was no significant difference in blood pressure or pulse rate between '\n",
            " 'the 2 groups. Conclusions This meta- analysis shows that there is no '\n",
            " 'evidence to support the use of angiotenain 1:20,000 as a local anesthesia '\n",
            " 'for tooth extraction')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ24qJjg83Vw",
        "outputId": "577d4cba-772d-40c6-f29f-410579ae7199"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"early_stopping\": true,\n",
            "  \"encoder_no_repeat_ngram_size\": 4,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"length_penalty\": 0.8,\n",
            "  \"max_length\": 512,\n",
            "  \"min_length\": 8,\n",
            "  \"no_repeat_ngram_size\": 3,\n",
            "  \"num_beams\": 2,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"repetition_penalty\": 3.5,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1517: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 64, 128, 1], which does not match the required output shape [1, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('The results of this meta- analysis showed that the use of a single dose of '\n",
            " 'lidocoine was associated with an increase in heart rate. However, there were '\n",
            " 'no significant differences between these two groups on pulse rates and blood '\n",
            " 'pressure.')\n",
            "('The most frequent complications in cardiovascular compromised patients after '\n",
            " 'dental local anaesthesia with a vasoconstrictor agent were disclosed in ECG '\n",
            " 'arrhythmias. Most of these disclosed arrhythmias were clinical ly '\n",
            " 'insignificant. The use of  4 ampules of lignocaine with epinephrine 1:100000 '\n",
            " 'as a dental anaesthetic seems to be relatively safe for cardiovascular '\n",
            " 'compromised patients')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 1\n",
        "inputs = train_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = trainer.model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "pprint(tokenizer.decode(train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnkEjEgy1f7I",
        "outputId": "6030fc72-4ab0-42b5-b2ff-3ddc17083be9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With padding tokens in labels\n",
            "tensor(2.8820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor([[[-16.2487,  -0.6006,  -1.9300,  ..., -15.8971, -16.3333, -16.1956],\n",
            "         [-19.4892,  -3.5078,  -2.8381,  ..., -19.1383, -19.5005, -19.3040],\n",
            "         [-16.9491,  -3.9170,  -5.3435,  ..., -16.6350, -16.9932, -16.8125],\n",
            "         ...,\n",
            "         [-15.2111,   0.2993,  -1.5088,  ..., -14.8921, -15.2493, -15.1783],\n",
            "         [ -9.2678,   1.8246,   0.2085,  ...,  -9.0547,  -9.3041,  -9.3049],\n",
            "         [-13.5367,   0.1113,  -1.5535,  ..., -13.3507, -13.5380, -13.5523]]],\n",
            "       device='cuda:0', grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "call_outputs = model(\n",
        "    inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    # labels=train_dataset[id_to_choose]['labels'][train_dataset[id_to_choose]['labels']!=label_pad_token_id].unsqueeze(0),\n",
        "    labels=train_dataset[id_to_choose]['labels'].unsqueeze(0).to(device),\n",
        ")\n",
        "print(\"With padding tokens in labels\")\n",
        "print(call_outputs.loss)\n",
        "print(call_outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtmkk7U85_1V",
        "outputId": "529b88fe-7de9-4feb-92d4-fa363725531f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tokenizer config file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/special_tokens_map.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/266 final project/notebooks/peft_training_history/longt5-qlora-4-epochs-final/tokenizer.json')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save model\n",
        "# trainer.save_model(os.path.join(output_dir, \"longt5-qlora-final\"))\n",
        "final_save_dir = \"longt5-qlora-4-epochs-final\"\n",
        "trainer.model.save_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "tokenizer.save_pretrained(os.path.join(output_dir, final_save_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksDikXnWJQzI"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414,
          "referenced_widgets": [
            "99089c221cb34497951b6c5f8c63f067",
            "cf72c9743da14d3eb074a494fe25c944",
            "760619f86d784c9b9a254973c84ca08a",
            "1fbbf3b5c8294de7b3e02afabbce326f",
            "f9f1de13dccc40f08ddf00716e1ced04",
            "466cbdd04ab545c4b155ac30620022c2",
            "9108db71ef794d569f13f68c1ea9e29a",
            "7df896b2923c40db84ea5d38249b60d7",
            "a7b7865fffc54eba9a8aa672d7b5608e",
            "2fb855142b474bb3849cced3522fa447",
            "b828990c7a204795b04b3310a65c3158"
          ]
        },
        "id": "gnQNr36HJQzI",
        "outputId": "a348db50-f57f-4782-8923-80e82c82e41f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  review_id                                            summary\n",
              "0  28514886  Breast-fed infants typically have an intestina...\n",
              "1  18842808  No adverse effects were observed . The effects...\n",
              "2  24297836  Autonomic cardiovascular dysfunction accompani...\n",
              "3  32367221  Abstract . Pain on kneeling , KT-1000 measured...\n",
              "4  25038833  RESULTS Results of the Name-Face Association T..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6b376194-83fd-47ba-91fd-f910352bdbbe\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28514886</td>\n",
              "      <td>Breast-fed infants typically have an intestina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18842808</td>\n",
              "      <td>No adverse effects were observed . The effects...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24297836</td>\n",
              "      <td>Autonomic cardiovascular dysfunction accompani...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32367221</td>\n",
              "      <td>Abstract . Pain on kneeling , KT-1000 measured...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25038833</td>\n",
              "      <td>RESULTS Results of the Name-Face Association T...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6b376194-83fd-47ba-91fd-f910352bdbbe')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6b376194-83fd-47ba-91fd-f910352bdbbe button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6b376194-83fd-47ba-91fd-f910352bdbbe');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-583f065a-a6ef-49ac-8856-e764de19602b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-583f065a-a6ef-49ac-8856-e764de19602b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-583f065a-a6ef-49ac-8856-e764de19602b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/2021 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99089c221cb34497951b6c5f8c63f067"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3860: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys of tokenized dataset: ['review_id', 'pmid', 'title', 'abstract', 'target_text', 'background', 'input_text', 'input_ids', 'attention_mask', 'labels']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "label_pad_token_id = -100  # special label token that gets ignored in loss calculations\n",
        "\n",
        "test_data_path = os.path.join(source_data_path, 'test_tokenized_dataset')\n",
        "if os.path.exists(test_data_path):\n",
        "    test_dataset = Dataset.load_from_disk(test_data_path)\n",
        "else:\n",
        "    test_dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"validation\")  # test set does not have target summaries\n",
        "    test_dataset.save_to_disk(test_data_path)\n",
        "\n",
        "# Load Kmeans extraction\n",
        "df_kmeans_extractive_test = pd.read_csv(\n",
        "    # \"../experiment_1/BioBERT_K_Means_extractive.csv\",\n",
        "    \"/content/drive/MyDrive/266 final project/notebooks/BioBERT_K_Means_extractive.csv\",\n",
        "    index_col=0,\n",
        "    dtype={'review_id': str, 'summary': str}\n",
        ")\n",
        "display(df_kmeans_extractive_test.head())\n",
        "\n",
        "# df_kmeans_extractive_test's summary gets appended as \"input_text\" in test_dataset, but in the same order as test_dataset\n",
        "input_text_ordered = [\n",
        "    df_kmeans_extractive_test[df_kmeans_extractive_test['review_id'] == rid]['summary'].tolist()[0]\n",
        "    for rid in test_dataset['review_id']\n",
        "]\n",
        "test_dataset = test_dataset.add_column('input_text', input_text_ordered)\n",
        "\n",
        "# rename \"target\" to \"target_text\" to match training dataset\n",
        "test_dataset = test_dataset.rename_column('target', 'target_text')\n",
        "\n",
        "# Tokenize data\n",
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=max_input_token_length)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(text_target=examples['target_text'], padding='max_length', truncation=True, max_length=256)\n",
        "        labels[\"input_ids\"] = [\n",
        "            [(l if l != tokenizer.pad_token_id else label_pad_token_id) for l in label] for label in labels[\"input_ids\"]\n",
        "        ]\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, num_proc=4)\n",
        "tokenized_test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "print(f\"Keys of tokenized dataset: {list(tokenized_test_dataset.features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JSXVwzXJQzI"
      },
      "outputs": [],
      "source": [
        "# Load trained model!\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "\n",
        "final_save_dir = \"longt5-qlora-4-epochs-final\"\n",
        "config = PeftConfig.from_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "tokenizer = AutoTokenizer.from_pretrained(os.path.join(output_dir, final_save_dir))\n",
        "\n",
        "model = PeftModel.from_pretrained(model, os.path.join(output_dir, final_save_dir)).to(device)\n",
        "model.eval();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TaMEWpnJQzJ",
        "outputId": "9bc33892-2565-4e1c-d699-8b4ce6e66f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.0356, -0.0230,  0.0462,  ...,  0.0218, -0.0343, -0.0341],\n",
            "        [ 0.0432, -0.0342,  0.0384,  ...,  0.0329, -0.0587, -0.0017],\n",
            "        [ 0.0164, -0.0138,  0.0030,  ...,  0.0236, -0.0770, -0.0249],\n",
            "        ...,\n",
            "        [-0.0009,  0.0047,  0.0288,  ...,  0.0109,  0.0118,  0.0214],\n",
            "        [-0.0283, -0.0051, -0.0120,  ..., -0.0308,  0.0138, -0.0052],\n",
            "        [-0.0233, -0.0423, -0.0324,  ..., -0.0317,  0.0105,  0.0576]],\n",
            "       device='cuda:0')\n",
            "torch.Size([768, 16])\n"
          ]
        }
      ],
      "source": [
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\"))\n",
        "print(model.get_parameter(\"encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\").shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yBLTgC8JQzJ",
        "outputId": "5f72419d-4ada-4b1b-efd9-12ef2be38d0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [1, 8192]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 64, 128, 1], which does not match the required output shape [1, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
            "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BACKGROUND\n",
            "('Home-based resistance exercise is commonly used for individuals who might '\n",
            " 'not have access or the ability to use traditional resistance exercise .\\n'\n",
            " 'However , the extent to which home-based resistance exercise can improve '\n",
            " 'both strength and functional ability has not been investigated in healthy '\n",
            " 'older individuals using a systematic analysis .')\n",
            "GENERATED\n",
            "('There was no evidence to support the use of a combination of resistance '\n",
            " 'training and home-based physical activity for improving muscle strength in '\n",
            " 'older adults.')\n",
            "TARGET\n",
            "('Overall, home-based resistance exercise can improve both strength and '\n",
            " 'functional ability, but the improvements are generally small. The intensity '\n",
            " 'of the exercises might not progress sufficiently enough to produce large '\n",
            " 'improvements in strength as a result of less supervision or a lack of '\n",
            " 'motivation to increase the intensity further')\n"
          ]
        }
      ],
      "source": [
        "# try inferring for a single example\n",
        "id_to_choose = 100\n",
        "inputs = tokenized_test_dataset[id_to_choose: id_to_choose + 1]\n",
        "output = model.generate(\n",
        "    input_ids=inputs[\"input_ids\"].to(device),\n",
        "    attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "    max_new_tokens=128,\n",
        "    num_beams=4,\n",
        ")\n",
        "print(\"BACKGROUND\")\n",
        "pprint(tokenized_test_dataset[\"background\"][id_to_choose])\n",
        "print(\"GENERATED\")\n",
        "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
        "print(\"TARGET\")\n",
        "pprint(tokenizer.decode(tokenized_test_dataset[id_to_choose]['labels'][tokenized_test_dataset[id_to_choose]['labels']!=label_pad_token_id], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWKGt7MFJQzJ",
        "outputId": "7389e938-5583-4f50-f4c4-bccac43dbf5f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['There was no evidence to support the use of a combination of resistance training and home-based physical activity for improving muscle strength in older adults.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# tokenizer batch decode\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "XryUOGtKg751"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0CnG_rCJQzJ",
        "outputId": "b95c3c68-9cf6-450d-d86f-3248064a69d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1011/1011 [00:05<00:00, 192.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved generated summaries to /content/drive/MyDrive/266 final project/notebooks/peft_training_history/generated_summaries/generated_summaries.csv\n"
          ]
        }
      ],
      "source": [
        "# Now generate for all test examples, save to disk for evaluation elsewhere\n",
        "\n",
        "def generate_and_save(dataset, save_path, batch_size=8):\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    # generate\n",
        "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "        subset_dataset = dataset.select(range(i, (i + batch_size) if (i + batch_size) < len(dataset) else len(dataset)))\n",
        "        inputs = subset_dataset[:]\n",
        "\n",
        "        # check if generated summaries already exist. check individual files\n",
        "        rows_to_keep = []\n",
        "        for idx, review_id in enumerate(subset_dataset['review_id']):\n",
        "            if os.path.exists(os.path.join(save_path, f\"{review_id}.txt\")):\n",
        "                continue\n",
        "            else:\n",
        "                rows_to_keep.append(idx)\n",
        "\n",
        "        if len(rows_to_keep) == 0:\n",
        "            continue\n",
        "\n",
        "        inputs = subset_dataset.select(rows_to_keep)[:]\n",
        "\n",
        "        output = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"].to(device),\n",
        "            attention_mask=inputs[\"attention_mask\"].to(device),\n",
        "            max_new_tokens=128,\n",
        "            num_beams=4,\n",
        "        )\n",
        "        generated_summaries = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
        "\n",
        "        # save individually\n",
        "        for review_id, summary in zip(subset_dataset['review_id'], generated_summaries):\n",
        "            with open(os.path.join(save_path, f\"{review_id}.txt\"), 'w') as f:\n",
        "                f.write(summary)\n",
        "\n",
        "    # save aggregated into csv\n",
        "    # open all files\n",
        "    all_generated_summaries = {}\n",
        "    for fpath in os.listdir(save_path):\n",
        "        if fpath.endswith('.txt'):\n",
        "            all_generated_summaries[fpath.split('.')[0]] = open(os.path.join(save_path, fpath), 'r').read()\n",
        "\n",
        "    all_generated_summaries_sorted = [all_generated_summaries[rid] for rid in dataset['review_id']]\n",
        "    df = pd.DataFrame({\n",
        "        'review_id': dataset['review_id'],\n",
        "        'summary': all_generated_summaries_sorted\n",
        "    })\n",
        "    df.to_csv(os.path.join(save_path, 'generated_summaries.csv'), index=False)\n",
        "    print(f\"Saved generated summaries to {os.path.join(save_path, 'generated_summaries.csv')}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "df_generated = generate_and_save(\n",
        "    tokenized_test_dataset,\n",
        "    f\"{output_dir}/generated_summaries\",\n",
        "    batch_size=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_generated.shape)\n",
        "df_generated"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        },
        "id": "mLOVced9hwD7",
        "outputId": "80a3873c-1128-4b85-c348-126e650dc750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2021, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     review_id                                            summary\n",
              "0     28514886  There was no statistically significant differe...\n",
              "1     18842808  RESULTS The results of this meta- analysis sug...\n",
              "2     24297836  Conclusions : This systematic review provides ...\n",
              "3     32367221  Conclusions : There is no evidence to support ...\n",
              "4     25038833  There was no evidence of a statistically signi...\n",
              "...        ...                                                ...\n",
              "2016  19776504  In conclusion, there is no evidence to support...\n",
              "2017  27505198  Conclusions : This systematic review suggests ...\n",
              "2018  25251296  There was no significant association between B...\n",
              "2019  23235652  There was no evidence to support the use of zi...\n",
              "2020  30058911  Conclusions : This meta- analysis suggests tha...\n",
              "\n",
              "[2021 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-63033025-0927-4084-8ca9-679467ca5e96\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>28514886</td>\n",
              "      <td>There was no statistically significant differe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18842808</td>\n",
              "      <td>RESULTS The results of this meta- analysis sug...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24297836</td>\n",
              "      <td>Conclusions : This systematic review provides ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>32367221</td>\n",
              "      <td>Conclusions : There is no evidence to support ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>25038833</td>\n",
              "      <td>There was no evidence of a statistically signi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2016</th>\n",
              "      <td>19776504</td>\n",
              "      <td>In conclusion, there is no evidence to support...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017</th>\n",
              "      <td>27505198</td>\n",
              "      <td>Conclusions : This systematic review suggests ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2018</th>\n",
              "      <td>25251296</td>\n",
              "      <td>There was no significant association between B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019</th>\n",
              "      <td>23235652</td>\n",
              "      <td>There was no evidence to support the use of zi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020</th>\n",
              "      <td>30058911</td>\n",
              "      <td>Conclusions : This meta- analysis suggests tha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2021 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-63033025-0927-4084-8ca9-679467ca5e96')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-63033025-0927-4084-8ca9-679467ca5e96 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-63033025-0927-4084-8ca9-679467ca5e96');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dd992129-2933-4b35-adc0-5be0b74f54e4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dd992129-2933-4b35-adc0-5be0b74f54e4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dd992129-2933-4b35-adc0-5be0b74f54e4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_1ddf578e-2dfa-403c-9501-39d09f048af9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_generated')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1ddf578e-2dfa-403c-9501-39d09f048af9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df_generated');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()\n",
        "except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "JTI0rDF7cPJB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "GokqRnCpmCch"
      ],
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99089c221cb34497951b6c5f8c63f067": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf72c9743da14d3eb074a494fe25c944",
              "IPY_MODEL_760619f86d784c9b9a254973c84ca08a",
              "IPY_MODEL_1fbbf3b5c8294de7b3e02afabbce326f"
            ],
            "layout": "IPY_MODEL_f9f1de13dccc40f08ddf00716e1ced04"
          }
        },
        "cf72c9743da14d3eb074a494fe25c944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_466cbdd04ab545c4b155ac30620022c2",
            "placeholder": "​",
            "style": "IPY_MODEL_9108db71ef794d569f13f68c1ea9e29a",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "760619f86d784c9b9a254973c84ca08a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df896b2923c40db84ea5d38249b60d7",
            "max": 2021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a7b7865fffc54eba9a8aa672d7b5608e",
            "value": 2021
          }
        },
        "1fbbf3b5c8294de7b3e02afabbce326f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fb855142b474bb3849cced3522fa447",
            "placeholder": "​",
            "style": "IPY_MODEL_b828990c7a204795b04b3310a65c3158",
            "value": " 2021/2021 [00:43&lt;00:00, 81.93 examples/s]"
          }
        },
        "f9f1de13dccc40f08ddf00716e1ced04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "466cbdd04ab545c4b155ac30620022c2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9108db71ef794d569f13f68c1ea9e29a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7df896b2923c40db84ea5d38249b60d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7b7865fffc54eba9a8aa672d7b5608e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fb855142b474bb3849cced3522fa447": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b828990c7a204795b04b3310a65c3158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}