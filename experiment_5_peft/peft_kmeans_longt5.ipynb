{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q peft bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device.\n"
     ]
    }
   ],
   "source": [
    "# Purpose of notebook: fine-tune LongT5 on exctracted sentences from studies, but using LoRA and bitsandbytes quantization\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    LongT5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device.\")\n",
    "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = \"0.0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device.\")\n",
    "    max_split_size_mb = 256  # Set the max_split_size_mb value (e.g., 512 MB)\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"max_split_size_mb:{max_split_size_mb}\"\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS/CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,472,192 || trainable%: 0.3560704289999583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_id = 'pszemraj/long-t5-tglobal-base-16384-book-summary'\n",
    "output_dir = \"training_history\"\n",
    "\n",
    "# bitsandbytes\n",
    "# Source notebooks:\n",
    "# - https://colab.research.google.com/drive/1Vvju5kOyBsDr7RX_YAvp6ZsSOoSMjhKD?usp=sharing#scrollTo=E0Nl5mWL0k2T\n",
    "# - https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing#scrollTo=HOWcL0LU3JYt\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = LongT5ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=bnb_config,  # enable when in CUDA\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# freeze the model\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# use PEFT\n",
    "\n",
    "# Load the config\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    ")\n",
    "base_model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(base_model)\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "ms2_dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"train\")\n",
    "\n",
    "# Load your CSV file\n",
    "df = pd.read_csv('../experiment_1/biobert_extractive_only_training_dataset.csv.gz', compression='gzip')\n",
    "\n",
    "# # ---- not available yet. in the meantime:\n",
    "# all_extracted_summaries = []\n",
    "# for fpath in os.listdir('../experiment_1/biobert_extractive_only_training_dataset'):\n",
    "#     all_extracted_summaries.append(\n",
    "#         pickle.load(open(os.path.join('../experiment_1/biobert_extractive_only_training_dataset', fpath), 'rb'))\n",
    "#     )\n",
    "# df = pd.DataFrame(all_extracted_summaries)\n",
    "# # ----\n",
    "\n",
    "input_texts = df['summary'].tolist()\n",
    "\n",
    "# target texts come from ms2 dataset. match on df's review_id for order\n",
    "target_texts = [\n",
    "    ms2_dataset[ms2_dataset['review_id'].index(str(i))]['target'] for i in df[\"review_id\"]\n",
    "]\n",
    "\n",
    "# Tokenize data\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=512)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['target_text'], padding='max_length', truncation=True, max_length=128)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "dataset = Dataset.from_dict({'input_text': input_texts, 'target_text': target_texts})\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Split the dataset\n",
    "shuffle_dataset = tokenized_datasets.shuffle(seed=42)\n",
    "train_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10))\n",
    "val_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10, len(tokenized_datasets)))\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"longt5-qlora\"),\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Adjust batch size according to memory constraints\n",
    "    evaluation_strategy=\"steps\",  # or, \"epoch\" ?\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-4,\n",
    "    logging_dir=os.path.join(output_dir, \"longt5-qlora\", \"logs\"),\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device\n",
    "# model.hf_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This study aims to assess the efficacy of an electric cigarette, or '\n",
      " 'e-cigarette, in preventing and reducing smoking among adult men. The results '\n",
      " 'suggest that this device may be useful for those who are not ready to quit '\n",
      " 'but still want to reduce their cigarette consumption. In addition, it may '\n",
      " 'help those who do not intend to quit because it can deliver large amounts of '\n",
      " 'nicotine without causing side effects.')\n",
      "('The use of the EC can reduce the number of cigarettes smoked and withdrawal '\n",
      " 'symptoms , but the AEs reported are mainly related to a short period of use '\n",
      " '.')\n"
     ]
    }
   ],
   "source": [
    "# try inferring for a single example\n",
    "id_to_choose = 1\n",
    "inputs = tokenizer(dataset[id_to_choose]['input_text'], return_tensors='pt').to(device)\n",
    "# output = base_model.generate(**inputs, max_new_tokens=256, num_beams=4)\n",
    "output = model.generate(**inputs, max_new_tokens=256, num_beams=4)\n",
    "# output = trainer.model.generate(**inputs, max_new_tokens=256, num_beams=4)\n",
    "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "pprint(dataset[id_to_choose][\"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f111cacf6f344c369221a13f23a37778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/transformers/modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 209.8985, 'train_samples_per_second': 1.486, 'train_steps_per_second': 0.743, 'train_loss': 13.549718612279648, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=156, training_loss=13.549718612279648, metrics={'train_runtime': 209.8985, 'train_samples_per_second': 1.486, 'train_steps_per_second': 0.743, 'train_loss': 13.549718612279648, 'epoch': 3.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/transformers/modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bc1d2c42184a7898609da28cdfdc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 11.186555862426758,\n",
       " 'eval_runtime': 21.3928,\n",
       " 'eval_samples_per_second': 1.215,\n",
       " 'eval_steps_per_second': 0.187,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view results\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(os.path.join(output_dir, \"longt5-qlora-final\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
