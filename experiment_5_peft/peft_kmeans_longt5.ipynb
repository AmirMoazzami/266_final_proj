{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q peft bitsandbytes accelerate datasets tensorboardX loralib\n",
    "!pip install -q --upgrade git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device.\n"
     ]
    }
   ],
   "source": [
    "# Purpose of notebook: fine-tune LongT5 on exctracted sentences from studies, but using LoRA and bitsandbytes quantization\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    LongT5ForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device.\")\n",
    "    os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = \"0.0\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device.\")\n",
    "    max_split_size_mb = 256  # Set the max_split_size_mb value (e.g., 512 MB)\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"max_split_size_mb:{max_split_size_mb}\"\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS/CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11350\n",
      "2838\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "model_id = 'pszemraj/long-t5-tglobal-base-16384-book-summary'\n",
    "\n",
    "# Local\n",
    "output_dir = \"training_history\"\n",
    "extracted_file_path = '../experiment_1/biobert_extractive_only_training_dataset.csv.gz'\n",
    "source_data_path = \"data\"\n",
    "\n",
    "# Colab\n",
    "# output_dir = \"/content/drive/MyDrive/266 final project/notebooks/peft_training_history\"\n",
    "# extracted_file_path = '/content/drive/MyDrive/266 final project/notebooks/biobert_extractive_only_training_dataset.csv.gz'\n",
    "# source_data_path = \"/content/drive/MyDrive/266 final project/notebooks/peft_training_history/data\"\n",
    "\n",
    "# longT5 max token length is 16384, let's 1/2 that\n",
    "max_input_token_length = 8192\n",
    "\n",
    "# END CONFIG\n",
    "\n",
    "train_data_path = os.path.join(source_data_path, 'train_tokenized_dataset')\n",
    "val_data_path = os.path.join(source_data_path, 'val_tokenized_dataset')\n",
    "\n",
    "if os.path.exists(train_data_path) and os.path.exists(val_data_path):\n",
    "    train_dataset = Dataset.load_from_disk(train_data_path)\n",
    "    val_dataset = Dataset.load_from_disk(val_data_path)\n",
    "\n",
    "else:\n",
    "    ms2_dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"train\")\n",
    "\n",
    "    # Load your CSV file\n",
    "    df = pd.read_csv(extracted_file_path, compression='gzip')\n",
    "\n",
    "    # # ---- not available yet. in the meantime:\n",
    "    # all_extracted_summaries = []\n",
    "    # for fpath in os.listdir('../experiment_1/biobert_extractive_only_training_dataset'):\n",
    "    #     all_extracted_summaries.append(\n",
    "    #         pickle.load(open(os.path.join('../experiment_1/biobert_extractive_only_training_dataset', fpath), 'rb'))\n",
    "    #     )\n",
    "    # df = pd.DataFrame(all_extracted_summaries)\n",
    "    # # ----\n",
    "\n",
    "    target_texts = ms2_dataset['target']\n",
    "    input_texts = [\n",
    "        df[df['review_id'] == int(i)]['summary'].tolist()[0] for i in ms2_dataset['review_id']\n",
    "    ]\n",
    "    dataset = Dataset.from_dict({'input_text': input_texts, 'target_text': target_texts})\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "    # Tokenize data\n",
    "    def tokenize_function(examples):\n",
    "        model_inputs = tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=max_input_token_length)\n",
    "        labels = tokenizer(text_target=examples['target_text'], padding='max_length', truncation=True, max_length=256)\n",
    "        labels[\"input_ids\"] = [\n",
    "            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
    "        ]\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"input_text\", \"target_text\"])\n",
    "    print(f\"Keys of tokenized dataset: {list(tokenized_datasets.features)}\")\n",
    "\n",
    "    label_pad_token_id = -100\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        model=model_id,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "    )\n",
    "\n",
    "    # Split the dataset\n",
    "    shuffle_dataset = tokenized_datasets.shuffle(seed=42)\n",
    "    shuffle_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    train_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10))\n",
    "    val_dataset = shuffle_dataset.select(range(len(tokenized_datasets) * 8 // 10, len(tokenized_datasets)))\n",
    "\n",
    "    # save to disk for easy loading\n",
    "    train_dataset.save_to_disk(train_data_path)\n",
    "    val_dataset.save_to_disk(val_data_path)\n",
    "\n",
    "print(train_dataset.num_rows)\n",
    "print(val_dataset.num_rows)\n",
    "\n",
    "type(shuffle_dataset[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11350.000000\n",
       "mean        86.599912\n",
       "std         57.467615\n",
       "min          9.000000\n",
       "25%         44.000000\n",
       "50%         70.000000\n",
       "75%        114.000000\n",
       "max        256.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% percentile is 214.0\n"
     ]
    }
   ],
   "source": [
    "# ANALYSIS: what's the distribution of non-padding tokens in train_dataset[\"labels\"]?\n",
    "all_tokens = train_dataset[\"labels\"].numpy()\n",
    "non_pad_token_counts = np.array([len(np.where(tokens != label_pad_token_id)[0]) for tokens in all_tokens])\n",
    "# distribution of non_pad_token_counts\n",
    "display(pd.Series(non_pad_token_counts).describe())\n",
    "\n",
    "# what's the 95% percentile?\n",
    "print(\"95% percentile is\", np.percentile(non_pad_token_counts, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    11350.000000\n",
       "mean      3661.722291\n",
       "std       2308.642882\n",
       "min         71.000000\n",
       "25%       1855.000000\n",
       "50%       3050.000000\n",
       "75%       5047.000000\n",
       "max       8192.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% percentile is 8192.0\n",
      "If we truncated input_ids to 8192, this is the percentile it'll be at (anything at a higher percentile could risk losing information): 0.9473568281938326\n",
      "8192.0\n"
     ]
    }
   ],
   "source": [
    "# ANALYSIS: what's the distribution of non-padding tokens in train_dataset[\"input_ids\"]?\n",
    "all_tokens = train_dataset[\"input_ids\"].numpy()\n",
    "non_pad_token_counts = np.array([len(np.where(tokens != 0)[0]) for tokens in all_tokens])\n",
    "# distribution of non_pad_token_counts\n",
    "display(pd.Series(non_pad_token_counts).describe())\n",
    "\n",
    "# what's the 95% percentile?\n",
    "print(\"95% percentile is\", np.percentile(non_pad_token_counts, 95))\n",
    "\n",
    "# which percentile is \"8192 non-padding tokens\" on?\n",
    "print(\n",
    "    \"If we truncated input_ids to 8192, this is the percentile it'll be at (anything at a higher percentile could risk losing information):\",\n",
    "    (perc_8192 := pd.Series(non_pad_token_counts).rank(pct=True)[np.where(non_pad_token_counts <= 8192)[0]].max())\n",
    ")\n",
    "# confirm\n",
    "print(np.percentile(non_pad_token_counts, perc_8192 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 884,736 || all params: 248,472,192 || trainable%: 0.3560704289999583\n",
      "Tensorboard log path: training_history/longt5-qlora/logs\n",
      "run this in terminal: tensorboard --logdir training_history/longt5-qlora/logs\n"
     ]
    }
   ],
   "source": [
    "# bitsandbytes\n",
    "# Source notebooks:\n",
    "# - https://colab.research.google.com/drive/1Vvju5kOyBsDr7RX_YAvp6ZsSOoSMjhKD?usp=sharing#scrollTo=E0Nl5mWL0k2T\n",
    "# - https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf?usp=sharing#scrollTo=HOWcL0LU3JYt\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    # load_in_8bit=True,\n",
    ")\n",
    "\n",
    "base_model = LongT5ForConditionalGeneration.from_pretrained(model_id)\n",
    "model = LongT5ForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config=bnb_config,  # enable when in CUDA\n",
    "    # load_in_8bit=True,\n",
    "    # device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# BUG: `model` has its embeddings reinitiated. Copy over from `base_model` but retain data type\n",
    "# reinited_params = ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
    "# for param_name in reinited_params:\n",
    "#     model_param = model.get_parameter(param_name)\n",
    "#     base_model_param = base_model.get_parameter(param_name)\n",
    "#     model_param.data = base_model_param.data.to(model_param.dtype)\n",
    "\n",
    "# use PEFT LoRA\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    # target_modules=[\"q\", \"v\", \"k\"],\n",
    "    # target_modules=[\"q\", \"v\"],\n",
    "    target_modules=[\"q\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)  # enable for 4bit or 8bit quantization\n",
    "model.enable_input_require_grads()\n",
    "model = get_peft_model(model, lora_config)\n",
    "# Fix from this GitHub issue: https://github.com/huggingface/peft/issues/522#issuecomment-1705989330\n",
    "model.base_model.model.encoder.enable_input_require_grads()\n",
    "model.base_model.model.decoder.enable_input_require_grads()\n",
    "model.train()\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Training arguments\n",
    "logpath = os.path.join(output_dir, \"longt5-qlora\", \"logs\")\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=os.path.join(output_dir, \"longt5-qlora\"),\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=2,  # Adjust batch size according to memory constraints\n",
    "#     evaluation_strategy=\"steps\",  # or, \"epoch\" ?\n",
    "#     save_steps=500,\n",
    "#     eval_steps=500,\n",
    "#     max_steps=100,  # For debugging\n",
    "#     learning_rate=1e-4,\n",
    "#     logging_dir=os.path.join(output_dir, \"longt5-qlora\", \"logs\"),\n",
    "#     logging_steps=50,\n",
    "#     # # for 4bit or 8bit quantization\n",
    "#     # fp16=True,\n",
    "#     # optim=\"paged_adamw_8bit\",  # default: adamw_torch\n",
    "# )\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=os.path.join(output_dir, \"longt5-qlora\"),\n",
    "    evaluation_strategy=\"steps\",  # alternatively, \"epoch\"\n",
    "    learning_rate=1e-4,\n",
    "    logging_dir=logpath,\n",
    "    report_to=\"tensorboard\",\n",
    "    save_strategy=\"steps\",\n",
    "\n",
    "    # FOR REAL TRAINING\n",
    "    # num_train_epochs=3,\n",
    "    # auto_find_batch_size=True,\n",
    "    # eval_steps=500,\n",
    "    # logging_steps=100,\n",
    "    # save_steps=200,\n",
    "\n",
    "    # FOR DEBUGGING\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    eval_steps=2,\n",
    "    max_steps=10,\n",
    "    logging_steps=5,\n",
    "    save_steps=5,\n",
    "\n",
    "    # FOR 4BIT OR 8BIT QUANTIZATION\n",
    "    # fp16=True,\n",
    "    # optim=\"paged_adamw_8bit\",  # default: adamw_torch\n",
    ")\n",
    "\n",
    "print(\"Tensorboard log path:\", logpath)\n",
    "print(\"run this in terminal: tensorboard --logdir\", logpath)\n",
    "\n",
    "# Initialize Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     tokenizer=tokenizer,\n",
    "# )\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=val_dataset,\n",
    "    # select random subset of val_dataset for evaluation. shuffle deterministically\n",
    "    eval_dataset=val_dataset.shuffle(seed=42).select(range(100)),\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dtype of all parameters, get unique types\n",
    "param_dtypes = [param.dtype for param in model.parameters()]\n",
    "unique_dtypes = np.unique(param_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.5561,  0.4233,  0.8544,  ..., -0.9618,  0.6647,  0.9398],\n",
       "        [ 0.4269,  1.6681,  4.5766,  ..., -2.2274, -0.5151,  2.1782],\n",
       "        [-5.4195, -2.4177, -0.8740,  ..., -0.2788, -1.3139, -1.5880],\n",
       "        ...,\n",
       "        [ 1.5533,  0.5635,  1.6218,  ...,  1.9036,  0.7348,  0.1447],\n",
       "        [ 0.2494,  0.8528, -0.6396,  ...,  0.1166, -1.1269,  0.8604],\n",
       "        [ 0.8795, -0.3369, -1.7056,  ...,  0.4987,  1.2487,  0.6472]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_parameter(\"encoder.embed_tokens.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_parameter(\"encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()  # Colab\n",
    "torch.mps.empty_cache()  # MPS\n",
    "\n",
    "model.device\n",
    "# model.hf_device_map  # enable for 4bit or 8bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try inferring for a single example\n",
    "id_to_choose = 1\n",
    "inputs = tokenizer(dataset[id_to_choose]['input_text'], return_tensors='pt').to(device)\n",
    "labels = tokenizer(dataset[id_to_choose]['target_text'], return_tensors='pt').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = base_model.generate(**inputs, max_new_tokens=256, num_beams=4)\n",
    "output = model.generate(**inputs, max_new_tokens=256, num_beams=4)\n",
    "# output = trainer.model.generate(**inputs, max_new_tokens=256, num_beams=4)\n",
    "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "pprint(dataset[id_to_choose][\"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/torch/utils/checkpoint.py:441: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/torch/utils/checkpoint.py:73: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:74: UserWarning: MPS: The constant padding of more than 3 dimensions is not currently supported natively. It uses View Ops default implementation to run. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Pad.mm:474.)\n",
      "  x = nn.functional.pad(x, pad=pad, mode=\"constant\", value=pad_value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006]) torch.int64 mps:0\n",
      "tensor torch.Size([1, 8, 12, 128, 446]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n",
      "check_backward_validity\n",
      "len(inputs) 11\n",
      "tensor torch.Size([1, 228, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 228]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1006, 768]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 1, 1, 1006]) torch.float32 mps:0\n",
      "tensor torch.Size([1, 12, 228, 1006]) torch.float32 mps:0\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor NoneType\n",
      "non-tensor bool\n",
      "non-tensor bool\n",
      "end check_backward_validity\n"
     ]
    }
   ],
   "source": [
    "call_outputs = model(**inputs, labels=labels['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.5540, device='mps:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16384])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_sample_input_ids = train_dataset[\"input_ids\"][:1].to(device)\n",
    "one_sample_attention_mask = train_dataset[\"attention_mask\"][:1].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(one_sample_input_ids, attention_mask=one_sample_attention_mask, return_dict=True)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.shared.weight False\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.relative_attention_bias.weight False\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.global_relative_attention_bias.weight False\n",
      "base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.0.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.0.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.0.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.1.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.1.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.1.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.2.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.2.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.2.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.3.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.3.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.3.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.4.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.4.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.4.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.5.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.5.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.5.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.6.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.6.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.6.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.7.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.7.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.7.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.8.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.8.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.8.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.9.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.9.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.9.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.10.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.10.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.10.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.weight False\n",
      "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.k.weight False\n",
      "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.v.weight False\n",
      "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.o.weight False\n",
      "base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.global_input_layer_norm.weight False\n",
      "base_model.model.encoder.block.11.layer.0.layer_norm.weight False\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_0.weight False\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wi_1.weight False\n",
      "base_model.model.encoder.block.11.layer.1.DenseReluDense.wo.weight False\n",
      "base_model.model.encoder.block.11.layer.1.layer_norm.weight False\n",
      "base_model.model.encoder.final_layer_norm.weight False\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight False\n",
      "base_model.model.decoder.block.0.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.0.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.0.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.0.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.0.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.1.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.1.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.1.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.1.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.1.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.1.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.2.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.2.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.2.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.2.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.2.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.2.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.3.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.3.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.3.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.3.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.3.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.3.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.4.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.4.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.4.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.4.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.4.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.4.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.5.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.5.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.5.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.5.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.5.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.5.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.6.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.6.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.6.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.6.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.6.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.6.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.7.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.7.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.7.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.7.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.7.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.7.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.8.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.8.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.8.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.8.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.8.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.8.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.9.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.9.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.9.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.9.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.9.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.9.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.10.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.10.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.10.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.10.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.10.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.10.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.weight False\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.k.weight False\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.v.weight False\n",
      "base_model.model.decoder.block.11.layer.0.SelfAttention.o.weight False\n",
      "base_model.model.decoder.block.11.layer.0.layer_norm.weight False\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.weight False\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight True\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight True\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.k.weight False\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.v.weight False\n",
      "base_model.model.decoder.block.11.layer.1.EncDecAttention.o.weight False\n",
      "base_model.model.decoder.block.11.layer.1.layer_norm.weight False\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_0.weight False\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wi_1.weight False\n",
      "base_model.model.decoder.block.11.layer.2.DenseReluDense.wo.weight False\n",
      "base_model.model.decoder.block.11.layer.2.layer_norm.weight False\n",
      "base_model.model.decoder.final_layer_norm.weight False\n",
      "base_model.model.lm_head.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, param in trainer.model.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor without grad_fn: base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.0.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.1.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.2.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.3.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.4.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.5.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.6.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.7.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.8.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.9.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.10.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.encoder.block.11.layer.0.TransientGlobalSelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.0.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.0.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.1.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.1.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.2.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.2.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.3.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.3.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.4.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.4.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.5.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.5.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.6.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.6.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.7.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.7.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.8.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.8.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.9.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.9.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.10.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.10.layer.1.EncDecAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.11.layer.0.SelfAttention.q.lora_B.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_A.default.weight\n",
      "Tensor without grad_fn: base_model.model.decoder.block.11.layer.1.EncDecAttention.q.lora_B.default.weight\n"
     ]
    }
   ],
   "source": [
    "def find_tensor_without_grad_fn(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.grad is None:\n",
    "            print(f\"Tensor without grad_fn: {name}\")\n",
    "\n",
    "find_tensor_without_grad_fn(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (If needed) Load model from checkpoint\n",
    "latest_checkpoint = max([int(f.split('-')[1]) for f in os.listdir(os.path.join(output_dir, checkpoint_path)) if f.startswith('checkpoint')])\n",
    "if latest_checkpoint:\n",
    "    resume_from_checkpoint = os.path.join(output_dir, checkpoint_path, f\"checkpoint-{latest_checkpoint}\")\n",
    "    print(\"Resuming from checkpoint:\", resume_from_checkpoint)\n",
    "else:\n",
    "    resume_from_checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b53f9d6591146a7900bf3f52af637ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/torch/utils/checkpoint.py:441: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/transformers/modeling_utils.py:865: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelenghoekhor/Downloads/pytorch-test/env/lib/python3.8/site-packages/transformers/modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bc1d2c42184a7898609da28cdfdc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 11.186555862426758,\n",
       " 'eval_runtime': 21.3928,\n",
       " 'eval_samples_per_second': 1.215,\n",
       " 'eval_steps_per_second': 0.187,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view results\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on the same example\n",
    "id_to_choose = 1\n",
    "inputs = tokenizer(dataset[id_to_choose]['input_text'], return_tensors='pt').to(device)\n",
    "output = model.generate(**inputs, max_new_tokens=256, num_beams=4)\n",
    "pprint(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "pprint(dataset[id_to_choose][\"target_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(os.path.join(output_dir, \"longt5-qlora-final\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
