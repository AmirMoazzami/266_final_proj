{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdce99a",
   "metadata": {},
   "source": [
    "***Extractive step: BioBERT, K means***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a512c38",
   "metadata": {},
   "source": [
    "pipeline:\n",
    "I use BioBERT for embdeddings\n",
    "\n",
    "I use Spacy for sentence segmentation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Check if MPS is available and set the device accordingly\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "    \n",
    "\n",
    "# Load the dataset and cut down \n",
    "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\n",
    "# Use select to create a subset\n",
    "# dataset = dataset.select(range(20,30))  \n",
    "\n",
    "\n",
    "# Initialize the BioBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1').to(device)\n",
    "\n",
    "\n",
    "def bert_sentence_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # Move inputs to the device\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Move outputs back to CPU\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "#this is fixed : each selected sentence is from a different cluster\n",
    "def select_top_sentences(sentences, embeddings, n_sentences=5):\n",
    "    if len(sentences) <= n_sentences:\n",
    "        return ' '.join(sentences)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_sentences, n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "\n",
    "    # Initialize a set to store indices of selected sentences\n",
    "    selected_indices = set()\n",
    "    top_sentences = []\n",
    "\n",
    "    for i in range(n_sentences):\n",
    "        # Calculate distances of all sentences from the i-th centroid\n",
    "        distances = np.linalg.norm(embeddings - kmeans.cluster_centers_[i], axis=1)\n",
    "\n",
    "        # Sort the sentences by their distance from the centroid\n",
    "        sorted_indices = np.argsort(distances)\n",
    "\n",
    "        # Find the closest sentence that hasn't been selected yet\n",
    "        for index in sorted_indices:\n",
    "            if index not in selected_indices:\n",
    "                selected_indices.add(index)\n",
    "                top_sentences.append(sentences[index])\n",
    "                break\n",
    "\n",
    "    return ' '.join(top_sentences)\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    review_id = row['review_id']\n",
    "    abstract_list = row['abstract']\n",
    "    combined_summary = ''\n",
    "\n",
    "    for abstract in abstract_list:\n",
    "        # Check if the abstract is a string; if not, join it into a single string\n",
    "        abstract_text = ' '.join(abstract) if isinstance(abstract, list) else abstract\n",
    "\n",
    "        # Use SpaCy for sentence segmentation\n",
    "        doc = nlp(abstract_text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "        # Generate embeddings for each sentence\n",
    "        embeddings = bert_sentence_embeddings(sentences)\n",
    "        # Select the top sentences from these embeddings\n",
    "        summary = select_top_sentences(sentences, embeddings)\n",
    "\n",
    "        # Combine the summaries from each abstract\n",
    "        combined_summary += summary + ' '\n",
    "\n",
    "    return {\"review_id\": review_id, \"summary\": combined_summary.strip()}\n",
    "\n",
    "# Apply the function to each element of the dataset\n",
    "summaries_dataset = dataset.map(process_row)\n",
    "\n",
    "# Saving the dataset\n",
    "import pickle\n",
    "with open('summaries_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(summaries_dataset, file)\n",
    "\n",
    "# # Convert to pandas DataFrame\n",
    "# df = pd.DataFrame(summaries_dataset)\n",
    "# df = df[['review_id', 'summary']]\n",
    "# # Save to CSV\n",
    "# csv_file_path = 'test.csv'  # Update with your desired file path\n",
    "# df.to_csv(csv_file_path, index=True)\n",
    "\n",
    "# print(f\"Saved summaries to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c8017",
   "metadata": {},
   "source": [
    "***abstractive step: Long T5***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024bf9c",
   "metadata": {},
   "source": [
    "this is without batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6640cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.cuda.amp import autocast\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# # Set the device to MPS if available, else CPU\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# # Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fine-tuned for summarization\n",
    "# longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "# longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "\n",
    "# # Dictionary to store the final summaries\n",
    "# final_summaries = {}\n",
    "\n",
    "# # Iterate over each summary in the summaries dataset\n",
    "# for row in summaries_dataset:\n",
    "#     review_id = row['review_id']\n",
    "#     extractive_summary = row['summary']\n",
    "\n",
    "#     # Prepare the input for the model\n",
    "#     inputs = longt5_tokenizer(\n",
    "#         extractive_summary, \n",
    "#         truncation=True, \n",
    "#         padding=\"longest\", \n",
    "#         return_tensors=\"pt\", \n",
    "#         max_length=16384\n",
    "#     ).to(device)\n",
    "\n",
    "#     # Generate the summary with LongT5\n",
    "#     try:\n",
    "#         summary_ids = longt5_model.generate(\n",
    "#             inputs['input_ids'], \n",
    "#             num_beams=4,\n",
    "#             min_length=50,\n",
    "#             max_length=400,             \n",
    "#             length_penalty=2.0, \n",
    "#             early_stopping=True\n",
    "#         )\n",
    "        \n",
    "#         # Decode the generated IDs to text\n",
    "#         longt5_summary = longt5_tokenizer.decode(\n",
    "#             summary_ids[0], \n",
    "#             skip_special_tokens=True\n",
    "#         )\n",
    "        \n",
    "#         # Store the summary in the final summaries dictionary\n",
    "#         final_summaries[review_id] = longt5_summary\n",
    "\n",
    "#     except IndexError as e:\n",
    "#         print(f\"Error processing review_id {review_id}: {e}\")\n",
    "#         final_summaries[review_id] = \"\"\n",
    "\n",
    "# # Display the final summaries\n",
    "# for review_id, summary in final_summaries.items():\n",
    "#     print(f\"Review ID: {review_id}\\nAbstractive Summary: {summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Saving the dataset\n",
    "# # with open('summaries_dataset.pkl', 'wb') as file:\n",
    "# #     pickle.dump(summaries_dataset, file)\n",
    "\n",
    "# # Later, you can load the dataset\n",
    "# with open('summaries_dataset.pkl', 'rb') as file:\n",
    "#     summaries_dataset = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a283b80",
   "metadata": {},
   "source": [
    "batching version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.cuda.amp import autocast\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# # Check if MPS is available and set the device accordingly\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# # Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fine-tuned for summarization\n",
    "# longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "# longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# # Define the batch size\n",
    "# batch_size = 1\n",
    "\n",
    "# # Initialize the results list\n",
    "# results = []\n",
    "\n",
    "# # Convert the dataset to a list of dictionaries if not already\n",
    "# data_list = list(summaries_dataset)\n",
    "\n",
    "# # Generate summaries in batches\n",
    "# for i in range(0, len(data_list), batch_size):\n",
    "#     batch = data_list[i:i + batch_size]\n",
    "#     input_texts = [row['summary'] for row in batch]\n",
    "#     review_ids_batch = [row['review_id'] for row in batch]\n",
    "\n",
    "#     inputs = longt5_tokenizer(\n",
    "#         input_texts,\n",
    "#         truncation=True,\n",
    "#         padding=\"longest\",\n",
    "#         return_tensors=\"pt\",\n",
    "#         max_length=16384\n",
    "#     ).to(device)\n",
    "\n",
    "#     try:\n",
    "#         summary_ids = longt5_model.generate(\n",
    "#             inputs['input_ids'],\n",
    "#             num_beams=2,\n",
    "#             no_repeat_ngram_size=2,\n",
    "#             min_length=10,\n",
    "#             max_length=512,\n",
    "#             early_stopping=True\n",
    "#         ).to('cpu')\n",
    "        \n",
    "#         batch_summaries = longt5_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "#         # Append each summary with its ReviewID to the results list\n",
    "#         for review_id, summary in zip(review_ids_batch, batch_summaries):\n",
    "#             results.append({'review_id': review_id, 'Summary': summary})\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in batch starting at index {i}: {e}\")\n",
    "#         for review_id in review_ids_batch:\n",
    "#             results.append({'review_id': review_id, 'Summary': \"\"})\n",
    "\n",
    "# # Convert the results to a DataFrame\n",
    "# summaries_df_val = pd.DataFrame(results)\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# output_file_val = 'summary_BioBERT_K_Means_Long_T5_prediction.csv'\n",
    "# summaries_df_val.to_csv(output_file_val, index=True)\n",
    "# print(f\"Saved summaries to {output_file_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf3a1a",
   "metadata": {},
   "source": [
    "#batching version 2: using a method that saves summaries and clears memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import os\n",
    "# import gc  # For garbage collection\n",
    "# import pickle\n",
    "\n",
    "\n",
    "# # Check for device availability and set accordingly\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"Using CUDA device.\")\n",
    "# elif torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"CUDA and MPS not available. Using CPU.\")\n",
    "\n",
    "\n",
    "# # Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fine-tuned for summarization\n",
    "# longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "# longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# # Define the batch size and save directory\n",
    "# batch_size = 1\n",
    "# save_dir = 'summaries'  # Directory to save individual summary files\n",
    "# os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# # Initialize the results list\n",
    "# results = []\n",
    "\n",
    "\n",
    "\n",
    "# #loading the .pkl file if needed \n",
    "# # you can load the dataset\n",
    "# with open('summaries_dataset.pkl', 'rb') as file:\n",
    "#     summaries_dataset = pickle.load(file)\n",
    "\n",
    "\n",
    "\n",
    "# # Convert the dataset to a list of dictionaries if not already\n",
    "# data_list = list(summaries_dataset)\n",
    "\n",
    "# #Select a subset of the dataset if needed\n",
    "# data_list = data_list[1000:1020]\n",
    "\n",
    "\n",
    "# # Generate summaries in batches\n",
    "# for i in range(0, len(data_list), batch_size):\n",
    "#     batch = data_list[i:i + batch_size]\n",
    "#     input_texts = [row['summary'] for row in batch]\n",
    "#     review_ids_batch = [row['review_id'] for row in batch]\n",
    "\n",
    "#     inputs = longt5_tokenizer(\n",
    "#         input_texts,\n",
    "#         truncation=True,\n",
    "#         padding=\"longest\",\n",
    "#         return_tensors=\"pt\",\n",
    "#         max_length=16384\n",
    "#     ).to(device)\n",
    "\n",
    "#     try:\n",
    "#         summary_ids = longt5_model.generate(\n",
    "#             inputs['input_ids'],\n",
    "#             num_beams=2,\n",
    "#             no_repeat_ngram_size=2,\n",
    "#             min_length=10,\n",
    "#             max_length=512,\n",
    "#             early_stopping=True\n",
    "#         ).to('cpu')\n",
    "        \n",
    "#         batch_summaries = longt5_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "#         # Save summaries and append to results\n",
    "#         for review_id, summary in zip(review_ids_batch, batch_summaries):\n",
    "#             with open(os.path.join(save_dir, f\"{review_id}.txt\"), \"w\") as f:\n",
    "#                 f.write(summary)\n",
    "#             results.append({'review_id': review_id, 'Summary': summary})\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in batch starting at index {i}: {e}\")\n",
    "#         for review_id in review_ids_batch:\n",
    "#             results.append({'review_id': review_id, 'Summary': \"\"})\n",
    "\n",
    "#     # Clear memory\n",
    "#     del inputs, summary_ids\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "#     elif torch.backends.mps.is_available():\n",
    "#         torch.mps.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "# # Convert the results to a DataFrame\n",
    "# summaries_df_val = pd.DataFrame(results)\n",
    "\n",
    "# # Define the destination folder\n",
    "# output_folder = 'summaries'\n",
    "\n",
    "# # Define the CSV file name within the folder\n",
    "# output_file_val = os.path.join(output_folder, 'summary_BioBERT_K_Means_Long_T5_prediction.csv')\n",
    "# # Save the DataFrame to the CSV file\n",
    "# summaries_df_val.to_csv(output_file_val, index=True)\n",
    "# print(f\"Saved summaries to {output_file_val}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256de1f",
   "metadata": {},
   "source": [
    "#version 3: instead of saving to a txt file, append to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3d902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Metal Performance Shaders) device.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/faridgholitabar/miniforge3/266env/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:75: UserWarning: MPS: The constant padding of more than 3 dimensions is not currently supported natively. It uses View Ops default implementation to run. This may have performance implications. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/Pad.mm:474.)\n",
      "  x = nn.functional.pad(x, pad=pad, mode=\"constant\", value=pad_value)\n",
      "/Users/faridgholitabar/miniforge3/266env/lib/python3.8/site-packages/transformers/modeling_utils.py:859: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0 done!\n",
      "Index 1 done!\n",
      "Index 2 done!\n",
      "Index 3 done!\n",
      "Index 4 done!\n",
      "Index 5 done!\n",
      "Index 6 done!\n",
      "Index 7 done!\n",
      "Index 8 done!\n",
      "Index 9 done!\n",
      "Index 10 done!\n",
      "Index 11 done!\n",
      "Index 12 done!\n",
      "Index 13 done!\n",
      "Index 14 done!\n",
      "Index 15 done!\n",
      "Index 16 done!\n",
      "Index 17 done!\n",
      "Index 18 done!\n",
      "Index 19 done!\n",
      "Index 20 done!\n",
      "Index 21 done!\n",
      "Index 22 done!\n",
      "Index 23 done!\n",
      "Index 24 done!\n",
      "Index 25 done!\n",
      "Index 26 done!\n",
      "Index 27 done!\n",
      "Index 28 done!\n",
      "Index 29 done!\n",
      "Index 30 done!\n",
      "Index 31 done!\n",
      "Index 32 done!\n",
      "Index 33 done!\n",
      "Index 34 done!\n",
      "Index 35 done!\n",
      "Index 36 done!\n",
      "Index 37 done!\n",
      "Index 38 done!\n",
      "Index 39 done!\n",
      "Index 40 done!\n",
      "Index 41 done!\n",
      "Index 42 done!\n",
      "Index 43 done!\n",
      "Index 44 done!\n",
      "Index 45 done!\n",
      "Index 46 done!\n",
      "Index 47 done!\n",
      "Index 48 done!\n",
      "Index 49 done!\n",
      "Index 50 done!\n",
      "Index 51 done!\n",
      "Index 52 done!\n",
      "Index 53 done!\n",
      "Index 54 done!\n",
      "Index 55 done!\n",
      "Index 56 done!\n",
      "Index 57 done!\n",
      "Index 58 done!\n",
      "Index 59 done!\n",
      "Index 60 done!\n",
      "Index 61 done!\n",
      "Index 62 done!\n",
      "Index 63 done!\n",
      "Index 64 done!\n",
      "Index 65 done!\n",
      "Index 66 done!\n",
      "Index 67 done!\n",
      "Index 68 done!\n",
      "Index 69 done!\n",
      "Index 70 done!\n",
      "Index 71 done!\n",
      "Index 72 done!\n",
      "Index 73 done!\n",
      "Index 74 done!\n",
      "Index 75 done!\n",
      "Index 76 done!\n",
      "Index 77 done!\n",
      "Index 78 done!\n",
      "Index 79 done!\n",
      "Index 80 done!\n",
      "Index 81 done!\n",
      "Index 82 done!\n",
      "Index 83 done!\n",
      "Index 84 done!\n",
      "Index 85 done!\n",
      "Index 86 done!\n",
      "Index 87 done!\n",
      "Index 88 done!\n",
      "Index 89 done!\n",
      "Index 90 done!\n",
      "Index 91 done!\n",
      "Index 92 done!\n",
      "Index 93 done!\n",
      "Index 94 done!\n",
      "Index 95 done!\n",
      "Index 96 done!\n",
      "Index 97 done!\n",
      "Index 98 done!\n",
      "Index 99 done!\n",
      "Index 100 done!\n",
      "Index 101 done!\n",
      "Index 102 done!\n",
      "Index 103 done!\n",
      "Index 104 done!\n",
      "Index 105 done!\n",
      "Index 106 done!\n",
      "Index 107 done!\n",
      "Index 108 done!\n",
      "Index 109 done!\n",
      "Index 110 done!\n",
      "Index 111 done!\n",
      "Index 112 done!\n",
      "Index 113 done!\n",
      "Index 114 done!\n",
      "Index 115 done!\n",
      "Index 116 done!\n",
      "Index 117 done!\n",
      "Index 118 done!\n",
      "Index 119 done!\n",
      "Index 120 done!\n",
      "Index 121 done!\n",
      "Index 122 done!\n",
      "Index 123 done!\n",
      "Index 124 done!\n",
      "Index 125 done!\n",
      "Index 126 done!\n",
      "Index 127 done!\n",
      "Index 128 done!\n",
      "Index 129 done!\n",
      "Index 130 done!\n",
      "Index 131 done!\n",
      "Index 132 done!\n",
      "Index 133 done!\n",
      "Index 134 done!\n",
      "Index 135 done!\n",
      "Index 136 done!\n",
      "Index 137 done!\n",
      "Index 138 done!\n",
      "Index 139 done!\n",
      "Index 140 done!\n",
      "Index 141 done!\n",
      "Index 142 done!\n",
      "Index 143 done!\n",
      "Index 144 done!\n",
      "Index 145 done!\n",
      "Index 146 done!\n",
      "Index 147 done!\n",
      "Index 148 done!\n",
      "Index 149 done!\n",
      "Index 150 done!\n",
      "Index 151 done!\n",
      "Index 152 done!\n",
      "Index 153 done!\n",
      "Index 154 done!\n",
      "Index 155 done!\n",
      "Index 156 done!\n",
      "Index 157 done!\n",
      "Index 158 done!\n",
      "Index 159 done!\n",
      "Index 160 done!\n",
      "Index 161 done!\n",
      "Index 162 done!\n",
      "Index 163 done!\n",
      "Index 164 done!\n",
      "Index 165 done!\n",
      "Index 166 done!\n",
      "Index 167 done!\n",
      "Index 168 done!\n",
      "Index 169 done!\n",
      "Index 170 done!\n",
      "Index 171 done!\n",
      "Index 172 done!\n",
      "Index 173 done!\n",
      "Index 174 done!\n",
      "Index 175 done!\n",
      "Index 176 done!\n",
      "Index 177 done!\n",
      "Index 178 done!\n",
      "Index 179 done!\n",
      "Index 180 done!\n",
      "Index 181 done!\n",
      "Index 182 done!\n",
      "Index 183 done!\n",
      "Index 184 done!\n",
      "Index 185 done!\n",
      "Index 186 done!\n",
      "Index 187 done!\n",
      "Index 188 done!\n",
      "Index 189 done!\n",
      "Index 190 done!\n",
      "Index 191 done!\n",
      "Index 192 done!\n",
      "Index 193 done!\n",
      "Index 194 done!\n",
      "Index 195 done!\n",
      "Index 196 done!\n",
      "Index 197 done!\n",
      "Index 198 done!\n",
      "Index 199 done!\n",
      "Index 200 done!\n",
      "Index 201 done!\n",
      "Index 202 done!\n",
      "Index 203 done!\n",
      "Index 204 done!\n",
      "Index 205 done!\n",
      "Index 206 done!\n",
      "Index 207 done!\n",
      "Index 208 done!\n",
      "Index 209 done!\n",
      "Index 210 done!\n",
      "Index 211 done!\n",
      "Index 212 done!\n",
      "Index 213 done!\n",
      "Index 214 done!\n",
      "Index 215 done!\n",
      "Index 216 done!\n",
      "Index 217 done!\n",
      "Index 218 done!\n",
      "Index 219 done!\n",
      "Index 220 done!\n",
      "Index 221 done!\n",
      "Index 222 done!\n",
      "Index 223 done!\n",
      "Index 224 done!\n",
      "Index 225 done!\n",
      "Index 226 done!\n",
      "Index 227 done!\n",
      "Index 228 done!\n",
      "Index 229 done!\n",
      "Index 230 done!\n",
      "Index 231 done!\n",
      "Index 232 done!\n",
      "Index 233 done!\n",
      "Index 234 done!\n",
      "Index 235 done!\n",
      "Index 236 done!\n",
      "Index 237 done!\n",
      "Index 238 done!\n",
      "Index 239 done!\n",
      "Index 240 done!\n",
      "Index 241 done!\n",
      "Index 242 done!\n",
      "Index 243 done!\n",
      "Index 244 done!\n",
      "Index 245 done!\n",
      "Index 246 done!\n",
      "Index 247 done!\n",
      "Index 248 done!\n",
      "Index 249 done!\n",
      "Index 250 done!\n",
      "Index 251 done!\n",
      "Index 252 done!\n",
      "Index 253 done!\n",
      "Index 254 done!\n",
      "Index 255 done!\n",
      "Index 256 done!\n",
      "Index 257 done!\n",
      "Index 258 done!\n",
      "Index 259 done!\n",
      "Index 260 done!\n",
      "Index 261 done!\n",
      "Index 262 done!\n",
      "Index 263 done!\n",
      "Index 264 done!\n",
      "Index 265 done!\n",
      "Index 266 done!\n",
      "Index 267 done!\n",
      "Index 268 done!\n",
      "Index 269 done!\n",
      "Index 270 done!\n",
      "Index 271 done!\n",
      "Index 272 done!\n",
      "Index 273 done!\n",
      "Index 274 done!\n",
      "Index 275 done!\n",
      "Index 276 done!\n",
      "Index 277 done!\n",
      "Index 278 done!\n",
      "Index 279 done!\n",
      "Index 280 done!\n",
      "Index 281 done!\n",
      "Index 282 done!\n",
      "Index 283 done!\n",
      "Index 284 done!\n",
      "Index 285 done!\n",
      "Index 286 done!\n",
      "Index 287 done!\n",
      "Index 288 done!\n",
      "Index 289 done!\n",
      "Index 290 done!\n",
      "Index 291 done!\n",
      "Index 292 done!\n",
      "Index 293 done!\n",
      "Index 294 done!\n",
      "Index 295 done!\n",
      "Index 296 done!\n",
      "Index 297 done!\n",
      "Index 298 done!\n",
      "Index 299 done!\n",
      "Index 300 done!\n",
      "Index 301 done!\n",
      "Index 302 done!\n",
      "Index 303 done!\n",
      "Index 304 done!\n",
      "Index 305 done!\n",
      "Index 306 done!\n",
      "Index 307 done!\n",
      "Index 308 done!\n",
      "Index 309 done!\n",
      "Index 310 done!\n",
      "Index 311 done!\n",
      "Index 312 done!\n",
      "Index 313 done!\n",
      "Index 314 done!\n",
      "Index 315 done!\n",
      "Index 316 done!\n",
      "Index 317 done!\n",
      "Index 318 done!\n",
      "Index 319 done!\n",
      "Index 320 done!\n",
      "Index 321 done!\n",
      "Index 322 done!\n",
      "Index 323 done!\n",
      "Index 324 done!\n",
      "Index 325 done!\n",
      "Index 326 done!\n",
      "Index 327 done!\n",
      "Index 328 done!\n",
      "Index 329 done!\n",
      "Index 330 done!\n",
      "Index 331 done!\n",
      "Index 332 done!\n",
      "Index 333 done!\n",
      "Index 334 done!\n",
      "Index 335 done!\n",
      "Index 336 done!\n",
      "Index 337 done!\n",
      "Index 338 done!\n",
      "Index 339 done!\n",
      "Index 340 done!\n",
      "Index 341 done!\n",
      "Index 342 done!\n",
      "Index 343 done!\n",
      "Index 344 done!\n",
      "Index 345 done!\n",
      "Index 346 done!\n",
      "Index 347 done!\n",
      "Index 348 done!\n",
      "Index 349 done!\n",
      "Index 350 done!\n",
      "Index 351 done!\n",
      "Index 352 done!\n",
      "Index 353 done!\n",
      "Index 354 done!\n",
      "Index 355 done!\n",
      "Index 356 done!\n",
      "Index 357 done!\n",
      "Index 358 done!\n",
      "Index 359 done!\n",
      "Index 360 done!\n",
      "Index 361 done!\n",
      "Index 362 done!\n",
      "Index 363 done!\n",
      "Index 364 done!\n",
      "Index 365 done!\n",
      "Index 366 done!\n",
      "Index 367 done!\n",
      "Index 368 done!\n",
      "Index 369 done!\n",
      "Index 370 done!\n",
      "Index 371 done!\n",
      "Index 372 done!\n",
      "Index 373 done!\n",
      "Index 374 done!\n",
      "Index 375 done!\n",
      "Index 376 done!\n",
      "Index 377 done!\n",
      "Index 378 done!\n",
      "Index 379 done!\n",
      "Index 380 done!\n",
      "Index 381 done!\n",
      "Index 382 done!\n",
      "Index 383 done!\n",
      "Index 384 done!\n",
      "Index 385 done!\n",
      "Index 386 done!\n",
      "Index 387 done!\n",
      "Index 388 done!\n",
      "Index 389 done!\n",
      "Index 390 done!\n",
      "Index 391 done!\n",
      "Index 392 done!\n",
      "Index 393 done!\n",
      "Index 394 done!\n",
      "Index 395 done!\n",
      "Index 396 done!\n",
      "Index 397 done!\n",
      "Index 398 done!\n",
      "Index 399 done!\n",
      "Index 400 done!\n",
      "Index 401 done!\n",
      "Index 402 done!\n",
      "Index 403 done!\n",
      "Index 404 done!\n",
      "Index 405 done!\n",
      "Index 406 done!\n",
      "Index 407 done!\n",
      "Index 408 done!\n",
      "Index 409 done!\n",
      "Index 410 done!\n",
      "Index 411 done!\n",
      "Index 412 done!\n",
      "Index 413 done!\n",
      "Index 414 done!\n",
      "Index 415 done!\n",
      "Index 416 done!\n",
      "Index 417 done!\n",
      "Index 418 done!\n",
      "Index 419 done!\n",
      "Index 420 done!\n",
      "Index 421 done!\n",
      "Index 422 done!\n",
      "Index 423 done!\n",
      "Index 424 done!\n",
      "Index 425 done!\n",
      "Index 426 done!\n",
      "Index 427 done!\n",
      "Index 428 done!\n",
      "Index 429 done!\n",
      "Index 430 done!\n",
      "Index 431 done!\n",
      "Index 432 done!\n",
      "Index 433 done!\n",
      "Index 434 done!\n",
      "Index 435 done!\n",
      "Index 436 done!\n",
      "Index 437 done!\n",
      "Index 438 done!\n",
      "Index 439 done!\n",
      "Index 440 done!\n",
      "Index 441 done!\n",
      "Index 442 done!\n",
      "Index 443 done!\n",
      "Index 444 done!\n",
      "Index 445 done!\n",
      "Index 446 done!\n",
      "Index 447 done!\n",
      "Index 448 done!\n",
      "Index 449 done!\n",
      "Index 450 done!\n",
      "Index 451 done!\n",
      "Index 452 done!\n",
      "Index 453 done!\n",
      "Index 454 done!\n",
      "Index 455 done!\n",
      "Index 456 done!\n",
      "Index 457 done!\n",
      "Index 458 done!\n",
      "Index 459 done!\n",
      "Index 460 done!\n",
      "Index 461 done!\n",
      "Index 462 done!\n",
      "Index 463 done!\n",
      "Index 464 done!\n",
      "Index 465 done!\n",
      "Index 466 done!\n",
      "Index 467 done!\n",
      "Index 468 done!\n",
      "Index 469 done!\n",
      "Index 470 done!\n",
      "Index 471 done!\n",
      "Index 472 done!\n",
      "Index 473 done!\n",
      "Index 474 done!\n",
      "Index 475 done!\n",
      "Index 476 done!\n",
      "Index 477 done!\n",
      "Index 478 done!\n",
      "Index 479 done!\n",
      "Index 480 done!\n",
      "Index 481 done!\n",
      "Index 482 done!\n",
      "Index 483 done!\n",
      "Index 484 done!\n",
      "Index 485 done!\n",
      "Index 486 done!\n",
      "Index 487 done!\n",
      "Index 488 done!\n",
      "Index 489 done!\n",
      "Index 490 done!\n",
      "Index 491 done!\n",
      "Index 492 done!\n",
      "Index 493 done!\n",
      "Index 494 done!\n",
      "Index 495 done!\n",
      "Index 496 done!\n",
      "Index 497 done!\n",
      "Index 498 done!\n",
      "Index 499 done!\n",
      "Index 500 done!\n",
      "Index 501 done!\n",
      "Index 502 done!\n",
      "Index 503 done!\n",
      "Index 504 done!\n",
      "Index 505 done!\n",
      "Index 506 done!\n",
      "Index 507 done!\n",
      "Index 508 done!\n",
      "Index 509 done!\n",
      "Index 510 done!\n",
      "Index 511 done!\n",
      "Index 512 done!\n",
      "Index 513 done!\n",
      "Index 514 done!\n",
      "Index 515 done!\n",
      "Index 516 done!\n",
      "Index 517 done!\n",
      "Index 518 done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 519 done!\n",
      "Index 520 done!\n",
      "Index 521 done!\n",
      "Index 522 done!\n",
      "Index 523 done!\n",
      "Index 524 done!\n",
      "Index 525 done!\n",
      "Index 526 done!\n",
      "Index 527 done!\n",
      "Index 528 done!\n",
      "Index 529 done!\n",
      "Index 530 done!\n",
      "Index 531 done!\n",
      "Index 532 done!\n",
      "Index 533 done!\n",
      "Index 534 done!\n",
      "Index 535 done!\n",
      "Index 536 done!\n",
      "Index 537 done!\n",
      "Index 538 done!\n",
      "Index 539 done!\n",
      "Index 540 done!\n",
      "Index 541 done!\n",
      "Index 542 done!\n",
      "Index 543 done!\n",
      "Index 544 done!\n",
      "Index 545 done!\n",
      "Index 546 done!\n",
      "Index 547 done!\n",
      "Index 548 done!\n",
      "Index 549 done!\n",
      "Index 550 done!\n",
      "Index 551 done!\n",
      "Index 552 done!\n",
      "Index 553 done!\n",
      "Index 554 done!\n",
      "Index 555 done!\n",
      "Index 556 done!\n",
      "Index 557 done!\n",
      "Index 558 done!\n",
      "Index 559 done!\n",
      "Index 560 done!\n",
      "Index 561 done!\n",
      "Index 562 done!\n",
      "Index 563 done!\n",
      "Index 564 done!\n",
      "Index 565 done!\n",
      "Index 566 done!\n",
      "Index 567 done!\n",
      "Index 568 done!\n",
      "Index 569 done!\n",
      "Index 570 done!\n",
      "Index 571 done!\n",
      "Index 572 done!\n",
      "Index 573 done!\n",
      "Index 574 done!\n",
      "Index 575 done!\n",
      "Index 576 done!\n",
      "Index 577 done!\n",
      "Index 578 done!\n",
      "Index 579 done!\n",
      "Index 580 done!\n",
      "Index 581 done!\n",
      "Index 582 done!\n",
      "Index 583 done!\n",
      "Index 584 done!\n",
      "Index 585 done!\n",
      "Index 586 done!\n",
      "Index 587 done!\n",
      "Index 588 done!\n",
      "Index 589 done!\n",
      "Index 590 done!\n",
      "Index 591 done!\n",
      "Index 592 done!\n",
      "Index 593 done!\n",
      "Index 594 done!\n",
      "Index 595 done!\n",
      "Index 596 done!\n",
      "Index 597 done!\n",
      "Index 598 done!\n",
      "Index 599 done!\n",
      "Index 600 done!\n",
      "Index 601 done!\n",
      "Index 602 done!\n",
      "Index 603 done!\n",
      "Index 604 done!\n",
      "Index 605 done!\n",
      "Index 606 done!\n",
      "Index 607 done!\n",
      "Index 608 done!\n",
      "Index 609 done!\n",
      "Index 610 done!\n",
      "Index 611 done!\n",
      "Index 612 done!\n",
      "Index 613 done!\n",
      "Index 614 done!\n",
      "Index 615 done!\n",
      "Index 616 done!\n",
      "Index 617 done!\n",
      "Index 618 done!\n",
      "Index 619 done!\n",
      "Index 620 done!\n",
      "Index 621 done!\n",
      "Index 622 done!\n",
      "Index 623 done!\n",
      "Index 624 done!\n",
      "Index 625 done!\n",
      "Index 626 done!\n",
      "Index 627 done!\n",
      "Index 628 done!\n",
      "Index 629 done!\n",
      "Index 630 done!\n",
      "Index 631 done!\n",
      "Index 632 done!\n",
      "Index 633 done!\n",
      "Index 634 done!\n",
      "Index 635 done!\n",
      "Index 636 done!\n",
      "Index 637 done!\n",
      "Index 638 done!\n",
      "Index 639 done!\n",
      "Index 640 done!\n",
      "Index 641 done!\n",
      "Index 642 done!\n",
      "Index 643 done!\n",
      "Index 644 done!\n",
      "Index 645 done!\n",
      "Index 646 done!\n",
      "Index 647 done!\n",
      "Index 648 done!\n",
      "Index 649 done!\n",
      "Index 650 done!\n",
      "Index 651 done!\n",
      "Index 652 done!\n",
      "Index 653 done!\n",
      "Index 654 done!\n",
      "Index 655 done!\n",
      "Index 656 done!\n",
      "Index 657 done!\n",
      "Index 658 done!\n",
      "Index 659 done!\n",
      "Index 660 done!\n",
      "Index 661 done!\n",
      "Index 662 done!\n",
      "Index 663 done!\n",
      "Index 664 done!\n",
      "Index 665 done!\n",
      "Index 666 done!\n",
      "Index 667 done!\n",
      "Index 668 done!\n",
      "Index 669 done!\n",
      "Index 670 done!\n",
      "Index 671 done!\n",
      "Index 672 done!\n",
      "Index 673 done!\n",
      "Index 674 done!\n",
      "Index 675 done!\n",
      "Index 676 done!\n",
      "Index 677 done!\n",
      "Index 678 done!\n",
      "Index 679 done!\n",
      "Index 680 done!\n",
      "Index 681 done!\n",
      "Index 682 done!\n",
      "Index 683 done!\n",
      "Index 684 done!\n",
      "Index 685 done!\n",
      "Index 686 done!\n",
      "Index 687 done!\n",
      "Index 688 done!\n",
      "Index 689 done!\n",
      "Index 690 done!\n",
      "Index 691 done!\n",
      "Index 692 done!\n",
      "Index 693 done!\n",
      "Index 694 done!\n",
      "Index 695 done!\n",
      "Index 696 done!\n",
      "Index 697 done!\n",
      "Index 698 done!\n",
      "Index 699 done!\n",
      "Index 700 done!\n",
      "Index 701 done!\n",
      "Index 702 done!\n",
      "Index 703 done!\n",
      "Index 704 done!\n",
      "Index 705 done!\n",
      "Index 706 done!\n",
      "Index 707 done!\n",
      "Index 708 done!\n",
      "Index 709 done!\n",
      "Index 710 done!\n",
      "Index 711 done!\n",
      "Index 712 done!\n",
      "Index 713 done!\n",
      "Index 714 done!\n",
      "Index 715 done!\n",
      "Index 716 done!\n",
      "Index 717 done!\n",
      "Index 718 done!\n",
      "Index 719 done!\n",
      "Index 720 done!\n",
      "Index 721 done!\n",
      "Index 722 done!\n",
      "Index 723 done!\n",
      "Index 724 done!\n",
      "Index 725 done!\n",
      "Index 726 done!\n",
      "Index 727 done!\n",
      "Index 728 done!\n",
      "Index 729 done!\n",
      "Index 730 done!\n",
      "Index 731 done!\n",
      "Index 732 done!\n",
      "Index 733 done!\n",
      "Index 734 done!\n",
      "Index 735 done!\n",
      "Index 736 done!\n",
      "Index 737 done!\n",
      "Index 738 done!\n",
      "Index 739 done!\n",
      "Index 740 done!\n",
      "Index 741 done!\n",
      "Index 742 done!\n",
      "Index 743 done!\n",
      "Index 744 done!\n",
      "Index 745 done!\n",
      "Index 746 done!\n",
      "Index 747 done!\n",
      "Index 748 done!\n",
      "Index 749 done!\n",
      "Index 750 done!\n",
      "Index 751 done!\n",
      "Index 752 done!\n",
      "Index 753 done!\n",
      "Index 754 done!\n",
      "Index 755 done!\n",
      "Index 756 done!\n",
      "Index 757 done!\n",
      "Index 758 done!\n",
      "Index 759 done!\n",
      "Index 760 done!\n",
      "Index 761 done!\n",
      "Index 762 done!\n",
      "Index 763 done!\n",
      "Index 764 done!\n",
      "Index 765 done!\n",
      "Index 766 done!\n",
      "Index 767 done!\n",
      "Index 768 done!\n",
      "Index 769 done!\n",
      "Index 770 done!\n",
      "Index 771 done!\n",
      "Index 772 done!\n",
      "Index 773 done!\n",
      "Index 774 done!\n",
      "Index 775 done!\n",
      "Index 776 done!\n",
      "Index 777 done!\n",
      "Index 778 done!\n",
      "Index 779 done!\n",
      "Index 780 done!\n",
      "Index 781 done!\n",
      "Index 782 done!\n",
      "Index 783 done!\n",
      "Index 784 done!\n",
      "Index 785 done!\n",
      "Index 786 done!\n",
      "Index 787 done!\n",
      "Index 788 done!\n",
      "Index 789 done!\n",
      "Index 790 done!\n",
      "Index 791 done!\n",
      "Index 792 done!\n",
      "Index 793 done!\n",
      "Index 794 done!\n",
      "Index 795 done!\n",
      "Index 796 done!\n",
      "Index 797 done!\n",
      "Index 798 done!\n",
      "Index 799 done!\n",
      "Index 800 done!\n",
      "Index 801 done!\n",
      "Index 802 done!\n",
      "Index 803 done!\n",
      "Index 804 done!\n",
      "Index 805 done!\n",
      "Index 806 done!\n",
      "Index 807 done!\n",
      "Index 808 done!\n",
      "Index 809 done!\n",
      "Index 810 done!\n",
      "Index 811 done!\n",
      "Index 812 done!\n",
      "Index 813 done!\n",
      "Index 814 done!\n",
      "Index 815 done!\n",
      "Index 816 done!\n",
      "Index 817 done!\n",
      "Index 818 done!\n",
      "Index 819 done!\n",
      "Index 820 done!\n",
      "Index 821 done!\n",
      "Index 822 done!\n",
      "Index 823 done!\n",
      "Index 824 done!\n",
      "Index 825 done!\n",
      "Index 826 done!\n",
      "Index 827 done!\n",
      "Index 828 done!\n",
      "Index 829 done!\n",
      "Index 830 done!\n",
      "Index 831 done!\n",
      "Index 832 done!\n",
      "Index 833 done!\n",
      "Index 834 done!\n",
      "Index 835 done!\n",
      "Index 836 done!\n",
      "Index 837 done!\n",
      "Index 838 done!\n",
      "Index 839 done!\n",
      "Index 840 done!\n",
      "Index 841 done!\n",
      "Index 842 done!\n",
      "Index 843 done!\n",
      "Index 844 done!\n",
      "Index 845 done!\n",
      "Index 846 done!\n",
      "Index 847 done!\n",
      "Index 848 done!\n",
      "Index 849 done!\n",
      "Index 850 done!\n",
      "Index 851 done!\n",
      "Index 852 done!\n",
      "Index 853 done!\n",
      "Index 854 done!\n",
      "Index 855 done!\n",
      "Index 856 done!\n",
      "Index 857 done!\n",
      "Index 858 done!\n",
      "Index 859 done!\n",
      "Index 860 done!\n",
      "Index 861 done!\n",
      "Index 862 done!\n",
      "Index 863 done!\n",
      "Index 864 done!\n",
      "Index 865 done!\n",
      "Index 866 done!\n",
      "Index 867 done!\n",
      "Index 868 done!\n",
      "Index 869 done!\n",
      "Index 870 done!\n",
      "Index 871 done!\n",
      "Index 872 done!\n",
      "Index 873 done!\n",
      "Index 874 done!\n",
      "Index 875 done!\n",
      "Index 876 done!\n",
      "Index 877 done!\n",
      "Index 878 done!\n",
      "Index 879 done!\n",
      "Index 880 done!\n",
      "Index 881 done!\n",
      "Index 882 done!\n",
      "Index 883 done!\n",
      "Index 884 done!\n",
      "Index 885 done!\n",
      "Index 886 done!\n",
      "Index 887 done!\n",
      "Index 888 done!\n",
      "Index 889 done!\n",
      "Index 890 done!\n",
      "Index 891 done!\n",
      "Index 892 done!\n",
      "Index 893 done!\n",
      "Index 894 done!\n",
      "Index 895 done!\n",
      "Index 896 done!\n",
      "Index 897 done!\n",
      "Index 898 done!\n",
      "Index 899 done!\n",
      "Index 900 done!\n",
      "Index 901 done!\n",
      "Index 902 done!\n",
      "Index 903 done!\n",
      "Index 904 done!\n",
      "Index 905 done!\n",
      "Index 906 done!\n",
      "Index 907 done!\n",
      "Index 908 done!\n",
      "Index 909 done!\n",
      "Index 910 done!\n",
      "Index 911 done!\n",
      "Index 912 done!\n",
      "Index 913 done!\n",
      "Index 914 done!\n",
      "Index 915 done!\n",
      "Index 916 done!\n",
      "Index 917 done!\n",
      "Index 918 done!\n",
      "Index 919 done!\n",
      "Index 920 done!\n",
      "Index 921 done!\n",
      "Index 922 done!\n",
      "Index 923 done!\n",
      "Index 924 done!\n",
      "Index 925 done!\n",
      "Index 926 done!\n",
      "Index 927 done!\n",
      "Index 928 done!\n",
      "Index 929 done!\n",
      "Index 930 done!\n",
      "Index 931 done!\n",
      "Index 932 done!\n",
      "Index 933 done!\n",
      "Index 934 done!\n",
      "Index 935 done!\n",
      "Index 936 done!\n",
      "Index 937 done!\n",
      "Index 938 done!\n",
      "Index 939 done!\n",
      "Index 940 done!\n",
      "Index 941 done!\n",
      "Index 942 done!\n",
      "Index 943 done!\n",
      "Index 944 done!\n",
      "Index 945 done!\n",
      "Index 946 done!\n",
      "Index 947 done!\n",
      "Index 948 done!\n",
      "Index 949 done!\n",
      "Index 950 done!\n",
      "Index 951 done!\n",
      "Index 952 done!\n",
      "Index 953 done!\n",
      "Index 954 done!\n",
      "Index 955 done!\n",
      "Index 956 done!\n",
      "Index 957 done!\n",
      "Index 958 done!\n",
      "Index 959 done!\n",
      "Index 960 done!\n",
      "Index 961 done!\n",
      "Index 962 done!\n",
      "Index 963 done!\n",
      "Index 964 done!\n",
      "Index 965 done!\n",
      "Index 966 done!\n",
      "Index 967 done!\n",
      "Index 968 done!\n",
      "Index 969 done!\n",
      "Index 970 done!\n",
      "Index 971 done!\n",
      "Index 972 done!\n",
      "Index 973 done!\n",
      "Index 974 done!\n",
      "Index 975 done!\n",
      "Index 976 done!\n",
      "Index 977 done!\n",
      "Index 978 done!\n",
      "Index 979 done!\n",
      "Index 980 done!\n",
      "Index 981 done!\n",
      "Index 982 done!\n",
      "Index 983 done!\n",
      "Index 984 done!\n",
      "Index 985 done!\n",
      "Index 986 done!\n",
      "Index 987 done!\n",
      "Index 988 done!\n",
      "Index 989 done!\n",
      "Index 990 done!\n",
      "Index 991 done!\n",
      "Index 992 done!\n",
      "Index 993 done!\n",
      "Index 994 done!\n",
      "Index 995 done!\n",
      "Index 996 done!\n",
      "Index 997 done!\n",
      "Index 998 done!\n",
      "Index 999 done!\n",
      "Index 1000 done!\n",
      "Index 1001 done!\n",
      "Index 1002 done!\n",
      "Index 1003 done!\n",
      "Index 1004 done!\n",
      "Index 1005 done!\n",
      "Index 1006 done!\n",
      "Index 1007 done!\n",
      "Index 1008 done!\n",
      "Index 1009 done!\n",
      "Index 1010 done!\n",
      "Index 1011 done!\n",
      "Index 1012 done!\n",
      "Index 1013 done!\n",
      "Index 1014 done!\n",
      "Index 1015 done!\n",
      "Index 1016 done!\n",
      "Index 1017 done!\n",
      "Index 1018 done!\n",
      "Index 1019 done!\n",
      "Index 1020 done!\n",
      "Index 1021 done!\n",
      "Index 1022 done!\n",
      "Index 1023 done!\n",
      "Index 1024 done!\n",
      "Index 1025 done!\n",
      "Index 1026 done!\n",
      "Index 1027 done!\n",
      "Index 1028 done!\n",
      "Index 1029 done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 1030 done!\n",
      "Index 1031 done!\n",
      "Index 1032 done!\n",
      "Index 1033 done!\n",
      "Index 1034 done!\n",
      "Index 1035 done!\n",
      "Index 1036 done!\n",
      "Index 1037 done!\n",
      "Index 1038 done!\n",
      "Index 1039 done!\n",
      "Index 1040 done!\n",
      "Index 1041 done!\n",
      "Index 1042 done!\n",
      "Index 1043 done!\n",
      "Index 1044 done!\n",
      "Index 1045 done!\n",
      "Index 1046 done!\n",
      "Index 1047 done!\n",
      "Index 1048 done!\n",
      "Index 1049 done!\n",
      "Index 1050 done!\n",
      "Index 1051 done!\n",
      "Index 1052 done!\n",
      "Index 1053 done!\n",
      "Index 1054 done!\n",
      "Index 1055 done!\n",
      "Index 1056 done!\n",
      "Index 1057 done!\n",
      "Index 1058 done!\n",
      "Index 1059 done!\n",
      "Index 1060 done!\n",
      "Index 1061 done!\n",
      "Index 1062 done!\n",
      "Index 1063 done!\n",
      "Index 1064 done!\n",
      "Index 1065 done!\n",
      "Index 1066 done!\n",
      "Index 1067 done!\n",
      "Index 1068 done!\n",
      "Index 1069 done!\n",
      "Index 1070 done!\n",
      "Index 1071 done!\n",
      "Index 1072 done!\n",
      "Index 1073 done!\n",
      "Index 1074 done!\n",
      "Index 1075 done!\n",
      "Index 1076 done!\n",
      "Index 1077 done!\n",
      "Index 1078 done!\n",
      "Index 1079 done!\n",
      "Index 1080 done!\n",
      "Index 1081 done!\n",
      "Index 1082 done!\n",
      "Index 1083 done!\n",
      "Index 1084 done!\n",
      "Index 1085 done!\n",
      "Index 1086 done!\n",
      "Index 1087 done!\n",
      "Index 1088 done!\n",
      "Index 1089 done!\n",
      "Index 1090 done!\n",
      "Index 1091 done!\n",
      "Index 1092 done!\n",
      "Index 1093 done!\n",
      "Index 1094 done!\n",
      "Index 1095 done!\n",
      "Index 1096 done!\n",
      "Index 1097 done!\n",
      "Index 1098 done!\n",
      "Index 1099 done!\n",
      "Index 1100 done!\n",
      "Index 1101 done!\n",
      "Index 1102 done!\n",
      "Index 1103 done!\n",
      "Index 1104 done!\n",
      "Index 1105 done!\n",
      "Index 1106 done!\n",
      "Index 1107 done!\n",
      "Index 1108 done!\n",
      "Index 1109 done!\n",
      "Index 1110 done!\n",
      "Index 1111 done!\n",
      "Index 1112 done!\n",
      "Index 1113 done!\n",
      "Index 1114 done!\n",
      "Index 1115 done!\n",
      "Index 1116 done!\n",
      "Index 1117 done!\n",
      "Index 1118 done!\n",
      "Index 1119 done!\n",
      "Index 1120 done!\n",
      "Index 1121 done!\n",
      "Index 1122 done!\n",
      "Index 1123 done!\n",
      "Index 1124 done!\n",
      "Index 1125 done!\n",
      "Index 1126 done!\n",
      "Index 1127 done!\n",
      "Index 1128 done!\n",
      "Index 1129 done!\n",
      "Index 1130 done!\n",
      "Index 1131 done!\n",
      "Index 1132 done!\n",
      "Index 1133 done!\n",
      "Index 1134 done!\n",
      "Index 1135 done!\n",
      "Index 1136 done!\n",
      "Index 1137 done!\n",
      "Index 1138 done!\n",
      "Index 1139 done!\n",
      "Index 1140 done!\n",
      "Index 1141 done!\n",
      "Index 1142 done!\n",
      "Index 1143 done!\n",
      "Index 1144 done!\n",
      "Index 1145 done!\n",
      "Index 1146 done!\n",
      "Index 1147 done!\n",
      "Index 1148 done!\n",
      "Index 1149 done!\n",
      "Index 1150 done!\n",
      "Index 1151 done!\n",
      "Index 1152 done!\n",
      "Index 1153 done!\n",
      "Index 1154 done!\n",
      "Index 1155 done!\n",
      "Index 1156 done!\n",
      "Index 1157 done!\n",
      "Index 1158 done!\n",
      "Index 1159 done!\n",
      "Index 1160 done!\n",
      "Index 1161 done!\n",
      "Index 1162 done!\n",
      "Index 1163 done!\n",
      "Index 1164 done!\n",
      "Index 1165 done!\n",
      "Index 1166 done!\n",
      "Index 1167 done!\n",
      "Index 1168 done!\n",
      "Index 1169 done!\n",
      "Index 1170 done!\n",
      "Index 1171 done!\n",
      "Index 1172 done!\n",
      "Index 1173 done!\n",
      "Index 1174 done!\n",
      "Index 1175 done!\n",
      "Index 1176 done!\n",
      "Index 1177 done!\n",
      "Index 1178 done!\n",
      "Index 1179 done!\n",
      "Index 1180 done!\n",
      "Index 1181 done!\n",
      "Index 1182 done!\n",
      "Index 1183 done!\n",
      "Index 1184 done!\n",
      "Index 1185 done!\n",
      "Index 1186 done!\n",
      "Index 1187 done!\n",
      "Index 1188 done!\n",
      "Index 1189 done!\n",
      "Index 1190 done!\n",
      "Index 1191 done!\n",
      "Index 1192 done!\n",
      "Index 1193 done!\n",
      "Index 1194 done!\n",
      "Index 1195 done!\n",
      "Index 1196 done!\n",
      "Index 1197 done!\n",
      "Index 1198 done!\n",
      "Index 1199 done!\n",
      "Index 1200 done!\n",
      "Index 1201 done!\n",
      "Index 1202 done!\n",
      "Index 1203 done!\n",
      "Index 1204 done!\n",
      "Index 1205 done!\n",
      "Index 1206 done!\n",
      "Index 1207 done!\n",
      "Index 1208 done!\n",
      "Index 1209 done!\n",
      "Index 1210 done!\n",
      "Index 1211 done!\n",
      "Index 1212 done!\n",
      "Index 1213 done!\n",
      "Index 1214 done!\n",
      "Index 1215 done!\n",
      "Index 1216 done!\n",
      "Index 1217 done!\n",
      "Index 1218 done!\n",
      "Index 1219 done!\n",
      "Index 1220 done!\n",
      "Index 1221 done!\n",
      "Index 1222 done!\n",
      "Index 1223 done!\n",
      "Index 1224 done!\n",
      "Index 1225 done!\n",
      "Index 1226 done!\n",
      "Index 1227 done!\n",
      "Index 1228 done!\n",
      "Index 1229 done!\n",
      "Index 1230 done!\n",
      "Index 1231 done!\n",
      "Index 1232 done!\n",
      "Index 1233 done!\n",
      "Index 1234 done!\n",
      "Index 1235 done!\n",
      "Index 1236 done!\n",
      "Index 1237 done!\n",
      "Index 1238 done!\n",
      "Index 1239 done!\n",
      "Index 1240 done!\n",
      "Index 1241 done!\n",
      "Index 1242 done!\n",
      "Index 1243 done!\n",
      "Index 1244 done!\n",
      "Index 1245 done!\n",
      "Index 1246 done!\n",
      "Index 1247 done!\n",
      "Index 1248 done!\n",
      "Index 1249 done!\n",
      "Index 1250 done!\n",
      "Index 1251 done!\n",
      "Index 1252 done!\n",
      "Index 1253 done!\n",
      "Index 1254 done!\n",
      "Index 1255 done!\n",
      "Index 1256 done!\n",
      "Index 1257 done!\n",
      "Index 1258 done!\n",
      "Index 1259 done!\n",
      "Index 1260 done!\n",
      "Index 1261 done!\n",
      "Index 1262 done!\n",
      "Index 1263 done!\n",
      "Index 1264 done!\n",
      "Index 1265 done!\n",
      "Index 1266 done!\n",
      "Index 1267 done!\n",
      "Index 1268 done!\n",
      "Index 1269 done!\n",
      "Index 1270 done!\n",
      "Index 1271 done!\n",
      "Index 1272 done!\n",
      "Index 1273 done!\n",
      "Index 1274 done!\n",
      "Index 1275 done!\n",
      "Index 1276 done!\n",
      "Index 1277 done!\n",
      "Index 1278 done!\n",
      "Error in batch starting at index 1279: MPS backend out of memory (MPS allocated: 1.96 GB, other allocations: 79.65 GB, max allowed: 81.60 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).\n",
      "Index 1279 done!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'summary_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 85\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m done!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Clear memory\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs, summary_ids\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m     87\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_ids' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import gc  # For garbage collection\n",
    "import pickle\n",
    "\n",
    "# Check for device availability and set accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS not available. Using CPU.\")\n",
    "\n",
    "# Load LongT5 Model and Tokenizer\n",
    "model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # Fine-tuned for summarization\n",
    "longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# Load the dataset from a pickle file\n",
    "with open('summaries_dataset.pkl', 'rb') as file:\n",
    "    summaries_dataset = pickle.load(file)\n",
    "\n",
    "# Convert the dataset to a list\n",
    "data_list = list(summaries_dataset)\n",
    "\n",
    "# # Select a subset of the dataset for processing\n",
    "# data_list = data_list[1240:1255]\n",
    "\n",
    "# Define the destination folder and CSV file name\n",
    "output_folder = 'summaries'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "output_file_val = os.path.join(output_folder, 'summary_BioBERT_K_Means_Long_T5_prediction.csv')\n",
    "\n",
    "# Check if the output file already exists, create it with headers if not\n",
    "if not os.path.exists(output_file_val):\n",
    "    pd.DataFrame(columns=['review_id', 'Summary']).to_csv(output_file_val, index=False)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Generate summaries in batches\n",
    "for i in range(0, len(data_list), batch_size):\n",
    "    batch = data_list[i:i + batch_size]\n",
    "    input_texts = [row['summary'] for row in batch]\n",
    "    review_ids_batch = [row['review_id'] for row in batch]\n",
    "\n",
    "    # Tokenize the input texts\n",
    "    inputs = longt5_tokenizer(\n",
    "        input_texts,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=16384\n",
    "    ).to(device)\n",
    "\n",
    "    try:\n",
    "        # Generate summaries\n",
    "        summary_ids = longt5_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            num_beams=2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            min_length=10,\n",
    "            max_length=512,\n",
    "            early_stopping=True\n",
    "        ).to('cpu')\n",
    "\n",
    "        # Decode the summaries\n",
    "        batch_summaries = longt5_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Append summaries directly to the CSV file\n",
    "        with open(output_file_val, 'a') as f:\n",
    "            pd.DataFrame({'review_id': review_ids_batch, 'Summary': batch_summaries}).to_csv(f, header=False, index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch starting at index {i}: {e}\")\n",
    "\n",
    "    print(f\"Index {i} done!\")\n",
    "\n",
    "    # Clear memory\n",
    "    if 'inputs' in locals():\n",
    "        del inputs\n",
    "    if 'summary_ids' in locals():\n",
    "        del summary_ids\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"Saved summaries to {output_file_val}. Abstractive summarization portion done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f171b1",
   "metadata": {},
   "source": [
    "#batching using huggingface module, map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Check if MPS is available and set the device accordingly\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# # Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fine-tuned for summarization\n",
    "# longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "# longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# # Load your dataset here (assuming 'summaries_dataset' is your dataset name)\n",
    "# # summaries_dataset = load_dataset('your_dataset_name')\n",
    "\n",
    "# # Define the batch processing function\n",
    "# def process_batch(batch):\n",
    "#     input_texts = batch['summary']\n",
    "#     review_ids_batch = batch['review_id']\n",
    "\n",
    "#     inputs = longt5_tokenizer(\n",
    "#         input_texts,\n",
    "#         truncation=True,\n",
    "#         padding=\"longest\",\n",
    "#         return_tensors=\"pt\",\n",
    "#         max_length=16384\n",
    "#     ).to(device)\n",
    "\n",
    "#     try:\n",
    "#         summary_ids = longt5_model.generate(\n",
    "#             inputs['input_ids'],\n",
    "#             num_beams=4,\n",
    "#             min_length=50,\n",
    "#             max_length=512,\n",
    "#             early_stopping=True\n",
    "#         ).to('cpu')\n",
    "        \n",
    "#         batch_summaries = longt5_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "#         # Return processed results\n",
    "#         return {'review_id': review_ids_batch, 'Abstractive Summary': batch_summaries}\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in processing batch: {e}\")\n",
    "#         # Returning empty summaries in case of error\n",
    "#         return {'review_id': review_ids_batch, 'Abstractive Summary': ['' for _ in review_ids_batch]}\n",
    "\n",
    "# # Apply the function to the entire dataset using map\n",
    "# batch_size = 1  # Set your batch size\n",
    "# processed_dataset = summaries_dataset.map(process_batch, batched=True, batch_size=batch_size)\n",
    "\n",
    "# # Convert the processed results to a pandas DataFrame\n",
    "# summaries_df = pd.DataFrame.from_dict({\n",
    "#     'review_id': processed_dataset['review_id'],\n",
    "#     'Abstractive Summary': processed_dataset['Abstractive Summary']\n",
    "# })\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# output_file = 'summary_predictions.csv'\n",
    "# summaries_df.to_csv(output_file, index=False)\n",
    "# print(f\"Saved summaries to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
