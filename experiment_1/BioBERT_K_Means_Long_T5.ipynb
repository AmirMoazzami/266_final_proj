{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bdce99a",
   "metadata": {},
   "source": [
    "***Extractive step: BioBERT, K means***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a512c38",
   "metadata": {},
   "source": [
    "pipeline:\n",
    "I use BioBERT for embdeddings\n",
    "\n",
    "I use Spacy for sentence segmentation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# Check if MPS is available and set the device accordingly\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"MPS not available. Using CPU.\")\n",
    "    \n",
    "\n",
    "# Load the dataset and cut down \n",
    "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\n",
    "# Use select to create a subset\n",
    "# dataset = dataset.select(range(20,30))  \n",
    "\n",
    "\n",
    "# Initialize the BioBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.1')\n",
    "model = BertModel.from_pretrained('dmis-lab/biobert-base-cased-v1.1').to(device)\n",
    "\n",
    "\n",
    "def bert_sentence_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        # Move inputs to the device\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', max_length=512, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Move outputs back to CPU\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "#this is fixed : each selected sentence is from a different cluster\n",
    "def select_top_sentences(sentences, embeddings, n_sentences=5):\n",
    "    if len(sentences) <= n_sentences:\n",
    "        return ' '.join(sentences)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_sentences, n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "\n",
    "    # Initialize a set to store indices of selected sentences\n",
    "    selected_indices = set()\n",
    "    top_sentences = []\n",
    "\n",
    "    for i in range(n_sentences):\n",
    "        # Calculate distances of all sentences from the i-th centroid\n",
    "        distances = np.linalg.norm(embeddings - kmeans.cluster_centers_[i], axis=1)\n",
    "\n",
    "        # Sort the sentences by their distance from the centroid\n",
    "        sorted_indices = np.argsort(distances)\n",
    "\n",
    "        # Find the closest sentence that hasn't been selected yet\n",
    "        for index in sorted_indices:\n",
    "            if index not in selected_indices:\n",
    "                selected_indices.add(index)\n",
    "                top_sentences.append(sentences[index])\n",
    "                break\n",
    "\n",
    "    return ' '.join(top_sentences)\n",
    "\n",
    "\n",
    "def process_row(row):\n",
    "    review_id = row['review_id']\n",
    "    abstract_list = row['abstract']\n",
    "    combined_summary = ''\n",
    "\n",
    "    for abstract in abstract_list:\n",
    "        # Check if the abstract is a string; if not, join it into a single string\n",
    "        abstract_text = ' '.join(abstract) if isinstance(abstract, list) else abstract\n",
    "\n",
    "        # Use SpaCy for sentence segmentation\n",
    "        doc = nlp(abstract_text)\n",
    "        sentences = [sent.text.strip() for sent in doc.sents]\n",
    "\n",
    "        # Generate embeddings for each sentence\n",
    "        embeddings = bert_sentence_embeddings(sentences)\n",
    "        # Select the top sentences from these embeddings\n",
    "        summary = select_top_sentences(sentences, embeddings)\n",
    "\n",
    "        # Combine the summaries from each abstract\n",
    "        combined_summary += summary + ' '\n",
    "\n",
    "    return {\"review_id\": review_id, \"summary\": combined_summary.strip()}\n",
    "\n",
    "# Apply the function to each element of the dataset\n",
    "summaries_dataset = dataset.map(process_row)\n",
    "\n",
    "# Saving the dataset\n",
    "import pickle\n",
    "with open('summaries_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(summaries_dataset, file)\n",
    "\n",
    "# # Convert to pandas DataFrame\n",
    "# df = pd.DataFrame(summaries_dataset)\n",
    "# df = df[['review_id', 'summary']]\n",
    "# # Save to CSV\n",
    "# csv_file_path = 'test.csv'  # Update with your desired file path\n",
    "# df.to_csv(csv_file_path, index=True)\n",
    "\n",
    "# print(f\"Saved summaries to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94c8017",
   "metadata": {},
   "source": [
    "***abstractive step: Long T5***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024bf9c",
   "metadata": {},
   "source": [
    "this is without batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6640cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.cuda.amp import autocast\n",
    "# from datasets import load_dataset\n",
    "\n",
    "\n",
    "# # Set the device to MPS if available, else CPU\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# # Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fine-tuned for summarization\n",
    "# longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "# longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "\n",
    "# # Dictionary to store the final summaries\n",
    "# final_summaries = {}\n",
    "\n",
    "# # Iterate over each summary in the summaries dataset\n",
    "# for row in summaries_dataset:\n",
    "#     review_id = row['review_id']\n",
    "#     extractive_summary = row['summary']\n",
    "\n",
    "#     # Prepare the input for the model\n",
    "#     inputs = longt5_tokenizer(\n",
    "#         extractive_summary, \n",
    "#         truncation=True, \n",
    "#         padding=\"longest\", \n",
    "#         return_tensors=\"pt\", \n",
    "#         max_length=16384\n",
    "#     ).to(device)\n",
    "\n",
    "#     # Generate the summary with LongT5\n",
    "#     try:\n",
    "#         summary_ids = longt5_model.generate(\n",
    "#             inputs['input_ids'], \n",
    "#             num_beams=4,\n",
    "#             min_length=50,\n",
    "#             max_length=400,             \n",
    "#             length_penalty=2.0, \n",
    "#             early_stopping=True\n",
    "#         )\n",
    "        \n",
    "#         # Decode the generated IDs to text\n",
    "#         longt5_summary = longt5_tokenizer.decode(\n",
    "#             summary_ids[0], \n",
    "#             skip_special_tokens=True\n",
    "#         )\n",
    "        \n",
    "#         # Store the summary in the final summaries dictionary\n",
    "#         final_summaries[review_id] = longt5_summary\n",
    "\n",
    "#     except IndexError as e:\n",
    "#         print(f\"Error processing review_id {review_id}: {e}\")\n",
    "#         final_summaries[review_id] = \"\"\n",
    "\n",
    "# # Display the final summaries\n",
    "# for review_id, summary in final_summaries.items():\n",
    "#     print(f\"Review ID: {review_id}\\nAbstractive Summary: {summary}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf55a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the dataset\n",
    "# with open('summaries_dataset.pkl', 'wb') as file:\n",
    "#     pickle.dump(summaries_dataset, file)\n",
    "\n",
    "# Later, you can load the dataset\n",
    "with open('summaries_dataset.pkl', 'rb') as file:\n",
    "    summaries_dataset = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a283b80",
   "metadata": {},
   "source": [
    "batching version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd3cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Set the device to MPS if available, else CPU\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load LongT5 Model and Tokenizer\n",
    "model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fine-tuned for summarization\n",
    "longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 1\n",
    "\n",
    "# Initialize the results list\n",
    "results = []\n",
    "\n",
    "# Convert the dataset to a list of dictionaries if not already\n",
    "data_list = list(summaries_dataset)\n",
    "\n",
    "# Generate summaries in batches\n",
    "for i in range(0, len(data_list), batch_size):\n",
    "    batch = data_list[i:i + batch_size]\n",
    "    input_texts = [row['summary'] for row in batch]\n",
    "    review_ids_batch = [row['review_id'] for row in batch]\n",
    "\n",
    "    inputs = longt5_tokenizer(\n",
    "        input_texts,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=16384\n",
    "    ).to(device)\n",
    "\n",
    "    try:\n",
    "        summary_ids = longt5_model.generate(\n",
    "            inputs['input_ids'],\n",
    "            num_beams=2,\n",
    "            no_repeat_ngram_size=2,\n",
    "            min_length=10,\n",
    "            max_length=512,\n",
    "            early_stopping=True\n",
    "        ).to('cpu')\n",
    "        \n",
    "        batch_summaries = longt5_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Append each summary with its ReviewID to the results list\n",
    "        for review_id, summary in zip(review_ids_batch, batch_summaries):\n",
    "            results.append({'review_id': review_id, 'Summary': summary})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in batch starting at index {i}: {e}\")\n",
    "        for review_id in review_ids_batch:\n",
    "            results.append({'review_id': review_id, 'Summary': \"\"})\n",
    "\n",
    "# Convert the results to a DataFrame\n",
    "summaries_df_val = pd.DataFrame(results)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_file_val = 'summary_BioBERT_K_Means_Long_T5_prediction.csv'\n",
    "summaries_df_val.to_csv(output_file_val, index=True)\n",
    "print(f\"Saved summaries to {output_file_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead5536d",
   "metadata": {},
   "source": [
    "#batching: using a method that saves summaries and clears memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28236c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3f171b1",
   "metadata": {},
   "source": [
    "#batching using huggingface module, map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# # Check if MPS is available and set the device accordingly\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "# else:\n",
    "#     device = torch.device(\"cpu\")\n",
    "#     print(\"MPS not available. Using CPU.\")\n",
    "\n",
    "# # Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fine-tuned for summarization\n",
    "# longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "# longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# # Load your dataset here (assuming 'summaries_dataset' is your dataset name)\n",
    "# # summaries_dataset = load_dataset('your_dataset_name')\n",
    "\n",
    "# # Define the batch processing function\n",
    "# def process_batch(batch):\n",
    "#     input_texts = batch['summary']\n",
    "#     review_ids_batch = batch['review_id']\n",
    "\n",
    "#     inputs = longt5_tokenizer(\n",
    "#         input_texts,\n",
    "#         truncation=True,\n",
    "#         padding=\"longest\",\n",
    "#         return_tensors=\"pt\",\n",
    "#         max_length=16384\n",
    "#     ).to(device)\n",
    "\n",
    "#     try:\n",
    "#         summary_ids = longt5_model.generate(\n",
    "#             inputs['input_ids'],\n",
    "#             num_beams=4,\n",
    "#             min_length=50,\n",
    "#             max_length=512,\n",
    "#             early_stopping=True\n",
    "#         ).to('cpu')\n",
    "        \n",
    "#         batch_summaries = longt5_tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "#         # Return processed results\n",
    "#         return {'review_id': review_ids_batch, 'Abstractive Summary': batch_summaries}\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error in processing batch: {e}\")\n",
    "#         # Returning empty summaries in case of error\n",
    "#         return {'review_id': review_ids_batch, 'Abstractive Summary': ['' for _ in review_ids_batch]}\n",
    "\n",
    "# # Apply the function to the entire dataset using map\n",
    "# batch_size = 1  # Set your batch size\n",
    "# processed_dataset = summaries_dataset.map(process_batch, batched=True, batch_size=batch_size)\n",
    "\n",
    "# # Convert the processed results to a pandas DataFrame\n",
    "# summaries_df = pd.DataFrame.from_dict({\n",
    "#     'review_id': processed_dataset['review_id'],\n",
    "#     'Abstractive Summary': processed_dataset['Abstractive Summary']\n",
    "# })\n",
    "\n",
    "# # Save the DataFrame to a CSV file\n",
    "# output_file = 'summary_predictions.csv'\n",
    "# summaries_df.to_csv(output_file, index=False)\n",
    "# print(f\"Saved summaries to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(summaries_dataset)\n",
    "df = df[['review_id', 'summary']]\n",
    "# Save to CSV\n",
    "csv_file_path = 'BioBERT_K_Means_extractive.csv'  # Update with your desired file path\n",
    "df.to_csv(csv_file_path, index=True)\n",
    "\n",
    "print(f\"Saved summaries to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
