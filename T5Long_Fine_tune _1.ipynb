{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6baf52d7-9b7e-477c-a1e4-161df03b3529",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q datasets sentencepiece rouge_score\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c87e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2385c429-3d90-412c-98ec-fd66c665efae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip -q install torch\n",
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28067337-faa8-4dee-ae51-b5a6b3f56de0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722f043b-9ce4-402c-a11c-f1270ab1c501",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF is set to: max_split_size_mb:256\n",
      "device: cpu\n",
      "['review_id', 'pmid', 'title', 'abstract', 'target', 'background']\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Memory optimization for CUDA\n",
    "max_split_size_mb = 256  # Set the max_split_size_mb value (e.g., 512 MB)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"max_split_size_mb:{max_split_size_mb}\"\n",
    "print(f\"PYTORCH_CUDA_ALLOC_CONF is set to: {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "#when on Nvida machins \n",
    "# device = torch.device(\"cuda\")\n",
    "# print(\"device:\", device)\n",
    "\n",
    "#when you are on mac\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "\n",
    "# Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"google/long-t5-local-base\"\n",
    "model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fined-tuned for summarization\n",
    "longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# Load validation dataset from Hugging Face datasets\n",
    "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\n",
    "\n",
    "# Prepare DataFrame for output\n",
    "# output_df = pd.DataFrame(columns=['ReviewID', 'Candidate_Summary', 'Target'])\n",
    "output_df = pd.DataFrame(dataset)\n",
    "\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "204c07ba-5173-4625-9643-b93211736d47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>pmid</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>target</th>\n",
       "      <th>background</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28514886</td>\n",
       "      <td>[15870317, 20863418, 17991656, 15585783, 20032...</td>\n",
       "      <td>[Quantitative Real-Time PCR Assays To Identify...</td>\n",
       "      <td>[ABSTRACT A healthy intestinal microbiota is c...</td>\n",
       "      <td>Current evidence from systematic review and me...</td>\n",
       "      <td>Necrotizing enterocolitis ( NEC ) is one of th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  review_id                                               pmid  \\\n",
       "0  28514886  [15870317, 20863418, 17991656, 15585783, 20032...   \n",
       "\n",
       "                                               title  \\\n",
       "0  [Quantitative Real-Time PCR Assays To Identify...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  [ABSTRACT A healthy intestinal microbiota is c...   \n",
       "\n",
       "                                              target  \\\n",
       "0  Current evidence from systematic review and me...   \n",
       "\n",
       "                                          background  \n",
       "0  Necrotizing enterocolitis ( NEC ) is one of th...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "847e1ad4-db68-447e-aea8-b398f174d113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df.drop(['pmid', 'title','background'], axis = 1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c12e54a5-fb63-4a05-a808-004d72d17280",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df['abstract'] = output_df['abstract'].apply(lambda x: \"\".join([f\"Study {i}: \" + b for i,b in enumerate(x)]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c7ca174-f4b4-454f-ab6f-56e71ab98912",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: transformers 4.35.2\n",
      "Uninstalling transformers-4.35.2:\n",
      "  Would remove:\n",
      "    /Users/amir.moazami/anaconda3/bin/transformers-cli\n",
      "    /Users/amir.moazami/anaconda3/lib/python3.11/site-packages/transformers-4.35.2.dist-info/*\n",
      "    /Users/amir.moazami/anaconda3/lib/python3.11/site-packages/transformers/*\n",
      "Proceed (Y/n)? ^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://interdigital.jfrog.io/artifactory/api/pypi/pypi-mirror/simple, https://interdigital.jfrog.io/artifactory/api/pypi/pypi-local/simple\n",
      "Requirement already satisfied: transformers in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (4.35.2)\n",
      "Requirement already satisfied: filelock in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: pytorch-lightning 1.5.10\n",
      "Uninstalling pytorch-lightning-1.5.10:\n",
      "  Successfully uninstalled pytorch-lightning-1.5.10\n",
      "Looking in indexes: https://pypi.org/simple, https://interdigital.jfrog.io/artifactory/api/pypi/pypi-mirror/simple, https://interdigital.jfrog.io/artifactory/api/pypi/pypi-local/simple\n",
      "Collecting pytorch-lightning==1.5.10\n",
      "  Using cached pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (1.24.3)\n",
      "Requirement already satisfied: torch>=1.7.* in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (2.1.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (0.18.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (4.65.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (6.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (2023.10.0)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (2.15.1)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (1.2.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (0.3.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (23.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (4.7.1)\n",
      "Requirement already satisfied: setuptools==59.5.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pytorch-lightning==1.5.10) (59.5.0)\n",
      "Requirement already satisfied: requests in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2.31.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (3.8.5)\n",
      "Requirement already satisfied: absl-py>=0.4 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.0.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.59.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.4.1)\n",
      "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (4.23.4)\n",
      "Requirement already satisfied: six>1.9 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.9.0)\n",
      "Requirement already satisfied: sympy in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from torch>=1.7.*->pytorch-lightning==1.5.10) (3.1.2)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from torchmetrics>=0.4.1->pytorch-lightning==1.5.10) (0.10.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.7.*->pytorch-lightning==1.5.10) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/amir.moazami/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.5.10) (3.2.2)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pytorch-lightning\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "simplet5 0.1.4 requires transformers==4.16.2, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pytorch-lightning-1.5.10\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.5.10 has a non-standard dependency specifier torch>=1.7.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers -q\n",
    "!pip uninstall transformers\n",
    "!pip install transformers\n",
    "!pip uninstall -y pytorch-lightning\n",
    "!pip install pytorch-lightning==1.5.10\n",
    "!pip install -q --upgrade simplet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb4738fb-7770-4768-b124-7db68d83eb5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !conda install protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4112e2c1-e9de-4f14-87ad-cafc02ba47dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce57711e79b4e118933f3311613406b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa41c5b786b64cbebeaab3e05328f2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6375c3533ba49de93c8573cf1265572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199954dc0b6644258e037544139af07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertTokenizer' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load the dataset and cut down to the first 5 for demonstration\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# dataset = dataset.select(range(3))  # Use select to create a subset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# model = BertModel.from_pretrained('bert-base-uncased')\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m BertTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m BertModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbert_sentence_embeddings\u001b[39m(sentences):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2024\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(save_directory):\n\u001b[1;32m   2023\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProvided path (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_directory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) should be a directory, not a file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m push_to_hub:\n\u001b[1;32m   2027\u001b[0m     commit_message \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit_message\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2256\u001b[0m, in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m old_pad_to_max_length:\n\u001b[1;32m   2250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m   2251\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   2252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `pad_to_max_length` argument is deprecated and will be removed in a future version, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2253\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse `padding=True` or `padding=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongest\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to pad to the longest sequence in the batch, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2254\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse `padding=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` to pad to a max length. In this case, you can give a specific \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2256\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximal input size of the model (e.g. 512 for Bert).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2257\u001b[0m             \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m   2258\u001b[0m         )\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2260\u001b[0m         padding_strategy \u001b[38;5;241m=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mLONGEST\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/tokenization_bert.py:179\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    166\u001b[0m     vocab_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    178\u001b[0m ):\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    180\u001b[0m         do_lower_case\u001b[38;5;241m=\u001b[39mdo_lower_case,\n\u001b[1;32m    181\u001b[0m         do_basic_tokenize\u001b[38;5;241m=\u001b[39mdo_basic_tokenize,\n\u001b[1;32m    182\u001b[0m         never_split\u001b[38;5;241m=\u001b[39mnever_split,\n\u001b[1;32m    183\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[1;32m    184\u001b[0m         sep_token\u001b[38;5;241m=\u001b[39msep_token,\n\u001b[1;32m    185\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[1;32m    186\u001b[0m         cls_token\u001b[38;5;241m=\u001b[39mcls_token,\n\u001b[1;32m    187\u001b[0m         mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[1;32m    188\u001b[0m         tokenize_chinese_chars\u001b[38;5;241m=\u001b[39mtokenize_chinese_chars,\n\u001b[1;32m    189\u001b[0m         strip_accents\u001b[38;5;241m=\u001b[39mstrip_accents,\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(vocab_file):\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    195\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a vocabulary file at path \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvocab_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. To load the vocabulary from a Google pretrained \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    197\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py:367\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvocab_size\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    364\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;124;03m    `int`: Size of the base vocabulary (without the added tokens).\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils.py:467\u001b[0m, in \u001b[0;36m_add_tokens\u001b[0;34m(self, new_tokens, special_tokens)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_special_tokens_to_add\u001b[39m(\u001b[38;5;28mself\u001b[39m, pair: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    459\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    Returns the number of added tokens when encoding a sequence with special tokens.\u001b[39;00m\n\u001b[1;32m    461\u001b[0m \n\u001b[1;32m    462\u001b[0m \u001b[38;5;124;03m    <Tip>\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \n\u001b[1;32m    464\u001b[0m \u001b[38;5;124;03m    This encodes a dummy input and checks the number of added tokens, and is therefore not efficient. Do not put\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    this inside your training loop.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \n\u001b[0;32m--> 467\u001b[0m \u001b[38;5;124;03m    </Tip>\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m        pair (`bool`, *optional*, defaults to `False`):\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m            Whether the number of added tokens should be computed in the case of a sequence pair or a single\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m            sequence.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;124;03m        `int`: Number of special tokens added to sequences.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m     token_ids_0 \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    478\u001b[0m     token_ids_1 \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/models/bert/tokenization_bert.py:219\u001b[0m, in \u001b[0;36mBertTokenizer.get_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_vocab\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BertTokenizer' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Load the dataset and cut down to the first 5 for demonstration\n",
    "# dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\n",
    "# dataset = dataset.select(range(3))  # Use select to create a subset\n",
    "\n",
    "# Initialize BERT\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def bert_sentence_embeddings(sentences):\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors='pt', max_length=512, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def select_top_sentences(sentences, embeddings, n_sentences=5):\n",
    "    if len(sentences) < n_sentences:\n",
    "        return ' '.join(sentences)\n",
    "    kmeans = KMeans(n_clusters=n_sentences, n_init=10)\n",
    "    kmeans.fit(embeddings)\n",
    "    top_sentences =[]\n",
    "    i = 0\n",
    "    while len(top_sentences) < n_sentences:\n",
    "        top_sentence_indices = np.argmin(\n",
    "        np.linalg.norm(embeddings[:, np.newaxis] - kmeans.cluster_centers_[i], axis=2), axis=0)\n",
    "        top_sentences.append(sentences[top_sentence_indices[0]])\n",
    "    return ' '.join(top_sentences)\n",
    "\n",
    "def process_row( abstract_text):\n",
    "    # Split abstract into sentences\n",
    "    sentences = abstract_text.split('. ')\n",
    "    # Generate embeddings for each sentence\n",
    "    embeddings = bert_sentence_embeddings(sentences)\n",
    "    # Select the top sentences from these embeddings\n",
    "    summary = select_top_sentences(sentences, embeddings)\n",
    "        \n",
    "    return summary\n",
    "        \n",
    "\n",
    "                             \n",
    "                             \n",
    "output_df['abstract'] = output_df['abstract'].apply(lambda x:process_row(x) )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1791ca14-a2ba-4420-bb15-7e3fcf706661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "output_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad472d5-5d94-41ad-9417-1318e91bbb33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_df.rename(columns={\"target\":\"target_text\", \"abstract\":\"source_text\"}, inplace = True)\n",
    "output_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06773bba-a77b-479c-996d-9838e2e8a9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93795d07-ba9f-4ba0-87da-666e2d24a9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de3d8c-92ff-42f7-b4d2-0e4972e72c9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"google/long-t5-local-base\"\n",
    "# model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fined-tuned for summarization\n",
    "# longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "# longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "from simplet5 import SimpleT5\n",
    "model=SimpleT5()\n",
    "model.from_pretrained(model_type=\"t5\",model_name=\"pszemraj/long-t5-tglobal-base-16384-book-summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d919a104-c929-4dee-a117-1ac965a19cab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name  | Type                       | Params\n",
      "-----------------------------------------------------\n",
      "0 | model | T5ForConditionalGeneration | 247 M \n",
      "-----------------------------------------------------\n",
      "247 M     Trainable params\n",
      "0         Non-trainable params\n",
      "247 M     Total params\n",
      "990.311   Total estimated model params size (MB)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /mnt/wekamount/RI-Users/amir.moazami/Projects/266/266_final_proj/lightning_logs/version_259871/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2022 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  39%|███▉      | 795/2022 [16:34<25:34,  1.25s/it, loss=2.85, v_num=259871, train_loss_step=3.140]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model.train(train_df=output_df,\n",
    "            eval_df=output_df, \n",
    "            source_max_token_len=2048, \n",
    "            target_max_token_len=1024, \n",
    "            batch_size=2, max_epochs=5, use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c4ad0a-ff95-4fe0-b384-c304d26018bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e607171c-c4f9-47e6-8aa6-1cce1f533d4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:907: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  if not transposed:\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  else:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Conclusions The results of this meta- analysis suggest that the use of probiotics in infants is associated with a reduction in risk of NAFLD.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.predict(output_df['source_text'][0])\n",
    "\n",
    "model.load_model(\"t5\",\"outputs/simplet5-epoch-0-train-loss-3.1796-val-loss-2.5885\", use_gpu=True)\n",
    "\n",
    "text_to_summarize=output_df['source_text'][0]\n",
    "model.predict(text_to_summarize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bbfa078-9ca0-4c02-817c-77344c607e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "                       \n",
    "        \n",
    "#         # Combine the summaries from each abstract\n",
    "#         combined_summary += summary + ' '\n",
    "\n",
    "#     return {\"review_id\": review_id, \"summary\": combined_summary.strip()}\n",
    "\n",
    "# Apply the function to each element of the dataset\n",
    "# summaries_dataset = dataset.map(process_row)\n",
    "\n",
    "# # Convert to pandas DataFrame\n",
    "# df = pd.DataFrame(summaries_dataset)\n",
    "# df = df[['review_id', 'summary']]\n",
    "# # Save to CSV\n",
    "# csv_file_path = 'test.csv'  # Update with your desired file path\n",
    "# df.to_csv(csv_file_path, index=True)\n",
    "\n",
    "# print(f\"Saved summaries to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2216ee1-7af6-4e00-b894-c18ba2e65d15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
