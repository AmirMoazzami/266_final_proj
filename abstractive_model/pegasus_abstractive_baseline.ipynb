{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import gc  # For garbage collection\n",
    "import pickle\n",
    "import csv\n",
    "from pprint import pprint  # Makes output readable without horizontal scrolling\n",
    "\n",
    "# PyTorch and Transformers imports\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "\n",
    "# Dataset import\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\n",
    "# Use select to create a subset\n",
    "# dataset = dataset.select(range(20,30)) \n",
    "\n",
    "\n",
    "# Check for device availability and set accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Metal Performance Shaders) device.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA and MPS not available. Using CPU.\")\n",
    "    \n",
    "    \n",
    "# Initialize the Pegasus tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\").to(device)\n",
    "\n",
    "# Tokenize the inputs (assuming df_dev is previously defined)\n",
    "inputs = tokenizer(df_dev['abstract'].tolist(), max_length=1024, \n",
    "                    truncation=True, padding=\"max_length\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Define the batch size and initialize the results list\n",
    "batch_size = 10\n",
    "results = []\n",
    "\n",
    "# Generate summaries in batches\n",
    "for i in range(0, len(inputs['input_ids']), batch_size):\n",
    "    input_ids_batch = inputs['input_ids'][i:i+batch_size]\n",
    "    review_ids_batch = df_dev['review_id'][i:i+batch_size].tolist()\n",
    "    summary_ids = model.generate(input_ids_batch,\n",
    "                                 num_beams=2,\n",
    "                                 no_repeat_ngram_size=2,\n",
    "                                 min_length=10,\n",
    "                                 max_length=512,\n",
    "                                 early_stopping=True)\n",
    "    batch_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "    for review_id, summary in zip(review_ids_batch, batch_summaries):\n",
    "        results.append({'review_id': review_id, 'Summary': summary})\n",
    "\n",
    "# Convert the results to a DataFrame and save to a CSV file\n",
    "summaries_df_val = pd.DataFrame(results)\n",
    "output_file_val = 'val-prediction.csv'\n",
    "summaries_df_val.to_csv(output_file_val, index=True)\n",
    "print(f\"Saved summaries to {output_file_val}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
