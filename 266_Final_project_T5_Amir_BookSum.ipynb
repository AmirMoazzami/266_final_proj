{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-as-BAR4ltF",
    "outputId": "2309438e-2a71-4874-ac69-06bbc8deb689",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-06 02:32:49--  https://ai2-s2-mslr.s3.us-west-2.amazonaws.com/mslr_data.tar.gz\n",
      "Resolving ai2-s2-mslr.s3.us-west-2.amazonaws.com (ai2-s2-mslr.s3.us-west-2.amazonaws.com)... 52.92.248.218, 52.92.250.18, 3.5.82.215, ...\n",
      "Connecting to ai2-s2-mslr.s3.us-west-2.amazonaws.com (ai2-s2-mslr.s3.us-west-2.amazonaws.com)|52.92.248.218|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 264402799 (252M) [application/x-tar]\n",
      "Saving to: ‘mslr_data.tar.gz’\n",
      "\n",
      "mslr_data.tar.gz    100%[===================>] 252.15M  8.87MB/s    in 29s     \n",
      "\n",
      "2023-11-06 02:33:19 (8.78 MB/s) - ‘mslr_data.tar.gz’ saved [264402799/264402799]\n",
      "\n",
      "mslr_data/\n",
      "mslr_data/cochrane/\n",
      "mslr_data/cochrane/dev-targets.csv\n",
      "mslr_data/cochrane/train-targets.csv\n",
      "mslr_data/cochrane/dev-inputs.csv\n",
      "mslr_data/cochrane/train-inputs.csv\n",
      "mslr_data/cochrane/test-inputs.csv\n",
      "mslr_data/ms2/\n",
      "mslr_data/ms2/dev-reviews-info.csv\n",
      "mslr_data/ms2/dev-targets.csv\n",
      "mslr_data/ms2/train-targets.csv\n",
      "mslr_data/ms2/convert_to_cochrane.py\n",
      "mslr_data/ms2/dev-inputs.csv\n",
      "mslr_data/ms2/train-reviews-info.csv\n",
      "mslr_data/ms2/train-inputs.csv\n",
      "mslr_data/ms2/test-reviews-info.csv\n",
      "mslr_data/ms2/readme.txt\n",
      "mslr_data/ms2/test-inputs.csv\n",
      "rm: cannot remove 'sample_data/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "#Download and Extract the Dataset:\n",
    "!wget https://ai2-s2-mslr.s3.us-west-2.amazonaws.com/mslr_data.tar.gz\n",
    "!tar -xvf mslr_data.tar.gz\n",
    "\n",
    "#Delete the Cochrane dataset and any other unwanted files:\n",
    "!rm -r mslr_data/cochrane/\n",
    "!rm mslr_data.tar.gz*\n",
    "\n",
    "#Move the ms2 directory up one level and remove the parent mslr_data directory:\n",
    "!mv mslr_data/ms2 ./\n",
    "!rm -r mslr_data/\n",
    "!rm -r sample_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VULGdb9s6Xd1"
   },
   "outputs": [],
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tT6nwRnB5Y-k",
    "outputId": "05c1e78b-40f2-4f18-cfc2-f1520a68d793",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from transformers import PegasusTokenizer, TFPegasusForConditionalGeneration, AutoConfig\n",
    "from rouge_score import rouge_scorer\n",
    "import evaluate\n",
    "\n",
    "#let's make longer output readable without horizontal scrolling\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXyLL8SQ6TdC",
    "outputId": "c524812a-a05c-4854-c3c1-55c5414d57cd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  ReviewID      PMID  \\\n",
      "0           0  30760312  22776744   \n",
      "1           1  30760312  25271670   \n",
      "2           2  30760312   3493740   \n",
      "3           3  30760312   1863023   \n",
      "4           4  30760312  16291984   \n",
      "\n",
      "                                               Title  \\\n",
      "0  Improved Cell Survival and Paracrine Capacity ...   \n",
      "1  Adipose-derived stem cells attenuate pulmonary...   \n",
      "2  Effect of bone marrow mesenchymal stem cells o...   \n",
      "3  Survival in patients with primary pulmonary hy...   \n",
      "4  Sildenafil citrate therapy for pulmonary arter...   \n",
      "\n",
      "                                            Abstract  \n",
      "0  Although transplantation of adult bone marrow ...  \n",
      "1  Abstract We investigated the effect of adipose...  \n",
      "2  The aim of the present study was to investigat...  \n",
      "3  OBJECTIVE To characterize mortality in persons...  \n",
      "4  BACKGROUND Sildenafil inhibits phosphodiestera...  \n",
      "\n",
      "Shape of the dataset: (323608, 5)\n",
      "\n",
      "Data Types:\n",
      " Unnamed: 0     int64\n",
      "ReviewID       int64\n",
      "PMID           int64\n",
      "Title         object\n",
      "Abstract      object\n",
      "dtype: object\n",
      "\n",
      "Missing Values:\n",
      " Unnamed: 0    0\n",
      "ReviewID      0\n",
      "PMID          0\n",
      "Title         0\n",
      "Abstract      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# for train-inputs.csv\n",
    "\n",
    "\n",
    "# Load the train-inputs.csv file\n",
    "train_inputs = pd.read_csv('ms2/train-inputs.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(train_inputs.head())\n",
    "\n",
    "# Get the shape of the dataset\n",
    "print(\"\\nShape of the dataset:\", train_inputs.shape)\n",
    "\n",
    "# Get data types of each column\n",
    "print(\"\\nData Types:\\n\", train_inputs.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\\n\", train_inputs.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "kd-Ik3gXAPk_",
    "outputId": "0dded4b5-3a0c-456b-ff32-c95baec78e8e",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the distribution of the text lengths for Title and Abstract columns\n",
    "train_inputs['Title Length'] = train_inputs['Title'].str.len()\n",
    "train_inputs['Abstract Length'] = train_inputs['Abstract'].str.len()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train_inputs['Title Length'], kde=True).set_title('Distribution of Title Text Lengths')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(train_inputs['Abstract Length'], kde=True).set_title('Distribution of Abstract Text Lengths')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpOB94wsAeMJ",
    "outputId": "183d8e75-1223-4312-cd99-dcb02544ccef",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  ReviewID                                             Target  \\\n",
      "0           0  30760312  Conclusions SC therapy is effective for PAH in...   \n",
      "1           1  19588356  There was a trend for endothelin receptor anta...   \n",
      "2           2  23893797  This present meta- analysis suggests that stat...   \n",
      "3           3  27167891  In conclusion , there is a lack of evidence of...   \n",
      "4           4  28753768  Several PROMs have been identified to evaluate...   \n",
      "\n",
      "                                          Background  \n",
      "0  Background Despite significant progress in dru...  \n",
      "1  BACKGROUND Pulmonary arterial hypertension is ...  \n",
      "2  BACKGROUND To achieve sufficient myocardial pe...  \n",
      "3  Waterpipe tobacco smoking is growing in popula...  \n",
      "4  CONTEXT Impaired sexual function has a signifi...  \n",
      "Number of rows and columns: (14191, 4)\n",
      "Unnamed: 0      0\n",
      "ReviewID        0\n",
      "Target          0\n",
      "Background    209\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the train-targets.csv file\n",
    "train_targets = pd.read_csv('ms2/train-targets.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(train_targets.head())\n",
    "\n",
    "# Get the shape of the dataset\n",
    "print(\"Number of rows and columns:\", train_targets.shape)\n",
    "\n",
    "# Check for missing values\n",
    "print(train_targets.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHGuhArqDXS8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "fFBor-riA7mn",
    "outputId": "b5695956-206c-41bf-cc02-ab35ad6b37eb",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the distribution of the text lengths for Target and Background columns\n",
    "train_targets['Target Length'] = train_targets['Target'].str.len()\n",
    "train_targets['Background Length'] = train_targets['Background'].str.len()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(train_targets['Target Length'], kde=True).set_title('Distribution of Target Text Lengths')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(train_targets['Background Length'], kde=True).set_title('Distribution of Background Text Lengths')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PAUFyPcDomi"
   },
   "source": [
    "**The ReviewID** in the MS^2 dataset serves several purposes:\n",
    "\n",
    "**Unique Identifier for Reviews:** **The ReviewID** serves as a unique identifier for each review in the dataset. This helps in distinguishing one review from another, ensuring that the data can be systematically organized, accessed, and processed.\n",
    "\n",
    "**Joining/Mapping Data Across CSVs:** The presence of **ReviewID** in both the *-inputs.csv and the *-targets.csv files (as well as the *-info.csv) indicates that it can be used as a key to join or map the input data (like Title and Abstract) with the corresponding target data (like Target and Background) for each review. This is crucial for tasks where you might need to combine information from both CSVs.\n",
    "\n",
    "**Reference to External Data Sources:** **The ReviewID** is the PubMed ID of the review. Given that PubMed is a widely used database for biomedical literature, having the PubMed ID allows for easy reference and potential retrieval of the original review from the PubMed database or other related platforms.\n",
    "\n",
    "**Consistency Across Conversions:** The conversion script aims to bring the MS^2 data into a format similar to the Cochrane dataset for the SDP workshop. By retaining identifiers like **the ReviewID**, it ensures that data remains consistent and traceable across different formats or versions of the dataset.\n",
    "\n",
    "**Informing the Model:** For tasks related to summarization or information extraction, knowing **the ReviewID** can potentially help in informing the model about the context or source of the review, especially if external data or metadata associated with that ID is used.\n",
    "\n",
    "In summary, **the ReviewID** is essential for data organization, joining datasets, referencing external sources, ensuring consistency, and potentially informing models about the context of each review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lv-wNlc0FgE3"
   },
   "source": [
    "#**Pegasus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7u_e7oGKIahB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#data prep\n",
    "#group the abstracts by ReviewID\n",
    "# then join them together so that each ReviewID has a single combined text made up of all its abstracts.\n",
    "\n",
    "grouped_abstracts = train_inputs.groupby('ReviewID')['Abstract'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "\n",
    "#This will give you a DataFrame (grouped_abstracts) where each row corresponds to a ReviewID and its combined abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uEYiQFMk475",
    "outputId": "da0661cf-c997-4faa-c782-b114b40a7254",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Target</th>\n",
       "      <th>Background</th>\n",
       "      <th>Target Length</th>\n",
       "      <th>Background Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2137711</td>\n",
       "      <td>A double-blind study was design ed to investig...</td>\n",
       "      <td>9862</td>\n",
       "      <td>Data from 12 controlled trials , involving ove...</td>\n",
       "      <td>Continuing differences of opinion among obstet...</td>\n",
       "      <td>847</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5338265</td>\n",
       "      <td>A DOUBLE-BLIND study on the effect of betameth...</td>\n",
       "      <td>2434</td>\n",
       "      <td>Studies in the United States,9 Canada,10 and t...</td>\n",
       "      <td>Bronchiolitis is a disorder most commonly caus...</td>\n",
       "      <td>172</td>\n",
       "      <td>3177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7577280</td>\n",
       "      <td>Low doses ( 0.05 mg/kg ) of intravenously admi...</td>\n",
       "      <td>9300</td>\n",
       "      <td>Metoclopramide 0.15 and 0.25 mg kg-1 was signi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>232</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7779475</td>\n",
       "      <td>A double-blind r and om selection comparison w...</td>\n",
       "      <td>12634</td>\n",
       "      <td>Firm recommendations for clinical practice are...</td>\n",
       "      <td>Although a number of r and omized controlled t...</td>\n",
       "      <td>203</td>\n",
       "      <td>361.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7847648</td>\n",
       "      <td>A multicenter retrospective audit of  carotid ...</td>\n",
       "      <td>9407</td>\n",
       "      <td>Real-time B-mode ultrasonographic imaging allo...</td>\n",
       "      <td>An estimated 500 000 people in the United Stat...</td>\n",
       "      <td>491</td>\n",
       "      <td>2699.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ReviewID                                           Abstract  Unnamed: 0  \\\n",
       "0   2137711  A double-blind study was design ed to investig...        9862   \n",
       "1   5338265  A DOUBLE-BLIND study on the effect of betameth...        2434   \n",
       "2   7577280  Low doses ( 0.05 mg/kg ) of intravenously admi...        9300   \n",
       "3   7779475  A double-blind r and om selection comparison w...       12634   \n",
       "4   7847648  A multicenter retrospective audit of  carotid ...        9407   \n",
       "\n",
       "                                              Target  \\\n",
       "0  Data from 12 controlled trials , involving ove...   \n",
       "1  Studies in the United States,9 Canada,10 and t...   \n",
       "2  Metoclopramide 0.15 and 0.25 mg kg-1 was signi...   \n",
       "3  Firm recommendations for clinical practice are...   \n",
       "4  Real-time B-mode ultrasonographic imaging allo...   \n",
       "\n",
       "                                          Background  Target Length  \\\n",
       "0  Continuing differences of opinion among obstet...            847   \n",
       "1  Bronchiolitis is a disorder most commonly caus...            172   \n",
       "2                                                NaN            232   \n",
       "3  Although a number of r and omized controlled t...            203   \n",
       "4  An estimated 500 000 people in the United Stat...            491   \n",
       "\n",
       "   Background Length  \n",
       "0              299.0  \n",
       "1             3177.0  \n",
       "2                NaN  \n",
       "3              361.0  \n",
       "4             2699.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `df_abstracts` is your DataFrame containing abstracts and ReviewID\n",
    "merged_df = pd.merge(grouped_abstracts, train_targets, on='ReviewID', how='inner')\n",
    "\n",
    "# Now, the merged DataFrame contains columns from both DataFrames, joined on ReviewID\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ElyzYF_dk475"
   },
   "source": [
    "The following is a more detailed explanation of each tensor:\n",
    "\n",
    "**input_ids**: This tensor has a shape of (batch_size, max_length), where batch_size is the number of inputs in the batch and max_length is the maximum length of the tokenized inputs. Each element of the tensor is an integer ID representing a token. The padding tokens are masked out using the attention_mask tensor.\n",
    "\n",
    "\n",
    "**attention_mask**: This tensor has the same shape as the input_ids tensor. Each element of the tensor is a boolean value indicating whether or not the corresponding token in the input sequence should be attended to. The padding tokens are masked out by setting the corresponding elements of the attention_mask tensor to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266,
     "referenced_widgets": [
      "5532885c6d104b108eef1bddfbd6dcf3",
      "894f0ce4cab44874bb56cf863613f0ce",
      "86a6dcf2e4d9432abfd5f5e2b277e4e3",
      "59c93eb9cb8e4b4da637b5dafed55cc6",
      "aaad611c4d194dfe89daf4bf0becb9e4",
      "2f52606fc1844b4c87c37449dd422af9",
      "f5374549ebea4a94aec3df0af4a7831a",
      "8cfde1d5b5854bbb8bab7a5d8178282a",
      "2b2e88e9a128453ebfd936923923aef6",
      "77b92decffaa4a4d96e63dfd975e90cb",
      "33a994ce50054a60838e63c61480bfb7",
      "5119e4581ca141de8a0743cb98c45ea0",
      "62dbae26182a45da9708fcc628b8f689",
      "295c9880262641fa98e6205c53d94bc3",
      "99b237cb96444ca285b5c9be7cba87bf",
      "7d1b3931c50d482ca09c174911d434a9",
      "64d75164a7f54a2faf6e36921f51c92b",
      "d1c3d1343434478d97e29fa263aee019",
      "91f731aa79f44803adbffe9daba12a28",
      "9d4c1fce75e94b03ac38cae675d48ad0",
      "1b9a3ecbae244ec590924c02ae94c612",
      "eab3563b99124e43a63b7e225eec1df0",
      "3c71fa83705646d49e794c3032734e4c",
      "da259f9339ea4023a4e6c48d3eb653f2",
      "022cb781a6034540a4fb6ce586f3a92f",
      "fbcabd6ec226484fb639331a65f234c0",
      "9d7017518a704194a110770a841f2fca",
      "e3e3fb15e644482eaef85e1651b48931",
      "8f2067dc576f407590e4161286206bf4",
      "d77821d277f34ccf89c5369bd43816a0",
      "e930598b50d948a48ae34889f688e9b6",
      "465da263fa2c49d291c01a9a18d0ed1e",
      "65ea2d87c9f9410d8ceec99b975c84fd",
      "628d30b6d0eb488ab538f3c6b309df36",
      "9f45b4f3253b4c5b8f3a31e6247b0877",
      "69878b7d93da42c1a517036c71dfa69c",
      "2bc86fffa3934e56af7657b9540d92ef",
      "a05d898d90894691bc2574ed763b54f0",
      "6479548a192d496694bd9fa1bc5cf5e8",
      "e20d82dd594044ffa66c31778222b0ab",
      "65a36a66efaf41fe8d93e36999228bba",
      "12d907ef38b641bf9f50a2d6b5fe8439",
      "790d43cbb5c440ed8aeba1244259b9a6",
      "d59964e67a1448868a188731295770b3",
      "693c6efb7e194e21a717da914df0711f",
      "72ae98b83c9a49e4aaf7fae10427ecbc",
      "95205558a9ea4492a886412ef994065d",
      "4e6a9dd21edc43a99233517e3ac6c840",
      "412e6f3d15a74c0eaf1bff1e6a1c7b57",
      "c544430c83b44e58a55bbb41a717234a",
      "6c87555652d24f52a45636382ddd272a",
      "af2fca824ce14a97a64806679d269d2d",
      "053fcc7a7d314469adbff72f85b06f74",
      "c7a16f0c4f6e46899b05e1c74ac99caa",
      "fbd5f28870ff48bfabe8ea81905c487b"
     ]
    },
    "id": "-WhGe-1AJCo4",
    "outputId": "9546bcb7-a76f-4772-9928-7304391a0578",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFPegasusForConditionalGeneration.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFPegasusForConditionalGeneration were not initialized from the PyTorch model and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# # Load Pegasus Model and Tokenizer for CNN/Daily Mail\n",
    "# cnnmodel = TFPegasusForConditionalGeneration.from_pretrained(\"google/pegasus-cnn_dailymail\", from_pt=True)\n",
    "# cnntokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-cnn_dailymail\", from_pt=True)\n",
    "# # Load T5 Model and Tokenizer\n",
    "# # t5_model = TFT5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# # t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpQ9wyVyk475",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(cnntokenizer.modules())\n",
    "\n",
    "\n",
    "#for param in model.base_model.parameters():\n",
    "    # param.requires_grad = False\n",
    "\n",
    "# keyword extractor , title, text classification\n",
    "\n",
    "# for param in cnntokenizer.base_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wepYU4akIp86",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #toeknize the input\n",
    "\n",
    "# # inputs = cnntokenizer(list(grouped_abstracts['Abstract']), max_length=1024, truncation=True, return_tensors=\"tf\", padding=\"max_length\")\n",
    "\n",
    "# # inputs = cnntokenizer(list(grouped_abstracts['Abstract'][:5]), max_length=1024, truncation=True, return_tensors=\"tf\", padding=\"max_length\")\n",
    "# inputs = cnntokenizer(list(merged_df['Abstract'][:2]),\n",
    "#                       max_length=512,\n",
    "#                       truncation=True,\n",
    "#                       return_tensors=\"tf\",\n",
    "#                       padding=\"max_length\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kG51RmbxL7CG"
   },
   "source": [
    "The following is a more detailed explanation of each tensor:\n",
    "\n",
    "**input_ids**: This tensor has a shape of (batch_size, max_length), where batch_size is the number of inputs in the batch and max_length is the maximum length of the tokenized inputs. Each element of the tensor is an integer ID representing a token. The padding tokens are masked out using the attention_mask tensor.\n",
    "\n",
    "\n",
    "**attention_mask**: This tensor has the same shape as the input_ids tensor. Each element of the tensor is a boolean value indicating whether or not the corresponding token in the input sequence should be attended to. The padding tokens are masked out by setting the corresponding elements of the attention_mask tensor to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xwk3hP1vMu-e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #generate the summary using default settings\n",
    "# summary_ids = cnnmodel.generate(\n",
    "#     inputs[\"input_ids\"],\n",
    "#     attention_mask=inputs[\"attention_mask\"]\n",
    "# )\n",
    "\n",
    "# candidates = cnntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ew7VpwW36Xd6",
    "outputId": "51ab7180-f745-48c3-d95a-72683c49b3be",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Input Abstract': 'A DOUBLE-BLIND study on the effect of betamethasone sodium '\n",
      "                   'phosphate in the treatment of 297 infants and children '\n",
      "                   'with acute bronchiolitis was conducted at five hospitals '\n",
      "                   'from December 1963 to June 1965 . For this study , the '\n",
      "                   'investigators design ed a common protocol and st and ard '\n",
      "                   'case report forms for use at the five collaborating '\n",
      "                   'centers . The information obtained permitted an evaluation '\n",
      "                   'of the effects of the  corticosteroid  in bronchiolitis '\n",
      "                   'and provided further insight into the natural history of '\n",
      "                   'the disease . The study was not design ed to explore the '\n",
      "                   'causes of bronchiolitis or the effect of supportive '\n",
      "                   'treatment . In recent years , corticosteroids have been '\n",
      "                   'used in the treatment of bronchiolitis on the hypothesis '\n",
      "                   'that their anti-inflammatory action would reduce '\n",
      "                   'bronchiolar inflammation and swelling . These drugs have '\n",
      "                   'been regarded as ineffective by some investigators and '\n",
      "                   'lifesaving by others . In 1964 , Sussman et al1reported no '\n",
      "                   'change in the clinical course of 49 arthritis , '\n",
      "                   'particularly with actively inflamed large joints , and '\n",
      "                   'ankylosing spondylitis . The fact that indomethacin is a '\n",
      "                   'rapidly effective , non-steroidal , anti-inflammatory '\n",
      "                   'agent , like phenylbutazone , makes it a useful '\n",
      "                   'alternative to predni-steroid therapy . As with many other '\n",
      "                   'drugs used in the treatment of chronic rheumatic disorders '\n",
      "                   ', indomethacin causes undesired effects which are '\n",
      "                   'particularly apt to occur in the early stages of treatment '\n",
      "                   '. Fortunately most of these are trivial and clear rapidly '\n",
      "                   'after withdrawal of the drug . In view of the occurrence '\n",
      "                   'of severe neurological disturbance in seven of our cases '\n",
      "                   'we feel that it would be unwise to ignore and dangerous to '\n",
      "                   'suppress the headache and associated symptoms so commonly '\n",
      "                   'encountered . If these can not be avoided by gradual '\n",
      "                   'induction or by subsequent reduction of dosage , they must '\n",
      "                   'be taken as an indication for stopping the drug . It would '\n",
      "                   'seem reasonable to insist that indomethacin should only be '\n",
      "                   'given with caution to any patient with a history of '\n",
      "                   'depressive illness . Active or recent peptic ulceration '\n",
      "                   'should be regarded as an absolute contraindication , and '\n",
      "                   'therapy should not be resumed in any patient who exhibits '\n",
      "                   'a rash during treatment A double blind trial of '\n",
      "                   'prednisolone treatment was carried out on 95 children with '\n",
      "                   'clinical evidence of epidemic bronchiolitis . The trial '\n",
      "                   'showed that there was no difference between the '\n",
      "                   'prednisoIone and the placebo group in the duration of '\n",
      "                   'symptoms and signs',\n",
      " 'ReviewID': 5338265,\n",
      " 'Summary': 'The information obtained permitted an evaluation of the effects '\n",
      "            'of the corticosteroid in bronchiolitis .<n>The study was not '\n",
      "            'design ed to explore the causes of bronchiolitis or the effect of '\n",
      "            'supportive treatment .'}\n"
     ]
    }
   ],
   "source": [
    "# from pprint import pprint\n",
    "\n",
    "# # Extract the relevant ReviewID for the given candidate\n",
    "# review_id_for_candidate = grouped_abstracts['ReviewID'].iloc[1]\n",
    "\n",
    "# # Extract the original input abstract\n",
    "# input_abstract = grouped_abstracts['Abstract'].iloc[1]\n",
    "\n",
    "# # Construct the data to print\n",
    "# data_to_print = {\n",
    "#     \"ReviewID\": review_id_for_candidate,\n",
    "#     \"Input Abstract\": input_abstract,\n",
    "#     \"Summary\": candidates[1]\n",
    "# }\n",
    "\n",
    "# pprint(data_to_print, compact=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gc0jUd4M6Xd6"
   },
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rIFQH0VV6Xd6",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ypBkBHL6Xd7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "# from pprint import pprint\n",
    "\n",
    "# # Assuming you have already imported the necessary libraries and initialized the model and tokenizer (cnnmodel and cnntokenizer)\n",
    "\n",
    "# # 1. Tokenize the input abstracts\n",
    "# inputs = cnntokenizer(list(merged_df['Abstract'][:5]),\n",
    "#                       max_length=1024,\n",
    "#                       truncation=True,\n",
    "#                       return_tensors=\"tf\",\n",
    "#                       padding=\"max_length\")\n",
    "\n",
    "# # 2. Generate summaries using the PEGASUS model\n",
    "# summary_ids = cnnmodel.generate(\n",
    "#     inputs[\"input_ids\"],\n",
    "#     attention_mask=inputs[\"attention_mask\"],\n",
    "#     max_length=150,  # adjust this as needed\n",
    "#     num_beams=4,\n",
    "#     length_penalty=2.0,\n",
    "#     early_stopping=True\n",
    "# )\n",
    "\n",
    "# # 3. Decode the summaries\n",
    "# candidates = cnntokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "# # Print a sample generated summary\n",
    "# pprint(candidates[0], compact=True)\n",
    "\n",
    "# # 4. Compute the ROUGE scores\n",
    "# rouge = load_metric('rouge')\n",
    "# references = list(merged_df['Target'][:5])  # Only considering the first 5 targets\n",
    "# results = rouge.compute(predictions=candidates, references=references)\n",
    "\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHlTvRGRk476"
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9z-Zh6rzk476",
    "outputId": "4894f1f5-5e81-4053-abb2-095364ac2619"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "2023-11-11 02:54:57.903789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9621 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.block.6.layer.0.LocalSelfAttention.v.weight', 'encoder.block.9.layer.0.LocalSelfAttention.k.weight', 'encoder.block.8.layer.0.LocalSelfAttention.v.weight', 'encoder.block.2.layer.0.LocalSelfAttention.q.weight', 'encoder.block.3.layer.0.LocalSelfAttention.o.weight', 'encoder.block.2.layer.0.LocalSelfAttention.v.weight', 'encoder.block.10.layer.0.LocalSelfAttention.k.weight', 'encoder.block.5.layer.0.LocalSelfAttention.k.weight', 'encoder.block.1.layer.0.LocalSelfAttention.o.weight', 'encoder.block.5.layer.0.LocalSelfAttention.o.weight', 'encoder.block.8.layer.0.LocalSelfAttention.q.weight', 'encoder.block.7.layer.0.LocalSelfAttention.v.weight', 'encoder.block.1.layer.0.LocalSelfAttention.k.weight', 'encoder.block.3.layer.0.LocalSelfAttention.k.weight', 'encoder.block.5.layer.0.LocalSelfAttention.v.weight', 'encoder.block.0.layer.0.LocalSelfAttention.relative_attention_bias.weight', 'encoder.block.9.layer.0.LocalSelfAttention.v.weight', 'encoder.block.9.layer.0.LocalSelfAttention.o.weight', 'encoder.block.11.layer.0.LocalSelfAttention.o.weight', 'encoder.block.1.layer.0.LocalSelfAttention.v.weight', 'encoder.block.6.layer.0.LocalSelfAttention.q.weight', 'encoder.block.11.layer.0.LocalSelfAttention.k.weight', 'encoder.block.7.layer.0.LocalSelfAttention.k.weight', 'encoder.block.4.layer.0.LocalSelfAttention.q.weight', 'encoder.block.6.layer.0.LocalSelfAttention.o.weight', 'encoder.block.2.layer.0.LocalSelfAttention.o.weight', 'encoder.block.10.layer.0.LocalSelfAttention.q.weight', 'encoder.block.10.layer.0.LocalSelfAttention.o.weight', 'encoder.block.8.layer.0.LocalSelfAttention.o.weight', 'encoder.block.10.layer.0.LocalSelfAttention.v.weight', 'encoder.block.6.layer.0.LocalSelfAttention.k.weight', 'encoder.block.0.layer.0.LocalSelfAttention.q.weight', 'encoder.block.7.layer.0.LocalSelfAttention.o.weight', 'encoder.block.9.layer.0.LocalSelfAttention.q.weight', 'encoder.block.4.layer.0.LocalSelfAttention.v.weight', 'encoder.block.7.layer.0.LocalSelfAttention.q.weight', 'encoder.block.0.layer.0.LocalSelfAttention.k.weight', 'encoder.block.5.layer.0.LocalSelfAttention.q.weight', 'encoder.block.11.layer.0.LocalSelfAttention.v.weight', 'encoder.block.4.layer.0.LocalSelfAttention.k.weight', 'encoder.block.8.layer.0.LocalSelfAttention.k.weight', 'encoder.block.11.layer.0.LocalSelfAttention.q.weight', 'encoder.block.0.layer.0.LocalSelfAttention.v.weight', 'encoder.block.2.layer.0.LocalSelfAttention.k.weight', 'encoder.block.1.layer.0.LocalSelfAttention.q.weight', 'encoder.block.3.layer.0.LocalSelfAttention.v.weight', 'encoder.block.0.layer.0.LocalSelfAttention.o.weight', 'encoder.block.4.layer.0.LocalSelfAttention.o.weight', 'encoder.block.3.layer.0.LocalSelfAttention.q.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFT5ForConditionalGeneration were not initialized from the PyTorch model and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load LongT5 Model and Tokenizer\n",
    "longt5_model = TFT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "# longt5_tokenizer = LongT5Tokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "longt5_tokenizer = T5Tokenizer.from_pretrained(\"google/long-t5-local-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lA1r3yqk476",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jy1JEaEPk476"
   },
   "outputs": [],
   "source": [
    "# Tokenization of the output\n",
    "longt5_inputs = longt5_tokenizer(\n",
    "    list(grouped_abstracts['Abstract']),\n",
    "    max_length=4096,  # Adjusted max_length for LongT5's capability\n",
    "    truncation=True,\n",
    "    return_tensors=\"tf\",\n",
    "    padding=\"max_length\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9msRPNqQk476"
   },
   "outputs": [],
   "source": [
    "# # # Generate Summaries\n",
    "# # longt5_summary_ids = longt5_model.generate(\n",
    "# #     longt5_inputs[\"input_ids\"],\n",
    "# #     attention_mask=longt5_inputs[\"attention_mask\"],\n",
    "# #     min_length=30,\n",
    "# #     max_length=200,  # You may adjust this based on your summary length requirements\n",
    "# #     length_penalty=2.0,\n",
    "# #     num_beams=4,\n",
    "# #     early_stopping=True\n",
    "# # )\n",
    "# # Assuming longt5_inputs is successfully created from the previous tokenization step\n",
    "\n",
    "# # Parameters for batch processing\n",
    "# batch_size = 10  # Adjust this based on your GPU's capability\n",
    "# num_abstracts = len(grouped_abstracts['Abstract'])\n",
    "# longt5_summary_ids = []\n",
    "\n",
    "# for i in range(0, num_abstracts, batch_size):\n",
    "#     # Slice the input_ids and attention_mask for the current batch\n",
    "#     input_ids_batch = longt5_inputs[\"input_ids\"][i:i + batch_size]\n",
    "#     attention_mask_batch = longt5_inputs[\"attention_mask\"][i:i + batch_size]\n",
    "\n",
    "#     # Generate summaries for the current batch\n",
    "#     batch_summary_ids = longt5_model.generate(\n",
    "#         input_ids=input_ids_batch,\n",
    "#         attention_mask=attention_mask_batch,\n",
    "#         min_length=30,\n",
    "#         max_length=150,  # Adjust as needed, smaller than original max_length\n",
    "#         length_penalty=2.0,\n",
    "#         num_beams=2,  # Reduced number of beams\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "#     longt5_summary_ids.extend(batch_summary_ids.numpy())\n",
    "\n",
    "# # longt5_summary_ids now contains the generated summaries for all abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XSA-IsTRk476",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Adjust max_length for tokenization\n",
    "# max_token_length = 2048  # Adjust this value as needed\n",
    "\n",
    "# # Tokenization of the output in batches\n",
    "# batch_size = 10  # Adjust based on your system's capability\n",
    "# longt5_summary_ids = []\n",
    "\n",
    "# for i in range(0, len(grouped_abstracts['Abstract']), batch_size):\n",
    "#     batch_abstracts = grouped_abstracts['Abstract'][i:i + batch_size]\n",
    "#     longt5_inputs = longt5_tokenizer(\n",
    "#         batch_abstracts,\n",
    "#         max_length=max_token_length,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"tf\",\n",
    "#         padding=\"max_length\"\n",
    "#     )\n",
    "\n",
    "#     # Generate Summaries in smaller batches\n",
    "#     summary_ids = longt5_model.generate(\n",
    "#         longt5_inputs[\"input_ids\"],\n",
    "#         attention_mask=longt5_inputs[\"attention_mask\"],\n",
    "#         min_length=30,\n",
    "#         max_length=150,  # Adjust as needed\n",
    "#         length_penalty=2.0,\n",
    "#         num_beams=2,  # Reduced number of beams\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "#     longt5_summary_ids.extend(summary_ids)\n",
    "\n",
    "# # Process longt5_summary_ids as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1USz8_hBk476",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust max_length for tokenization\n",
    "# max_token_length = 2048  # Adjust this value as needed\n",
    "\n",
    "# # Tokenization of the output in batches\n",
    "# batch_size = 10  # Adjust based on your system's capability\n",
    "# longt5_summary_ids = []\n",
    "\n",
    "# for i in range(0, len(grouped_abstracts['Abstract']), batch_size):\n",
    "#     batch_abstracts = grouped_abstracts['Abstract'][i:i + batch_size]\n",
    "#     longt5_inputs = longt5_tokenizer(\n",
    "#         batch_abstracts,\n",
    "#         max_length=max_token_length,\n",
    "#         truncation=True,\n",
    "#         return_tensors=\"tf\",\n",
    "#         padding=\"max_length\"\n",
    "#     )\n",
    "\n",
    "#     # Generate Summaries in smaller batches\n",
    "#     summary_ids = longt5_model.generate(\n",
    "#         longt5_inputs[\"input_ids\"],\n",
    "#         attention_mask=longt5_inputs[\"attention_mask\"],\n",
    "#         min_length=30,\n",
    "#         max_length=150,  # Adjust as needed\n",
    "#         length_penalty=2.0,\n",
    "#         num_beams=2,  # Reduced number of beams\n",
    "#         early_stopping=True\n",
    "#     )\n",
    "#     longt5_summary_ids.extend(summary_ids)\n",
    "\n",
    "# # Process longt5_summary_ids as needed\n",
    "# # Tokenize a smaller batch of abstracts with a shorter max_length\n",
    "# longt5_inputs = longt5_tokenizer(\n",
    "#     list(grouped_abstracts['Abstract'][:2]),  # Taking only 2 abstracts as an example\n",
    "#     max_length=512,  # Using a smaller max_length\n",
    "#     truncation=True,\n",
    "#     return_tensors=\"tf\",\n",
    "#     padding=\"max_length\"\n",
    "# )\n",
    "\n",
    "# # Generate summaries\n",
    "# longt5_summary_ids = longt5_model.generate(\n",
    "#     longt5_inputs[\"input_ids\"],\n",
    "#     attention_mask=longt5_inputs[\"attention_mask\"]\n",
    "# )\n",
    "\n",
    "# # Decode the summaries\n",
    "# longt5_summaries = longt5_tokenizer.batch_decode(longt5_summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "# # Example: printing the first summary\n",
    "# print(\"Summary:\", longt5_summaries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLZK-ylAk476",
    "outputId": "f005dabf-8b6f-46e1-8798-4a53121f1fe4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/generation/tf_utils.py:834: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "2023-11-11 02:55:31.593910: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xa7fec680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-11-11 02:55:31.593960: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2023-11-11 02:55:31.599143: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-11-11 02:55:31.836517: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800\n",
      "2023-11-11 02:55:31.850037: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2023-11-11 02:55:31.910542: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: loud catalogue catalogue catalogue scolaire AMDsho scolaire greeting greeting greeting greeting greetingbull Pop greeting greeting greeting greeting\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a smaller batch of abstracts with a shorter max_length\n",
    "longt5_inputs = longt5_tokenizer(\n",
    "    list(grouped_abstracts['Abstract'][:5]),  # Taking only 5 abstracts as an example\n",
    "    max_length=512,  # Using a smaller max_length\n",
    "    truncation=True,\n",
    "    return_tensors=\"tf\",\n",
    "    padding=\"max_length\"\n",
    ")\n",
    "\n",
    "# Generate summaries\n",
    "longt5_summary_ids = longt5_model.generate(\n",
    "    longt5_inputs[\"input_ids\"],\n",
    "    attention_mask=longt5_inputs[\"attention_mask\"]\n",
    ")\n",
    "\n",
    "# Decode the summaries\n",
    "longt5_summaries = longt5_tokenizer.batch_decode(longt5_summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "# Example: printing the first summary\n",
    "print(\"Summary:\", longt5_summaries[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-BpExTsk476",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Decode Summaries\n",
    "longt5_summaries = longt5_tokenizer.batch_decode(\n",
    "    longt5_summary_ids,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cTQ721DIk477"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZY3JgUck477",
    "outputId": "b2af88bf-2727-49a3-f76a-f6364a5e4069",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rouge2': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeL': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0)), 'rougeLsum': AggregateScore(low=Score(precision=0.0, recall=0.0, fmeasure=0.0), mid=Score(precision=0.0, recall=0.0, fmeasure=0.0), high=Score(precision=0.0, recall=0.0, fmeasure=0.0))}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Model Output\n",
    "from datasets import load_metric\n",
    "\n",
    "rouge_metric = load_metric('rouge')\n",
    "list_of_references = list(merged_df['Target'][:5])  # Only considering the first 5 targets\n",
    "longt5_rouge_results = rouge_metric.compute(predictions=longt5_summaries, references=list_of_references)\n",
    "print(longt5_rouge_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQhev2Y0k477"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqHidmH3k477"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Pk46O6rk477"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "svHI6R1Uk477"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6Kk6-xyk477"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tGim3qyYk477",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-12 04:08:52.011637: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-12 04:08:52.068844: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NcepLf7Bk47_",
    "outputId": "f2c07271-e470-4af7-9db3-45cf405f502c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jwsjdKTk47_"
   },
   "source": [
    "# New try with T5:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Fs_v3XLlJOD"
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETnm_COpk47_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bein9f75k47_",
    "outputId": "dfe6c565-d4d9-4e9d-e203-2b6d01a95d00",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type longt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFT5ForConditionalGeneration: ['encoder.block.6.layer.0.LocalSelfAttention.v.weight', 'encoder.block.9.layer.0.LocalSelfAttention.k.weight', 'encoder.block.8.layer.0.LocalSelfAttention.v.weight', 'encoder.block.2.layer.0.LocalSelfAttention.q.weight', 'encoder.block.3.layer.0.LocalSelfAttention.o.weight', 'encoder.block.2.layer.0.LocalSelfAttention.v.weight', 'encoder.block.10.layer.0.LocalSelfAttention.k.weight', 'encoder.block.5.layer.0.LocalSelfAttention.k.weight', 'encoder.block.1.layer.0.LocalSelfAttention.o.weight', 'encoder.block.5.layer.0.LocalSelfAttention.o.weight', 'encoder.block.8.layer.0.LocalSelfAttention.q.weight', 'encoder.block.7.layer.0.LocalSelfAttention.v.weight', 'encoder.block.1.layer.0.LocalSelfAttention.k.weight', 'encoder.block.3.layer.0.LocalSelfAttention.k.weight', 'encoder.block.5.layer.0.LocalSelfAttention.v.weight', 'encoder.block.0.layer.0.LocalSelfAttention.relative_attention_bias.weight', 'encoder.block.9.layer.0.LocalSelfAttention.v.weight', 'encoder.block.9.layer.0.LocalSelfAttention.o.weight', 'encoder.block.11.layer.0.LocalSelfAttention.o.weight', 'encoder.block.1.layer.0.LocalSelfAttention.v.weight', 'encoder.block.6.layer.0.LocalSelfAttention.q.weight', 'encoder.block.11.layer.0.LocalSelfAttention.k.weight', 'encoder.block.7.layer.0.LocalSelfAttention.k.weight', 'encoder.block.4.layer.0.LocalSelfAttention.q.weight', 'encoder.block.6.layer.0.LocalSelfAttention.o.weight', 'encoder.block.2.layer.0.LocalSelfAttention.o.weight', 'encoder.block.10.layer.0.LocalSelfAttention.q.weight', 'encoder.block.10.layer.0.LocalSelfAttention.o.weight', 'encoder.block.8.layer.0.LocalSelfAttention.o.weight', 'encoder.block.10.layer.0.LocalSelfAttention.v.weight', 'encoder.block.6.layer.0.LocalSelfAttention.k.weight', 'encoder.block.0.layer.0.LocalSelfAttention.q.weight', 'encoder.block.7.layer.0.LocalSelfAttention.o.weight', 'encoder.block.9.layer.0.LocalSelfAttention.q.weight', 'encoder.block.4.layer.0.LocalSelfAttention.v.weight', 'encoder.block.7.layer.0.LocalSelfAttention.q.weight', 'encoder.block.0.layer.0.LocalSelfAttention.k.weight', 'encoder.block.5.layer.0.LocalSelfAttention.q.weight', 'encoder.block.11.layer.0.LocalSelfAttention.v.weight', 'encoder.block.4.layer.0.LocalSelfAttention.k.weight', 'encoder.block.8.layer.0.LocalSelfAttention.k.weight', 'encoder.block.11.layer.0.LocalSelfAttention.q.weight', 'encoder.block.0.layer.0.LocalSelfAttention.v.weight', 'encoder.block.2.layer.0.LocalSelfAttention.k.weight', 'encoder.block.1.layer.0.LocalSelfAttention.q.weight', 'encoder.block.3.layer.0.LocalSelfAttention.v.weight', 'encoder.block.0.layer.0.LocalSelfAttention.o.weight', 'encoder.block.4.layer.0.LocalSelfAttention.o.weight', 'encoder.block.3.layer.0.LocalSelfAttention.q.weight']\n",
      "- This IS expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFT5ForConditionalGeneration from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFT5ForConditionalGeneration were not initialized from the PyTorch model and are newly initialized: ['encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load LongT5 Model and Tokenizer\n",
    "longt5_model = TFT5ForConditionalGeneration.from_pretrained(\"google/long-t5-local-base\")\n",
    "longt5_tokenizer = T5Tokenizer.from_pretrained(\"google/long-t5-local-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yrA3fUQEk47_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenization of the input\n",
    "batch_size = 5  # Adjust the batch size according to your memory constraints\n",
    "max_token_length = 512  # Adjust the max token length according to the model's capability and your dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35ppoyYUk47_",
    "outputId": "704c5d2d-c0f8-43f3-b6b6-3c616852055d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/generation/tf_utils.py:834: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/slurm-252732/ipykernel_2002373/1813980786.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     longt5_inputs = longt5_tokenizer(\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_abstracts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_token_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m                     \u001b[0;34mf\"num_return_sequences has to be 1, but is {generation_config.num_return_sequences} when doing\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                     \u001b[0;34m\" greedy search.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                 )\n\u001b[1;32m    902\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m                 \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mpad_token_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, input_ids, max_length, pad_token_id, eos_token_id, logits_processor, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m         \u001b[0;31m# 2-to-n generation steps can then be run in autoregressive fashion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m         \u001b[0;31m# only in case 1st generation step does NOT yield EOS token though\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m         \u001b[0mmaximum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m         generated, _, cur_len, _ = tf.while_loop(\n\u001b[0m\u001b[1;32m   1733\u001b[0m             \u001b[0mgreedy_search_cond_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0mgreedy_search_body_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinished_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m                   \u001b[0m_call_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorator_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_qualified_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                   \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m                   ('after %s' % date), instructions)\n\u001b[0;32m--> 648\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/tensorflow/python/ops/while_loop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;34m...\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m   \"\"\"\n\u001b[0;32m--> 252\u001b[0;31m   return while_loop(\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0mcond\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m       \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/tensorflow/python/ops/while_loop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    530\u001b[0m                                     return_same_structure)\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/tensorflow/python/ops/while_loop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(i, lv)\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(generated, finished_sequences, cur_len, model_kwargs)\u001b[0m\n\u001b[1;32m   1649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m             \u001b[0;31m# forward pass to get next token logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             model_outputs = self(\n\u001b[0m\u001b[1;32m   1654\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mcopied_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcopied_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0mlayout_map_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_subclass_model_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                     )\n\u001b[1;32m   1158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/t5/modeling_tf_t5.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0;31m# get decoder inputs from shifting lm labels to the right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m             \u001b[0mdecoder_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shift_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1359\u001b[0m             \u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                     )\n\u001b[1;32m   1158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0munpacked_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_processing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfn_args_and_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munpacked_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/t5/modeling_tf_t5.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, encoder_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    779\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                 \u001b[0mposition_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                     )\n\u001b[1;32m   1158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/t5/modeling_tf_t5.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, encoder_layer_head_mask, past_key_value, use_cache, output_attentions, training)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;31m# Keep cross-attention outputs and relative position weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0mattention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_outputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;31m# Apply Feed Forward layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;31m# Add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                     )\n\u001b[1;32m   1158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/t5/modeling_tf_t5.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, hidden_states, training)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0mdense_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDenseReluDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/engine/base_layer.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1157\u001b[0m                     )\n\u001b[1;32m   1158\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_saved_model_inputs_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_save_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/t5/modeling_tf_t5.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mvariance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/266/.venv/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m  11317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11318\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11319\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11320\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11321\u001b[0;31m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11323\u001b[0m       _result = _dispatcher_for_square(\n\u001b[1;32m  11324\u001b[0m           (x, name,), None)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Process in batches\n",
    "longt5_summaries = []\n",
    "rouge_metric = load_metric('rouge')\n",
    "list_of_references = list(merged_df['Target'])\n",
    "\n",
    "for i in range(0, len(grouped_abstracts['Abstract']), batch_size):\n",
    "    batch_abstracts = grouped_abstracts['Abstract'][i:i+batch_size]\n",
    "    longt5_inputs = longt5_tokenizer(\n",
    "        batch_abstracts.tolist(),\n",
    "        max_length=max_token_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\",\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Generate summaries\n",
    "    summary_ids = longt5_model.generate(\n",
    "        longt5_inputs[\"input_ids\"],\n",
    "        attention_mask=longt5_inputs[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    # Decode summaries\n",
    "    batch_summaries = longt5_tokenizer.batch_decode(\n",
    "        summary_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "\n",
    "    longt5_summaries.extend(batch_summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjXIRwX2k47_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate Model Output\n",
    "longt5_rouge_results = rouge_metric.compute(predictions=longt5_summaries[:len(list_of_references)], references=list_of_references)\n",
    "print(longt5_rouge_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WM36A94mMxHP"
   },
   "source": [
    "## LongT5 with fine-tuned model (\"BookSum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPiS7bKdk47_",
    "outputId": "c5be7531-da38-41bf-f994-c627377b8719",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets sentencepiece rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu121\n"
     ]
    }
   ],
   "source": [
    "!pip -q install torch\n",
    "!pip install -q --upgrade transformers datasets sentencepiece\n",
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiRN0CuPk47_",
    "outputId": "148069e3-30ef-46a6-ac5d-792c4d2d8969",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF is set to: max_split_size_mb:256\n",
      "device: cuda\n",
      "['review_id', 'pmid', 'title', 'abstract', 'target', 'background']\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "from transformers import AutoTokenizer, LongT5ForConditionalGeneration\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Memory optimization for CUDA\n",
    "max_split_size_mb = 256  # Set the max_split_size_mb value (e.g., 512 MB)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = f\"max_split_size_mb:{max_split_size_mb}\"\n",
    "print(f\"PYTORCH_CUDA_ALLOC_CONF is set to: {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# Load LongT5 Model and Tokenizer\n",
    "# model_to_use = \"google/long-t5-local-base\"\n",
    "model_to_use = \"pszemraj/long-t5-tglobal-base-16384-book-summary\"  # fined-tuned for summarization\n",
    "longt5_model = LongT5ForConditionalGeneration.from_pretrained(model_to_use).to(device)\n",
    "longt5_tokenizer = AutoTokenizer.from_pretrained(model_to_use)\n",
    "\n",
    "# Load validation dataset from Hugging Face datasets\n",
    "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split='validation')\n",
    "\n",
    "# Prepare DataFrame for output\n",
    "output_df = pd.DataFrame(columns=['ReviewID', 'Candidate_Summary', 'Target'])\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ncCISFPIBszM",
    "outputId": "d70f54c2-d4b7-4cfd-fa2e-e17044022230",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LongT5Config {\n",
       "  \"_name_or_path\": \"pszemraj/long-t5-tglobal-base-16384-book-summary\",\n",
       "  \"architectures\": [\n",
       "    \"LongT5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dense_act_fn\": \"gelu_new\",\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_type\": \"transient-global\",\n",
       "  \"encoder_no_repeat_ngram_size\": 4,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"global_block_size\": 16,\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"is_gated_act\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"length_penalty\": 0.8,\n",
       "  \"local_radius\": 127,\n",
       "  \"max_length\": 512,\n",
       "  \"min_length\": 8,\n",
       "  \"model_type\": \"longt5\",\n",
       "  \"n_positions\": 4096,\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"num_beams\": 2,\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"repetition_penalty\": 3.5,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.35.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32128\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longt5_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GD8JFY1yk47_",
    "outputId": "8cb36b6a-992c-4a79-e0b0-0dad9d2f3e53",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11754], which does not match the required output shape [1, 11754]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 92, 128, 1], which does not match the required output shape [1, 92, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/modeling_utils.py:861: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4244], which does not match the required output shape [1, 4244]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 34, 128, 1], which does not match the required output shape [1, 34, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [1746], which does not match the required output shape [1, 1746]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 14, 128, 1], which does not match the required output shape [1, 14, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [12116], which does not match the required output shape [1, 12116]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 95, 128, 1], which does not match the required output shape [1, 95, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6967], which does not match the required output shape [1, 6967]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 55, 128, 1], which does not match the required output shape [1, 55, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [10608], which does not match the required output shape [1, 10608]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 83, 128, 1], which does not match the required output shape [1, 83, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5415], which does not match the required output shape [1, 5415]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 43, 128, 1], which does not match the required output shape [1, 43, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7677], which does not match the required output shape [1, 7677]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 60, 128, 1], which does not match the required output shape [1, 60, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [16346], which does not match the required output shape [1, 16346]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 128, 128, 1], which does not match the required output shape [1, 128, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [10662], which does not match the required output shape [1, 10662]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 84, 128, 1], which does not match the required output shape [1, 84, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9998], which does not match the required output shape [1, 9998]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 79, 128, 1], which does not match the required output shape [1, 79, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [14848], which does not match the required output shape [1, 14848]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 116, 128, 1], which does not match the required output shape [1, 116, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4212], which does not match the required output shape [1, 4212]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 33, 128, 1], which does not match the required output shape [1, 33, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3760], which does not match the required output shape [1, 3760]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 30, 128, 1], which does not match the required output shape [1, 30, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11337], which does not match the required output shape [1, 11337]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 89, 128, 1], which does not match the required output shape [1, 89, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11560], which does not match the required output shape [1, 11560]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 91, 128, 1], which does not match the required output shape [1, 91, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3827], which does not match the required output shape [1, 3827]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5326], which does not match the required output shape [1, 5326]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 42, 128, 1], which does not match the required output shape [1, 42, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9210], which does not match the required output shape [1, 9210]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 72, 128, 1], which does not match the required output shape [1, 72, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4315], which does not match the required output shape [1, 4315]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9071], which does not match the required output shape [1, 9071]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 71, 128, 1], which does not match the required output shape [1, 71, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6368], which does not match the required output shape [1, 6368]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 50, 128, 1], which does not match the required output shape [1, 50, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9822], which does not match the required output shape [1, 9822]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 77, 128, 1], which does not match the required output shape [1, 77, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4026], which does not match the required output shape [1, 4026]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 32, 128, 1], which does not match the required output shape [1, 32, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4748], which does not match the required output shape [1, 4748]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 38, 128, 1], which does not match the required output shape [1, 38, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [12395], which does not match the required output shape [1, 12395]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 97, 128, 1], which does not match the required output shape [1, 97, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [10676], which does not match the required output shape [1, 10676]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [15650], which does not match the required output shape [1, 15650]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 123, 128, 1], which does not match the required output shape [1, 123, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7471], which does not match the required output shape [1, 7471]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 59, 128, 1], which does not match the required output shape [1, 59, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5186], which does not match the required output shape [1, 5186]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 41, 128, 1], which does not match the required output shape [1, 41, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5331], which does not match the required output shape [1, 5331]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2895], which does not match the required output shape [1, 2895]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 23, 128, 1], which does not match the required output shape [1, 23, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9162], which does not match the required output shape [1, 9162]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5703], which does not match the required output shape [1, 5703]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 45, 128, 1], which does not match the required output shape [1, 45, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5031], which does not match the required output shape [1, 5031]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 40, 128, 1], which does not match the required output shape [1, 40, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11948], which does not match the required output shape [1, 11948]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 94, 128, 1], which does not match the required output shape [1, 94, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [10115], which does not match the required output shape [1, 10115]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 80, 128, 1], which does not match the required output shape [1, 80, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [16384], which does not match the required output shape [1, 16384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4018], which does not match the required output shape [1, 4018]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5435], which does not match the required output shape [1, 5435]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [12778], which does not match the required output shape [1, 12778]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 100, 128, 1], which does not match the required output shape [1, 100, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5824], which does not match the required output shape [1, 5824]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 46, 128, 1], which does not match the required output shape [1, 46, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [1083], which does not match the required output shape [1, 1083]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 9, 128, 1], which does not match the required output shape [1, 9, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4003], which does not match the required output shape [1, 4003]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6553], which does not match the required output shape [1, 6553]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 52, 128, 1], which does not match the required output shape [1, 52, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7303], which does not match the required output shape [1, 7303]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 58, 128, 1], which does not match the required output shape [1, 58, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [15603], which does not match the required output shape [1, 15603]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 122, 128, 1], which does not match the required output shape [1, 122, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7335], which does not match the required output shape [1, 7335]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2678], which does not match the required output shape [1, 2678]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 21, 128, 1], which does not match the required output shape [1, 21, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8778], which does not match the required output shape [1, 8778]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 69, 128, 1], which does not match the required output shape [1, 69, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2575], which does not match the required output shape [1, 2575]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9730], which does not match the required output shape [1, 9730]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5187], which does not match the required output shape [1, 5187]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6698], which does not match the required output shape [1, 6698]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 53, 128, 1], which does not match the required output shape [1, 53, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [12780], which does not match the required output shape [1, 12780]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5920], which does not match the required output shape [1, 5920]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 47, 128, 1], which does not match the required output shape [1, 47, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11516], which does not match the required output shape [1, 11516]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 90, 128, 1], which does not match the required output shape [1, 90, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8082], which does not match the required output shape [1, 8082]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 64, 128, 1], which does not match the required output shape [1, 64, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5157], which does not match the required output shape [1, 5157]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9001], which does not match the required output shape [1, 9001]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [15035], which does not match the required output shape [1, 15035]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 118, 128, 1], which does not match the required output shape [1, 118, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7814], which does not match the required output shape [1, 7814]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 62, 128, 1], which does not match the required output shape [1, 62, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8443], which does not match the required output shape [1, 8443]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 66, 128, 1], which does not match the required output shape [1, 66, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8134], which does not match the required output shape [1, 8134]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9570], which does not match the required output shape [1, 9570]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 75, 128, 1], which does not match the required output shape [1, 75, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2199], which does not match the required output shape [1, 2199]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 18, 128, 1], which does not match the required output shape [1, 18, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3368], which does not match the required output shape [1, 3368]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 27, 128, 1], which does not match the required output shape [1, 27, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [12246], which does not match the required output shape [1, 12246]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 96, 128, 1], which does not match the required output shape [1, 96, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11277], which does not match the required output shape [1, 11277]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5417], which does not match the required output shape [1, 5417]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4991], which does not match the required output shape [1, 4991]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 39, 128, 1], which does not match the required output shape [1, 39, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2066], which does not match the required output shape [1, 2066]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 17, 128, 1], which does not match the required output shape [1, 17, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3763], which does not match the required output shape [1, 3763]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6270], which does not match the required output shape [1, 6270]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 49, 128, 1], which does not match the required output shape [1, 49, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [13422], which does not match the required output shape [1, 13422]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 105, 128, 1], which does not match the required output shape [1, 105, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8233], which does not match the required output shape [1, 8233]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 65, 128, 1], which does not match the required output shape [1, 65, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [10321], which does not match the required output shape [1, 10321]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 81, 128, 1], which does not match the required output shape [1, 81, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3323], which does not match the required output shape [1, 3323]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 26, 128, 1], which does not match the required output shape [1, 26, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3043], which does not match the required output shape [1, 3043]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 24, 128, 1], which does not match the required output shape [1, 24, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8253], which does not match the required output shape [1, 8253]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3128], which does not match the required output shape [1, 3128]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 25, 128, 1], which does not match the required output shape [1, 25, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9046], which does not match the required output shape [1, 9046]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [16369], which does not match the required output shape [1, 16369]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11583], which does not match the required output shape [1, 11583]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5074], which does not match the required output shape [1, 5074]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9532], which does not match the required output shape [1, 9532]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4226], which does not match the required output shape [1, 4226]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3780], which does not match the required output shape [1, 3780]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7446], which does not match the required output shape [1, 7446]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6308], which does not match the required output shape [1, 6308]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8294], which does not match the required output shape [1, 8294]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6090], which does not match the required output shape [1, 6090]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 48, 128, 1], which does not match the required output shape [1, 48, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [10568], which does not match the required output shape [1, 10568]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [10127], which does not match the required output shape [1, 10127]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3138], which does not match the required output shape [1, 3138]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2108], which does not match the required output shape [1, 2108]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11465], which does not match the required output shape [1, 11465]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6526], which does not match the required output shape [1, 6526]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 51, 128, 1], which does not match the required output shape [1, 51, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6931], which does not match the required output shape [1, 6931]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7776], which does not match the required output shape [1, 7776]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 61, 128, 1], which does not match the required output shape [1, 61, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6912], which does not match the required output shape [1, 6912]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 54, 128, 1], which does not match the required output shape [1, 54, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8497], which does not match the required output shape [1, 8497]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 67, 128, 1], which does not match the required output shape [1, 67, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2847], which does not match the required output shape [1, 2847]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [469], which does not match the required output shape [1, 469]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 4, 128, 1], which does not match the required output shape [1, 4, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6759], which does not match the required output shape [1, 6759]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [13656], which does not match the required output shape [1, 13656]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 107, 128, 1], which does not match the required output shape [1, 107, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [2909], which does not match the required output shape [1, 2909]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9438], which does not match the required output shape [1, 9438]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 74, 128, 1], which does not match the required output shape [1, 74, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [15871], which does not match the required output shape [1, 15871]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 124, 128, 1], which does not match the required output shape [1, 124, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7963], which does not match the required output shape [1, 7963]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 63, 128, 1], which does not match the required output shape [1, 63, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4684], which does not match the required output shape [1, 4684]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 37, 128, 1], which does not match the required output shape [1, 37, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4245], which does not match the required output shape [1, 4245]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [4570], which does not match the required output shape [1, 4570]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 36, 128, 1], which does not match the required output shape [1, 36, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [9803], which does not match the required output shape [1, 9803]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [8562], which does not match the required output shape [1, 8562]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [7218], which does not match the required output shape [1, 7218]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 57, 128, 1], which does not match the required output shape [1, 57, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6366], which does not match the required output shape [1, 6366]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [3009], which does not match the required output shape [1, 3009]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [11812], which does not match the required output shape [1, 11812]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:146: UserWarning: An output with one or more elements was resized since it had shape [1, 93, 128, 1], which does not match the required output shape [1, 93, 128, 384]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  local_attention_mask = torch.logical_and(_blocked_attention_mask, _3blocked_attention_mask)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [6191], which does not match the required output shape [1, 6191]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n",
      "/mnt/wekamount/RI-Users/amir.moazami/Projects/266/.venv/lib/python3.8/site-packages/transformers/models/longt5/modeling_longt5.py:170: UserWarning: An output with one or more elements was resized since it had shape [5906], which does not match the required output shape [1, 5906]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:28.)\n",
      "  true_block_ends = torch.logical_and(block_ends, block_ids >= 0)\n"
     ]
    }
   ],
   "source": [
    "# Assuming the longt5_tokenizer and longt5_model are already correctly initialized\n",
    "\n",
    "batch_size = 1  # this is my limit in Google Colab T4 before I start getting OOM issues\n",
    "max_token_length = 16384  # truly the limit of LongT5, but can reduce by multiples of 2 to optimize for speed if needed\n",
    "debugging = False  # for debugging\n",
    "\n",
    "# Initialize an empty DataFrame for the output\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "# Process validation data in batches and generate summaries\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    if debugging:\n",
    "      print(f\"Processing batch: review numbers {i} to {i + batch_size - 1}\")\n",
    "    batch_abstracts = dataset['abstract'][i: i + batch_size]\n",
    "    batch_review_ids = dataset['review_id'][i: i + batch_size]\n",
    "    batch_targets = dataset['target'][i: i + batch_size] if 'target' in dataset.column_names else [''] * batch_size\n",
    "    if debugging:\n",
    "      print(f\"{len(batch_abstracts)=}\")\n",
    "\n",
    "    # Tokenize\n",
    "    val_inputs = longt5_tokenizer.batch_encode_plus(\n",
    "        [\"\".join([\"Study 1: \" + b for b in abstracts]) for abstracts in batch_abstracts],  # concatenate studies into one string\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=max_token_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_attention_mask=True,\n",
    "    ).to(device)\n",
    "    if debugging:\n",
    "      print(\"val_inputs\", val_inputs[\"input_ids\"])\n",
    "      print(f\"Tokenization is done for batch {i}\")\n",
    "\n",
    "    # Generate summaries\n",
    "    # with autocast():  # switch float casting between 16 and 32-bit -- helps with memory\n",
    "    summary_ids = longt5_model.generate(\n",
    "        val_inputs[\"input_ids\"],\n",
    "        attention_mask=val_inputs[\"attention_mask\"],\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "    if debugging:\n",
    "      print(summary_ids.shape)\n",
    "\n",
    "    # Decode summaries\n",
    "    batch_summaries = longt5_tokenizer.batch_decode(\n",
    "        summary_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    if debugging:\n",
    "      print(f\"Summarization and decoding is done for batch {i}\")\n",
    "      print(batch_summaries)\n",
    "\n",
    "    # Create a temporary DataFrame and append it to the output DataFrame\n",
    "    temp_df = pd.DataFrame({\n",
    "        'ReviewID': batch_review_ids,\n",
    "        'Candidate_Summary': batch_summaries,\n",
    "        'Target': batch_targets\n",
    "    })\n",
    "    output_df = pd.concat([output_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # clear memory!\n",
    "    del val_inputs\n",
    "    del summary_ids\n",
    "    del batch_summaries\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if debugging:\n",
    "        break\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_df.to_csv('model_evaluation_output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "Npbmy6zoIK3c",
    "outputId": "8bd4bed0-e89c-46a2-d825-440e42cd2a07",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ReviewID</th>\n",
       "      <th>Candidate_Summary</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28514886</td>\n",
       "      <td>In this paper, the authors present a detailed ...</td>\n",
       "      <td>Current evidence from systematic review and me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18842808</td>\n",
       "      <td>The effects of soluble fiber Konjacglucomannaa...</td>\n",
       "      <td>The use of glucomannan did not appear to signi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24297836</td>\n",
       "      <td>In this study, we examine the autonomic functi...</td>\n",
       "      <td>Ensuring that the characteristics of the histo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32367221</td>\n",
       "      <td>The first four months after ACL-reconstruction...</td>\n",
       "      <td>The QT autograft detected comparable rate of L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25038833</td>\n",
       "      <td>In this study, we examine the effects of a com...</td>\n",
       "      <td>medicines with anti-cholinergic properties hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>19776504</td>\n",
       "      <td>In this study, the aim of the paper is to comp...</td>\n",
       "      <td>This systematic review with meta- analysis fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>27505198</td>\n",
       "      <td>In this study, we examine whether an acceptanc...</td>\n",
       "      <td>A wide range of techniques have been evaluated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>25251296</td>\n",
       "      <td>The aim of this study is to investigate whethe...</td>\n",
       "      <td>First , during anorexia nervosa adolescent fem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>23235652</td>\n",
       "      <td>In this study, Dr. Lira examines the effect of...</td>\n",
       "      <td>There is no convincing evidence that zinc supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>30058911</td>\n",
       "      <td>In this paper, Anderson focuses on the associa...</td>\n",
       "      <td>We argue that despite inconsistencies in the d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2021 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ReviewID                                  Candidate_Summary  \\\n",
       "0     28514886  In this paper, the authors present a detailed ...   \n",
       "1     18842808  The effects of soluble fiber Konjacglucomannaa...   \n",
       "2     24297836  In this study, we examine the autonomic functi...   \n",
       "3     32367221  The first four months after ACL-reconstruction...   \n",
       "4     25038833  In this study, we examine the effects of a com...   \n",
       "...        ...                                                ...   \n",
       "2016  19776504  In this study, the aim of the paper is to comp...   \n",
       "2017  27505198  In this study, we examine whether an acceptanc...   \n",
       "2018  25251296  The aim of this study is to investigate whethe...   \n",
       "2019  23235652  In this study, Dr. Lira examines the effect of...   \n",
       "2020  30058911  In this paper, Anderson focuses on the associa...   \n",
       "\n",
       "                                                 Target  \n",
       "0     Current evidence from systematic review and me...  \n",
       "1     The use of glucomannan did not appear to signi...  \n",
       "2     Ensuring that the characteristics of the histo...  \n",
       "3     The QT autograft detected comparable rate of L...  \n",
       "4     medicines with anti-cholinergic properties hav...  \n",
       "...                                                 ...  \n",
       "2016  This systematic review with meta- analysis fou...  \n",
       "2017  A wide range of techniques have been evaluated...  \n",
       "2018  First , during anorexia nervosa adolescent fem...  \n",
       "2019  There is no convincing evidence that zinc supp...  \n",
       "2020  We argue that despite inconsistencies in the d...  \n",
       "\n",
       "[2021 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CANDIDATE\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'In this study, the effect of adding glucotonicoid to bone loss is investigated. Twenty patients treated with chronic gypsy are given 15mg of vitamin K2, while 10 patients receive only 15ml of glucocarticoidin. The results show that the reduction in Osteocalcin induces bone loss. A phase 3 trial has been performed on 1649 post-menopaausal women who received strontium runelate for three years. Strontium reduced fractures in the first two years but did not increase bone mineral dentity. Women receiving higher levels of uclacinate osteocalcin had a lower relative risk of fracture than those receiving low levels. This suggests that high concentrations of phylentoquinonine may be associated with less bone loss and increased risk from hip fracture.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This systematic review suggests that supplementation with phytonadione and menaquinone-4 reduces bone loss .\\nIn the case of the latter , there is a strong effect on incident fractures among Japanese patients'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output_df)\n",
    "\n",
    "# examine a single review example\n",
    "review_row = 5\n",
    "print(\"CANDIDATE\")\n",
    "display(output_df.loc[review_row, \"Candidate_Summary\"])\n",
    "print(\"TARGET\")\n",
    "display(output_df.loc[review_row, \"Target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "y_qE2r12k47_",
    "outputId": "22fb38b7-a747-4d01-b3ec-0872eecbcf62",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /mnt/wekamount/RI-Users/amir.moazami/.cache/huggingface/modules/datasets_modules/metrics/rouge/08e5f021b5761265deaafbf424e57913106427f546189fe3f934069dd32c153f (last modified on Sat Nov  4 02:38:12 2023) since it couldn't be found locally at rouge, or remotely on the Hugging Face Hub.\n",
      "WARNING:datasets.load:Using the latest cached version of the module from /mnt/wekamount/RI-Users/amir.moazami/.cache/huggingface/modules/datasets_modules/metrics/rouge/08e5f021b5761265deaafbf424e57913106427f546189fe3f934069dd32c153f (last modified on Sat Nov  4 02:38:12 2023) since it couldn't be found locally at rouge, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=0.13777327647304702, recall=0.3685823671289688, fmeasure=0.1817304863018001), mid=Score(precision=0.14098931457374494, recall=0.3738926059110763, fmeasure=0.18479273397723522), high=Score(precision=0.14457804175527433, recall=0.3789444225590706, fmeasure=0.18773320898990328)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=0.015968242281837648, recall=0.04560239399342706, fmeasure=0.021279706464519658), mid=Score(precision=0.01665982941228705, recall=0.04784477453412425, fmeasure=0.02210182831036328), high=Score(precision=0.017417156147380784, recall=0.050099823101448386, fmeasure=0.023031334568014216)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=0.0778328256969952, recall=0.21990936138254075, fmeasure=0.10412556228141887), mid=Score(precision=0.0795642377746438, recall=0.22352596966205607, fmeasure=0.10556030179833661), high=Score(precision=0.08138306883282523, recall=0.22731562281008738, fmeasure=0.10710639614362069)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=0.09812114514087358, recall=0.2596656724663865, fmeasure=0.12859043611509446), mid=Score(precision=0.1008079672884445, recall=0.26313363911480236, fmeasure=0.13096775815402537), high=Score(precision=0.10337899475339817, recall=0.2666136844199758, fmeasure=0.13326686988886396))}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Load the ROUGE metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# Calculate ROUGE scores\n",
    "rouge_scores = rouge.compute(predictions=output_df['Candidate_Summary'], references=output_df['Target'])\n",
    "\n",
    "# Print the ROUGE scores\n",
    "display(rouge_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "022cb781a6034540a4fb6ce586f3a92f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d77821d277f34ccf89c5369bd43816a0",
      "max": 88,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e930598b50d948a48ae34889f688e9b6",
      "value": 88
     }
    },
    "053fcc7a7d314469adbff72f85b06f74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "12d907ef38b641bf9f50a2d6b5fe8439": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b9a3ecbae244ec590924c02ae94c612": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "295c9880262641fa98e6205c53d94bc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91f731aa79f44803adbffe9daba12a28",
      "max": 2275327883,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d4c1fce75e94b03ac38cae675d48ad0",
      "value": 2275327883
     }
    },
    "2b2e88e9a128453ebfd936923923aef6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2bc86fffa3934e56af7657b9540d92ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_790d43cbb5c440ed8aeba1244259b9a6",
      "placeholder": "​",
      "style": "IPY_MODEL_d59964e67a1448868a188731295770b3",
      "value": " 1.91M/1.91M [00:00&lt;00:00, 21.1MB/s]"
     }
    },
    "2f52606fc1844b4c87c37449dd422af9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33a994ce50054a60838e63c61480bfb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c71fa83705646d49e794c3032734e4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_da259f9339ea4023a4e6c48d3eb653f2",
       "IPY_MODEL_022cb781a6034540a4fb6ce586f3a92f",
       "IPY_MODEL_fbcabd6ec226484fb639331a65f234c0"
      ],
      "layout": "IPY_MODEL_9d7017518a704194a110770a841f2fca"
     }
    },
    "412e6f3d15a74c0eaf1bff1e6a1c7b57": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "465da263fa2c49d291c01a9a18d0ed1e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e6a9dd21edc43a99233517e3ac6c840": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c7a16f0c4f6e46899b05e1c74ac99caa",
      "placeholder": "​",
      "style": "IPY_MODEL_fbd5f28870ff48bfabe8ea81905c487b",
      "value": " 65.0/65.0 [00:00&lt;00:00, 3.80kB/s]"
     }
    },
    "5119e4581ca141de8a0743cb98c45ea0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62dbae26182a45da9708fcc628b8f689",
       "IPY_MODEL_295c9880262641fa98e6205c53d94bc3",
       "IPY_MODEL_99b237cb96444ca285b5c9be7cba87bf"
      ],
      "layout": "IPY_MODEL_7d1b3931c50d482ca09c174911d434a9"
     }
    },
    "5532885c6d104b108eef1bddfbd6dcf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_894f0ce4cab44874bb56cf863613f0ce",
       "IPY_MODEL_86a6dcf2e4d9432abfd5f5e2b277e4e3",
       "IPY_MODEL_59c93eb9cb8e4b4da637b5dafed55cc6"
      ],
      "layout": "IPY_MODEL_aaad611c4d194dfe89daf4bf0becb9e4"
     }
    },
    "59c93eb9cb8e4b4da637b5dafed55cc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77b92decffaa4a4d96e63dfd975e90cb",
      "placeholder": "​",
      "style": "IPY_MODEL_33a994ce50054a60838e63c61480bfb7",
      "value": " 1.12k/1.12k [00:00&lt;00:00, 20.8kB/s]"
     }
    },
    "628d30b6d0eb488ab538f3c6b309df36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9f45b4f3253b4c5b8f3a31e6247b0877",
       "IPY_MODEL_69878b7d93da42c1a517036c71dfa69c",
       "IPY_MODEL_2bc86fffa3934e56af7657b9540d92ef"
      ],
      "layout": "IPY_MODEL_a05d898d90894691bc2574ed763b54f0"
     }
    },
    "62dbae26182a45da9708fcc628b8f689": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_64d75164a7f54a2faf6e36921f51c92b",
      "placeholder": "​",
      "style": "IPY_MODEL_d1c3d1343434478d97e29fa263aee019",
      "value": "Downloading pytorch_model.bin: 100%"
     }
    },
    "6479548a192d496694bd9fa1bc5cf5e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64d75164a7f54a2faf6e36921f51c92b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65a36a66efaf41fe8d93e36999228bba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65ea2d87c9f9410d8ceec99b975c84fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "693c6efb7e194e21a717da914df0711f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72ae98b83c9a49e4aaf7fae10427ecbc",
       "IPY_MODEL_95205558a9ea4492a886412ef994065d",
       "IPY_MODEL_4e6a9dd21edc43a99233517e3ac6c840"
      ],
      "layout": "IPY_MODEL_412e6f3d15a74c0eaf1bff1e6a1c7b57"
     }
    },
    "69878b7d93da42c1a517036c71dfa69c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65a36a66efaf41fe8d93e36999228bba",
      "max": 1912529,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_12d907ef38b641bf9f50a2d6b5fe8439",
      "value": 1912529
     }
    },
    "6c87555652d24f52a45636382ddd272a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72ae98b83c9a49e4aaf7fae10427ecbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c544430c83b44e58a55bbb41a717234a",
      "placeholder": "​",
      "style": "IPY_MODEL_6c87555652d24f52a45636382ddd272a",
      "value": "Downloading (…)cial_tokens_map.json: 100%"
     }
    },
    "77b92decffaa4a4d96e63dfd975e90cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "790d43cbb5c440ed8aeba1244259b9a6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d1b3931c50d482ca09c174911d434a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "86a6dcf2e4d9432abfd5f5e2b277e4e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cfde1d5b5854bbb8bab7a5d8178282a",
      "max": 1120,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2b2e88e9a128453ebfd936923923aef6",
      "value": 1120
     }
    },
    "894f0ce4cab44874bb56cf863613f0ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f52606fc1844b4c87c37449dd422af9",
      "placeholder": "​",
      "style": "IPY_MODEL_f5374549ebea4a94aec3df0af4a7831a",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "8cfde1d5b5854bbb8bab7a5d8178282a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8f2067dc576f407590e4161286206bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91f731aa79f44803adbffe9daba12a28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95205558a9ea4492a886412ef994065d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af2fca824ce14a97a64806679d269d2d",
      "max": 65,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_053fcc7a7d314469adbff72f85b06f74",
      "value": 65
     }
    },
    "99b237cb96444ca285b5c9be7cba87bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9a3ecbae244ec590924c02ae94c612",
      "placeholder": "​",
      "style": "IPY_MODEL_eab3563b99124e43a63b7e225eec1df0",
      "value": " 2.28G/2.28G [00:25&lt;00:00, 122MB/s]"
     }
    },
    "9d4c1fce75e94b03ac38cae675d48ad0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d7017518a704194a110770a841f2fca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f45b4f3253b4c5b8f3a31e6247b0877": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6479548a192d496694bd9fa1bc5cf5e8",
      "placeholder": "​",
      "style": "IPY_MODEL_e20d82dd594044ffa66c31778222b0ab",
      "value": "Downloading (…)ve/main/spiece.model: 100%"
     }
    },
    "a05d898d90894691bc2574ed763b54f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aaad611c4d194dfe89daf4bf0becb9e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af2fca824ce14a97a64806679d269d2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c544430c83b44e58a55bbb41a717234a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c7a16f0c4f6e46899b05e1c74ac99caa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1c3d1343434478d97e29fa263aee019": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d59964e67a1448868a188731295770b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d77821d277f34ccf89c5369bd43816a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "da259f9339ea4023a4e6c48d3eb653f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3e3fb15e644482eaef85e1651b48931",
      "placeholder": "​",
      "style": "IPY_MODEL_8f2067dc576f407590e4161286206bf4",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "e20d82dd594044ffa66c31778222b0ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3e3fb15e644482eaef85e1651b48931": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e930598b50d948a48ae34889f688e9b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "eab3563b99124e43a63b7e225eec1df0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5374549ebea4a94aec3df0af4a7831a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fbcabd6ec226484fb639331a65f234c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_465da263fa2c49d291c01a9a18d0ed1e",
      "placeholder": "​",
      "style": "IPY_MODEL_65ea2d87c9f9410d8ceec99b975c84fd",
      "value": " 88.0/88.0 [00:00&lt;00:00, 4.82kB/s]"
     }
    },
    "fbd5f28870ff48bfabe8ea81905c487b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
