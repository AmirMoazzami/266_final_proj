{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "siWmXXPgidJi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb9c98c-9305-4217-f389-3c7f79ea9a57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q rouge_score transformers bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dycHxY1_idJj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import List, Dict\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "import bert_score\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "START_POPULATION='<pop>'\n",
        "END_POPULATION='</pop>'\n",
        "START_INTERVENTION='<int>'\n",
        "END_INTERVENTION='</int>'\n",
        "START_OUTCOME='<out>'\n",
        "END_OUTCOME='</out>'\n",
        "START_BACKGROUND = '<background>'\n",
        "END_BACKGROUND = '</background>'\n",
        "START_REFERENCE = '<ref>'\n",
        "END_REFERENCE = '</ref>'\n",
        "START_EVIDENCE = '<evidence>'\n",
        "END_EVIDENCE = '</evidence>'\n",
        "SEP_TOKEN = '<sep>'\n",
        "EXTRA_TOKENS = [\n",
        "    START_BACKGROUND,\n",
        "    END_BACKGROUND,\n",
        "    START_REFERENCE,\n",
        "    END_REFERENCE,\n",
        "    SEP_TOKEN,\n",
        "    START_POPULATION,\n",
        "    END_POPULATION,\n",
        "    START_INTERVENTION,\n",
        "    END_INTERVENTION,\n",
        "    START_OUTCOME,\n",
        "    END_OUTCOME,\n",
        "    START_EVIDENCE,\n",
        "    END_EVIDENCE,\n",
        "]\n",
        "\n",
        "\n",
        "def rouge_scores(\n",
        "    preds: List[List[torch.Tensor]], targets: List[List[torch.Tensor]],\n",
        "    tokenizer, use_stemmer=False, use_aggregator=False\n",
        "):\n",
        "    # largely copied from https://github.com/huggingface/nlp/blob/master/metrics/rouge/rouge.py#L84\n",
        "    # and from https://github.com/allenai/ms2/blob/a03ab009e00c5e412b4c55f6ec4f9b49c2d8a7f6/ms2/models/utils.py\n",
        "    rouge_types = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
        "    scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)\n",
        "    refs, hyps = [], []\n",
        "    for p, t in zip(preds, targets):\n",
        "        assert len(p) == len(t)\n",
        "        refs.extend(p)\n",
        "        hyps.extend(t)\n",
        "\n",
        "    if use_aggregator:\n",
        "        aggregator = scoring.BootstrapAggregator()\n",
        "        scores = None\n",
        "    else:\n",
        "        aggregator = None\n",
        "        scores = []\n",
        "\n",
        "    for ref, pred in zip(refs, hyps):\n",
        "        if isinstance(ref, torch.Tensor):\n",
        "            ref = tokenizer.decode(ref).lower()\n",
        "        if isinstance(pred, torch.Tensor):\n",
        "            pred = tokenizer.decode(pred).lower()\n",
        "        score = scorer.score(ref, pred)\n",
        "        if use_aggregator:\n",
        "            aggregator.add_scores(score)\n",
        "        else:\n",
        "            scores.append(score)\n",
        "\n",
        "    if use_aggregator:\n",
        "        result = aggregator.aggregate()\n",
        "    else:\n",
        "        result = {}\n",
        "        for key in scores[0]:\n",
        "            result[key] = list(score[key] for score in scores)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_tokenizer(tokenizer_type: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_type, additional_special_tokens=EXTRA_TOKENS)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def calculate_rouge(targets: Dict[str, Dict], generated: Dict[str, str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate ROUGE scores\n",
        "    :param targets: dict of docid -> {'target': target_text}\n",
        "    :param generated: dict of docid -> generated_text\n",
        "    :return: dict of ROUGE scores (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    \"\"\"\n",
        "    # copied from https://github.com/allenai/mslr-shared-task/blob/c2218c1a440cf5172d784065b48af2d6c5c50f9a/evaluator/evaluator.py\n",
        "    print(\"Computing ROUGE scores...\")\n",
        "    docids = list(targets.keys())\n",
        "    target_texts = [[targets[docid]['target']] for docid in docids]\n",
        "    generated_texts = [[generated.get(docid, '')] for docid in docids]\n",
        "\n",
        "    # rouge scoring\n",
        "    tokenizer = get_tokenizer('facebook/bart-base')\n",
        "    rouge_results = rouge_scores(generated_texts, target_texts, tokenizer, use_aggregator=True)\n",
        "    return rouge_results\n",
        "\n",
        "\n",
        "def calculate_mid_rouge(targets: Dict[str, Dict], generated: Dict[str, str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate ROUGE scores but only return mid fmeasure.\n",
        "    :param targets: dict of docid -> {'target': target_text}\n",
        "    :param generated: dict of docid -> generated_text\n",
        "    :return: dict of ROUGE scores (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    \"\"\"\n",
        "    results = calculate_rouge(targets, generated)\n",
        "    return {\n",
        "        \"rouge\": results,\n",
        "        \"rouge1\": results[\"rouge1\"].mid.fmeasure,\n",
        "        \"rouge2\": results[\"rouge2\"].mid.fmeasure,\n",
        "        \"rougeL\": results[\"rougeL\"].mid.fmeasure,\n",
        "        \"rougeLsum\": results[\"rougeLsum\"].mid.fmeasure,\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_bertscore(\n",
        "    targets: Dict[str, Dict], generated: Dict[str, str], model_type=\"roberta-large\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate BERTscore\n",
        "    :param targets: dict of docid -> {'target': target_text}\n",
        "    :param generated: dict of docid -> generated_text\n",
        "    :param model_type: model type for BERTscore. Choose from\n",
        "        ['bert-base-uncased', 'bert-large-uncased', 'roberta-base',\n",
        "        'roberta-large']. Default: 'roberta-large'\n",
        "    :return: dict of BERTscore results (bs_ps, bs_rs, bs_fs) (precision, recall, f1)\n",
        "    \"\"\"\n",
        "    # copied from https://github.com/allenai/mslr-shared-task/blob/c2218c1a440cf5172d784065b48af2d6c5c50f9a/evaluator/evaluator.py\n",
        "    # original bert score: https://github.com/Tiiiger/bert_score\n",
        "    print(\"Computing BERTscore...\")\n",
        "    docids = list(targets.keys())\n",
        "    target_texts = [targets[docid]['target'] for docid in docids]\n",
        "    generated_texts = [generated.get(docid, '') for docid in docids]\n",
        "\n",
        "    # BERTscore\n",
        "    bs_ps, bs_rs, bs_fs = bert_score.score(generated_texts, target_texts, model_type=model_type)\n",
        "    return {\n",
        "        \"bs_ps\": bs_ps,\n",
        "        \"bs_rs\": bs_rs,\n",
        "        \"bs_fs\": bs_fs\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_mean_bertscore(\n",
        "    targets: Dict[str, Dict], generated: Dict[str, str], model_type=\"microsoft/deberta-xlarge-mnli\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate mean BERTscore\n",
        "    :param targets: dict of docid -> {'target': target_text}\n",
        "    :param generated: dict of docid -> generated_text\n",
        "    :param model_type: model type for BERTscore. Choose from\n",
        "        ['bert-base-uncased', 'bert-large-uncased', 'roberta-base',\n",
        "        'roberta-large']. Default: 'roberta-large'\n",
        "    :return: dict of mean BERTscore results (bs_ps, bs_rs, bs_fs) (precision, recall, f1)\n",
        "    \"\"\"\n",
        "    individual_results = calculate_bertscore(targets, generated, model_type=model_type)\n",
        "\n",
        "    results = {\n",
        "        \"bertscore_avg_p\": torch.mean(individual_results[\"bs_ps\"]).item(),\n",
        "        \"bertscore_avg_r\": torch.mean(individual_results[\"bs_rs\"]).item(),\n",
        "        \"bertscore_avg_f\": torch.mean(individual_results[\"bs_fs\"]).item(),\n",
        "        \"bertscore_std_p\": torch.std(individual_results[\"bs_ps\"]).item(),\n",
        "        \"bertscore_std_r\": torch.std(individual_results[\"bs_rs\"]).item(),\n",
        "        \"bertscore_std_f\": torch.std(individual_results[\"bs_fs\"]).item(),\n",
        "    }\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with some toy examples\n",
        "\n",
        "targets = {\n",
        "    \"doc1\": {\"target\": \"The quick brown fox jumps over the lazy dog\"},\n",
        "    \"doc2\": {\"target\": \"fruit flies like a banana\"},\n",
        "    \"doc3\": {\"target\": \"fruit flies like a banana\"},\n",
        "    \"doc4\": {\"target\": \"fruit flies like a banana\"},\n",
        "    \"doc5\": {\"target\": \"everything is chaotic\"},\n",
        "    \"doc6\": {\"target\": \"The quick brown fox jumps over the lazy dog\"},\n",
        "}\n",
        "\n",
        "generated = {\n",
        "    \"doc1\": \"A lazy dog is under a hopping speedy fox\",  # synonym\n",
        "    \"doc2\": \"some insects are attracted to a yellow fruit\",  # one interpretation\n",
        "    \"doc3\": \"most fruits have the aerodynamic properties of a banana\",  # another interpretation\n",
        "    \"doc4\": \"nothing makes sense\",  # completely irrelevant\n",
        "    \"doc5\": \"nothing makes sense\",  # synonym\n",
        "    \"doc6\": \"The quick brown fox jumps over the lazy dog\",  # perfect/identical\n",
        "}\n",
        "\n",
        "rouge_results = calculate_rouge(targets, generated)\n",
        "display(rouge_results)\n",
        "\n",
        "bertscore_results = calculate_bertscore(targets, generated, \"microsoft/deberta-xlarge-mnli\")\n",
        "display(bertscore_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "Gu8yijH1qwmg",
        "outputId": "4108ea03-fba8-4a22-d769-f50c3e4313f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing ROUGE scores...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.12222222222222223, recall=0.0787037037037037, fmeasure=0.0989010989010989), mid=Score(precision=0.35555555555555557, recall=0.2962962962962963, fmeasure=0.32051282051282054), high=Score(precision=0.6333333333333333, recall=0.5972222222222222, fmeasure=0.6068376068376068)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.020833333333333332, recall=0.020833333333333332, fmeasure=0.020833333333333332), mid=Score(precision=0.20833333333333334, recall=0.20833333333333334, fmeasure=0.20833333333333334), high=Score(precision=0.5625, recall=0.5416666666666666, fmeasure=0.548611111111111)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.07037037037037037, recall=0.05787037037037037, fmeasure=0.06267806267806268), mid=Score(precision=0.2740740740740741, recall=0.24074074074074078, fmeasure=0.254985754985755), high=Score(precision=0.6037962962962963, recall=0.5740740740740741, fmeasure=0.5884920634920633)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.07037037037037037, recall=0.05787037037037037, fmeasure=0.06267806267806268), mid=Score(precision=0.3, recall=0.24537037037037038, fmeasure=0.2655677655677655), high=Score(precision=0.6, recall=0.5578703703703703, fmeasure=0.5732600732600732))}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing BERTscore...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'bs_ps': tensor([0.7381, 0.5301, 0.5480, 0.3825, 0.6740, 1.0000]),\n",
              " 'bs_rs': tensor([0.7276, 0.5883, 0.6598, 0.4499, 0.6900, 1.0000]),\n",
              " 'bs_fs': tensor([0.7328, 0.5577, 0.5987, 0.4135, 0.6819, 1.0000])}"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with some toy examples\n",
        "\n",
        "targets = {\n",
        "    \"28514886\": {\"target\": \"Current evidence from systematic review and meta- analysis revealed that probiotics are the most promising intervention in reduction of the incidence of NEC in VLBW neonates . As per the evidence , prebiotics modulate the composition of human intestine microflora to the benefit of the host by suppression of colonization of harmful microorganism and /or the stimulation of bifidobacterial growth , decreased stool viscosity , reduced gastrointestinal transit time , and better feed tolerance .\"},\n",
        "    \"18842808\": {\"target\": \"The use of glucomannan did not appear to significantly alter any other study endpoints . Pediatric patients , patients receiving dietary modification , and patients with impaired glucose metabolism did not benefit from glucomannan to the same degree . Glucomannan appears to beneficially affect total cholesterol , LDL cholesterol , triglycerides , body weight , and FBG , but not HDL cholesterol or BP\"},\n",
        "}\n",
        "\n",
        "generated = {\n",
        "    \"28514886\": \"Retrieve concise conclusion without background: BACKGROUND : Necrotizing enterocolitis ( NEC ) is one of the most destructive diseases associated with the intestine. We aim to determine the effect of a preterm formula containing partially hydrolyzed whey protein, modified vegetable oil with a high & bgr;-palmitic acid content, on the intestinal flora. We hypothesized that enteral supplementation of a prebiotic mixture consisting of neutral oligosaccharides ( (SC)GOS/(LC)FOS ) and acidic oligosaccharides ( AOS ) on intestinal permeability. In a double-blind trial 20 preterm infants ( gestational age 27 ( 24 - 31 ) weeks, postnatal age 42 ( 11 - 84 ) days ), and weight at study entry 1570 ( 1080 - 2300 ) g were enrolled. The infants were randomized to receive either a formula with 8 g/L of either GOS/LCFOS ( 1.5 or 3.0 g/kg ). The stool specimens were quantitatively cultured weekly for the number of bifidobacteria, gastric residue, bowel habits, and feeding tolerance.  Clinical examination including anthropometric measurements, microbiological analysis of fecal sample s, and blood leukocyte population analysis were performed at birth and 6 and 10 weeks... The results showed that the incidence of NEC, the group fed the oligosaccharide supplemented formula increased to the upper range of infant growth, and the placebo group. The incidence of > or = 1 serious infection, as measured by extrusion force ( P=0.006 ), was not significantly different in the supplemented group ( P = 0.056 )..  The intestinal microbiota of infants who received a st and ard formula seems to resemble a more mature gut flora, while the 0.8 g/dL group, 9.7 - 14 % of these neonates.. Conclusion : Neonatal enteric NEC. The intestinal flora of preterm neonates was not different between the 2 groups. and/.}).\",\n",
        "    \"18842808\": \"Retrieve concise conclusion without background: BACKGROUND The purpose of this study was to evaluate the effectiveness of the hydrosoluble fiber glucomannan to a Step-One-Diet in mildly hypercholesterolemic type II diabetic and non-diabetic subjects and to compare the response of these two subject groups to the treatments. MATERIAL / METHODS One hundred and seventy six men and women were included to receive either active fiber substance or placebo in r and omized placebo-controlled studies. The subjects were encouraged not to change their ordinary diets or general lifestyle during the investigation. RESULTS : After a three-days food recall, a balanced diet with adequate caloric intake was provided to all obese children. In all patients before and 2 - 4 months after the intervention, the plasma lipids ( weight, height, weight excess ) and laboratory data ( serum levels of cholesterol, HDL, triglycerides, glucose, fructosamine, glycosylated hemoglobin, RBC, WBC, hemoglobin, iron, calcium, Cu and Zn ) have been determined. Excess weight and triglycerids levels were significantly decreased in treated obese patients than in obese controls 4 months later. Both groups experienced decreases in ( P < 0.01 ) body weight, percent body fat, systolic blood pressure, waist circumference, and plasma glucose levels. After 12 weeks, HDL-C and TAG improved significantly in the fiber ( 10 % and -34 % ) and placebo ( 14 %, -43 % ) groups. The results of lipid profiles did not differ between subject groups. Overall plasma lathosterol concentrations, as well as FBG, and other lipids were lowered ( P<0.05 ). The study to perform a meta- analysis of r, omized controlled trials of glucarannan on plasma lipid, FBG.. and).-..\",\n",
        "}\n",
        "\n",
        "rouge_results = calculate_mid_rouge(targets, generated)\n",
        "display(rouge_results)\n",
        "\n",
        "bertscore_results = calculate_mean_bertscore(targets, generated, \"microsoft/deberta-xlarge-mnli\")\n",
        "display(bertscore_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "DjR_IKBpwCJl",
        "outputId": "5ec2166c-b1e6-40fa-d1d4-f180ba4929e6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing ROUGE scores...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'rouge': {'rouge1': AggregateScore(low=Score(precision=0.4727272727272727, recall=0.09961685823754789, fmeasure=0.16455696202531644), mid=Score(precision=0.4754940711462451, recall=0.10959103781442611, fmeasure=0.1779306549257017), high=Score(precision=0.4782608695652174, recall=0.11956521739130435, fmeasure=0.19130434782608696)),\n",
              "  'rouge2': AggregateScore(low=Score(precision=0.08823529411764706, recall=0.019230769230769232, fmeasure=0.03184713375796178), mid=Score(precision=0.09041394335511982, recall=0.020524475524475526, fmeasure=0.033416278249243286), high=Score(precision=0.09259259259259259, recall=0.02181818181818182, fmeasure=0.03498542274052478)),\n",
              "  'rougeL': AggregateScore(low=Score(precision=0.2727272727272727, recall=0.05747126436781609, fmeasure=0.0949367088607595), mid=Score(precision=0.2812911725955204, recall=0.06496751624187906, fmeasure=0.10543936892313338), high=Score(precision=0.2898550724637681, recall=0.07246376811594203, fmeasure=0.11594202898550726)),\n",
              "  'rougeLsum': AggregateScore(low=Score(precision=0.2727272727272727, recall=0.05747126436781609, fmeasure=0.0949367088607595), mid=Score(precision=0.2812911725955204, recall=0.06496751624187906, fmeasure=0.10543936892313338), high=Score(precision=0.2898550724637681, recall=0.07246376811594203, fmeasure=0.11594202898550726))},\n",
              " 'rouge1': 0.1779306549257017,\n",
              " 'rouge2': 0.033416278249243286,\n",
              " 'rougeL': 0.10543936892313338,\n",
              " 'rougeLsum': 0.10543936892313338}"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing BERTscore...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "{'bertscore_avg_p': 0.47608551383018494,\n",
              " 'bertscore_avg_r': 0.6032038927078247,\n",
              " 'bertscore_avg_f': 0.5320615768432617,\n",
              " 'bertscore_std_p': 0.006571157369762659,\n",
              " 'bertscore_std_r': 0.014886519871652126,\n",
              " 'bertscore_std_f': 0.0016888242680579424}"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}