{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siWmXXPgidJi",
        "outputId": "9eb9c98c-9305-4217-f389-3c7f79ea9a57"
      },
      "outputs": [],
      "source": [
        "!pip install -q rouge_score transformers bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dycHxY1_idJj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from typing import List, Dict\n",
        "from rouge_score import rouge_scorer, scoring\n",
        "import bert_score\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "START_POPULATION='<pop>'\n",
        "END_POPULATION='</pop>'\n",
        "START_INTERVENTION='<int>'\n",
        "END_INTERVENTION='</int>'\n",
        "START_OUTCOME='<out>'\n",
        "END_OUTCOME='</out>'\n",
        "START_BACKGROUND = '<background>'\n",
        "END_BACKGROUND = '</background>'\n",
        "START_REFERENCE = '<ref>'\n",
        "END_REFERENCE = '</ref>'\n",
        "START_EVIDENCE = '<evidence>'\n",
        "END_EVIDENCE = '</evidence>'\n",
        "SEP_TOKEN = '<sep>'\n",
        "EXTRA_TOKENS = [\n",
        "    START_BACKGROUND,\n",
        "    END_BACKGROUND,\n",
        "    START_REFERENCE,\n",
        "    END_REFERENCE,\n",
        "    SEP_TOKEN,\n",
        "    START_POPULATION,\n",
        "    END_POPULATION,\n",
        "    START_INTERVENTION,\n",
        "    END_INTERVENTION,\n",
        "    START_OUTCOME,\n",
        "    END_OUTCOME,\n",
        "    START_EVIDENCE,\n",
        "    END_EVIDENCE,\n",
        "]\n",
        "\n",
        "\n",
        "def rouge_scores(\n",
        "    preds: List[List[torch.Tensor]], targets: List[List[torch.Tensor]],\n",
        "    tokenizer, use_stemmer=False, use_aggregator=False\n",
        "):\n",
        "    # largely copied from https://github.com/huggingface/nlp/blob/master/metrics/rouge/rouge.py#L84\n",
        "    # and from https://github.com/allenai/ms2/blob/a03ab009e00c5e412b4c55f6ec4f9b49c2d8a7f6/ms2/models/utils.py\n",
        "    rouge_types = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
        "    scorer = rouge_scorer.RougeScorer(rouge_types=rouge_types, use_stemmer=use_stemmer)\n",
        "    refs, hyps = [], []\n",
        "    for p, t in zip(preds, targets):\n",
        "        assert len(p) == len(t)\n",
        "        refs.extend(p)\n",
        "        hyps.extend(t)\n",
        "\n",
        "    if use_aggregator:\n",
        "        aggregator = scoring.BootstrapAggregator()\n",
        "        scores = None\n",
        "    else:\n",
        "        aggregator = None\n",
        "        scores = []\n",
        "\n",
        "    for ref, pred in zip(refs, hyps):\n",
        "        if isinstance(ref, torch.Tensor):\n",
        "            ref = tokenizer.decode(ref).lower()\n",
        "        if isinstance(pred, torch.Tensor):\n",
        "            pred = tokenizer.decode(pred).lower()\n",
        "        score = scorer.score(ref, pred)\n",
        "        if use_aggregator:\n",
        "            aggregator.add_scores(score)\n",
        "        else:\n",
        "            scores.append(score)\n",
        "\n",
        "    if use_aggregator:\n",
        "        result = aggregator.aggregate()\n",
        "    else:\n",
        "        result = {}\n",
        "        for key in scores[0]:\n",
        "            result[key] = list(score[key] for score in scores)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_tokenizer(tokenizer_type: str):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_type, additional_special_tokens=EXTRA_TOKENS)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def calculate_rouge(targets: Dict[str, Dict], generated: Dict[str, str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate ROUGE scores\n",
        "    :param targets: dict of docid -> {'target': target_text}\n",
        "    :param generated: dict of docid -> generated_text\n",
        "    :return: dict of ROUGE scores (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    \"\"\"\n",
        "    # copied from https://github.com/allenai/mslr-shared-task/blob/c2218c1a440cf5172d784065b48af2d6c5c50f9a/evaluator/evaluator.py\n",
        "    print(\"Computing ROUGE scores...\")\n",
        "    docids = list(targets.keys())\n",
        "    target_texts = [[targets[docid]['target']] for docid in docids]\n",
        "    generated_texts = [[generated.get(docid, '')] for docid in docids]\n",
        "\n",
        "    # rouge scoring\n",
        "    tokenizer = get_tokenizer('facebook/bart-base')\n",
        "    rouge_results = rouge_scores(generated_texts, target_texts, tokenizer, use_aggregator=True)\n",
        "    return rouge_results\n",
        "\n",
        "\n",
        "def calculate_mid_rouge(targets: Dict[str, Dict], generated: Dict[str, str]) -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate ROUGE scores but only return mid fmeasure.\n",
        "    :param targets: dict of docid -> {'target': target_text}\n",
        "    :param generated: dict of docid -> generated_text\n",
        "    :return: dict of ROUGE scores (rouge1, rouge2, rougeL, rougeLsum)\n",
        "    \"\"\"\n",
        "    results = calculate_rouge(targets, generated)\n",
        "    return {\n",
        "        \"rouge\": results,\n",
        "        \"rouge1\": results[\"rouge1\"].mid.fmeasure,\n",
        "        \"rouge2\": results[\"rouge2\"].mid.fmeasure,\n",
        "        \"rougeL\": results[\"rougeL\"].mid.fmeasure,\n",
        "        \"rougeLsum\": results[\"rougeLsum\"].mid.fmeasure,\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_bertscore(\n",
        "    targets: Dict[str, Dict], generated: Dict[str, str], model_type=\"roberta-large\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate BERTscore\n",
        "    :param targets: dict of docid -> {'target': target_text}\n",
        "    :param generated: dict of docid -> generated_text\n",
        "    :param model_type: model type for BERTscore. Choose from\n",
        "        ['bert-base-uncased', 'bert-large-uncased', 'roberta-base',\n",
        "        'roberta-large']. Default: 'roberta-large'\n",
        "    :return: dict of BERTscore results (bs_ps, bs_rs, bs_fs) (precision, recall, f1)\n",
        "    \"\"\"\n",
        "    # copied from https://github.com/allenai/mslr-shared-task/blob/c2218c1a440cf5172d784065b48af2d6c5c50f9a/evaluator/evaluator.py\n",
        "    # original bert score: https://github.com/Tiiiger/bert_score\n",
        "    print(\"Computing BERTscore...\")\n",
        "    docids = list(targets.keys())\n",
        "    target_texts = [targets[docid]['target'] for docid in docids]\n",
        "    generated_texts = [generated.get(docid, '') for docid in docids]\n",
        "\n",
        "    # BERTscore\n",
        "    bs_ps, bs_rs, bs_fs = bert_score.score(generated_texts, target_texts, model_type=model_type)\n",
        "    return {\n",
        "        \"bs_ps\": bs_ps,\n",
        "        \"bs_rs\": bs_rs,\n",
        "        \"bs_fs\": bs_fs\n",
        "    }\n",
        "\n",
        "\n",
        "def calculate_mean_bertscore(\n",
        "    targets: Dict[str, Dict], generated: Dict[str, str], model_type=\"microsoft/deberta-xlarge-mnli\"\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Calculate mean BERTscore\n",
        "    :param targets: dict of docid -> {'target': target_text}\n",
        "    :param generated: dict of docid -> generated_text\n",
        "    :param model_type: model type for BERTscore. Choose from\n",
        "        ['bert-base-uncased', 'bert-large-uncased', 'roberta-base',\n",
        "        'roberta-large']. Default: 'roberta-large'\n",
        "    :return: dict of mean BERTscore results (bs_ps, bs_rs, bs_fs) (precision, recall, f1)\n",
        "    \"\"\"\n",
        "    individual_results = calculate_bertscore(targets, generated, model_type=model_type)\n",
        "\n",
        "    results = {\n",
        "        \"bertscore_avg_p\": torch.mean(individual_results[\"bs_ps\"]).item(),\n",
        "        \"bertscore_avg_r\": torch.mean(individual_results[\"bs_rs\"]).item(),\n",
        "        \"bertscore_avg_f\": torch.mean(individual_results[\"bs_fs\"]).item(),\n",
        "        \"bertscore_std_p\": torch.std(individual_results[\"bs_ps\"]).item(),\n",
        "        \"bertscore_std_r\": torch.std(individual_results[\"bs_rs\"]).item(),\n",
        "        \"bertscore_std_f\": torch.std(individual_results[\"bs_fs\"]).item(),\n",
        "    }\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "Gu8yijH1qwmg",
        "outputId": "4108ea03-fba8-4a22-d769-f50c3e4313f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing ROUGE scores...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=0.12222222222222223, recall=0.0787037037037037, fmeasure=0.0989010989010989), mid=Score(precision=0.35555555555555557, recall=0.2962962962962963, fmeasure=0.31746031746031744), high=Score(precision=0.6333333333333333, recall=0.5927083333333333, fmeasure=0.6032661782661781)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=0.020833333333333332, recall=0.020833333333333332, fmeasure=0.020833333333333332), mid=Score(precision=0.22916666666666666, recall=0.20833333333333334, fmeasure=0.21527777777777776), high=Score(precision=0.5416666666666666, recall=0.5208333333333334, fmeasure=0.5277777777777778)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=0.07037037037037037, recall=0.05787037037037037, fmeasure=0.06267806267806268), mid=Score(precision=0.3037037037037037, recall=0.2615740740740741, fmeasure=0.27696377696377694), high=Score(precision=0.6037037037037037, recall=0.5740740740740741, fmeasure=0.5846560846560847)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=0.07037037037037037, recall=0.05787037037037037, fmeasure=0.06267806267806268), mid=Score(precision=0.3, recall=0.24537037037037038, fmeasure=0.265974765974766), high=Score(precision=0.6037037037037037, recall=0.5740740740740741, fmeasure=0.5846560846560847))}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing BERTscore...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04cc3beb149f42a18a6aa62f748e69d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb37a7e830314afe944dfb69dceb6d0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/792 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "295716cdd6a642638259ca818913d4b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06d5a45d75ca40729fdb4d8183bad341",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce827dcdb7144aff8420f232fe3ae81b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/3.04G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'bs_ps': tensor([0.7381, 0.5301, 0.5480, 0.3825, 0.6740, 1.0000]),\n",
              " 'bs_rs': tensor([0.7276, 0.5883, 0.6598, 0.4499, 0.6900, 1.0000]),\n",
              " 'bs_fs': tensor([0.7328, 0.5577, 0.5987, 0.4135, 0.6819, 1.0000])}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test with some toy examples\n",
        "\n",
        "targets = {\n",
        "    \"doc1\": {\"target\": \"The quick brown fox jumps over the lazy dog\"},\n",
        "    \"doc2\": {\"target\": \"fruit flies like a banana\"},\n",
        "    \"doc3\": {\"target\": \"fruit flies like a banana\"},\n",
        "    \"doc4\": {\"target\": \"fruit flies like a banana\"},\n",
        "    \"doc5\": {\"target\": \"everything is chaotic\"},\n",
        "    \"doc6\": {\"target\": \"The quick brown fox jumps over the lazy dog\"},\n",
        "}\n",
        "\n",
        "generated = {\n",
        "    \"doc1\": \"A lazy dog is under a hopping speedy fox\",  # synonym\n",
        "    \"doc2\": \"some insects are attracted to a yellow fruit\",  # one interpretation\n",
        "    \"doc3\": \"most fruits have the aerodynamic properties of a banana\",  # another interpretation\n",
        "    \"doc4\": \"nothing makes sense\",  # completely irrelevant\n",
        "    \"doc5\": \"nothing makes sense\",  # synonym\n",
        "    \"doc6\": \"The quick brown fox jumps over the lazy dog\",  # perfect/identical\n",
        "}\n",
        "\n",
        "rouge_results = calculate_rouge(targets, generated)\n",
        "display(rouge_results)\n",
        "\n",
        "bertscore_results = calculate_bertscore(targets, generated, \"microsoft/deberta-xlarge-mnli\")\n",
        "display(bertscore_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "DjR_IKBpwCJl",
        "outputId": "5ec2166c-b1e6-40fa-d1d4-f180ba4929e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing ROUGE scores...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge': {'rouge1': AggregateScore(low=Score(precision=0.4727272727272727, recall=0.09961685823754789, fmeasure=0.16455696202531644), mid=Score(precision=0.4754940711462451, recall=0.10959103781442611, fmeasure=0.1779306549257017), high=Score(precision=0.4782608695652174, recall=0.11956521739130435, fmeasure=0.19130434782608696)),\n",
              "  'rouge2': AggregateScore(low=Score(precision=0.08823529411764706, recall=0.019230769230769232, fmeasure=0.03184713375796178), mid=Score(precision=0.09041394335511982, recall=0.020524475524475526, fmeasure=0.033416278249243286), high=Score(precision=0.09259259259259259, recall=0.02181818181818182, fmeasure=0.03498542274052478)),\n",
              "  'rougeL': AggregateScore(low=Score(precision=0.2727272727272727, recall=0.05747126436781609, fmeasure=0.0949367088607595), mid=Score(precision=0.2812911725955204, recall=0.06496751624187906, fmeasure=0.10543936892313338), high=Score(precision=0.2898550724637681, recall=0.07246376811594203, fmeasure=0.11594202898550726)),\n",
              "  'rougeLsum': AggregateScore(low=Score(precision=0.2727272727272727, recall=0.05747126436781609, fmeasure=0.0949367088607595), mid=Score(precision=0.2812911725955204, recall=0.06496751624187906, fmeasure=0.10543936892313338), high=Score(precision=0.2898550724637681, recall=0.07246376811594203, fmeasure=0.11594202898550726))},\n",
              " 'rouge1': 0.1779306549257017,\n",
              " 'rouge2': 0.033416278249243286,\n",
              " 'rougeL': 0.10543936892313338,\n",
              " 'rougeLsum': 0.10543936892313338}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing BERTscore...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'bertscore_avg_p': 0.47608551383018494,\n",
              " 'bertscore_avg_r': 0.6032038927078247,\n",
              " 'bertscore_avg_f': 0.5320615768432617,\n",
              " 'bertscore_std_p': 0.006571114994585514,\n",
              " 'bertscore_std_r': 0.014886646531522274,\n",
              " 'bertscore_std_f': 0.0016888664104044437}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Test with some toy examples\n",
        "\n",
        "targets = {\n",
        "    \"28514886\": {\"target\": \"Current evidence from systematic review and meta- analysis revealed that probiotics are the most promising intervention in reduction of the incidence of NEC in VLBW neonates . As per the evidence , prebiotics modulate the composition of human intestine microflora to the benefit of the host by suppression of colonization of harmful microorganism and /or the stimulation of bifidobacterial growth , decreased stool viscosity , reduced gastrointestinal transit time , and better feed tolerance .\"},\n",
        "    \"18842808\": {\"target\": \"The use of glucomannan did not appear to significantly alter any other study endpoints . Pediatric patients , patients receiving dietary modification , and patients with impaired glucose metabolism did not benefit from glucomannan to the same degree . Glucomannan appears to beneficially affect total cholesterol , LDL cholesterol , triglycerides , body weight , and FBG , but not HDL cholesterol or BP\"},\n",
        "}\n",
        "\n",
        "generated = {\n",
        "    \"28514886\": \"Retrieve concise conclusion without background: BACKGROUND : Necrotizing enterocolitis ( NEC ) is one of the most destructive diseases associated with the intestine. We aim to determine the effect of a preterm formula containing partially hydrolyzed whey protein, modified vegetable oil with a high & bgr;-palmitic acid content, on the intestinal flora. We hypothesized that enteral supplementation of a prebiotic mixture consisting of neutral oligosaccharides ( (SC)GOS/(LC)FOS ) and acidic oligosaccharides ( AOS ) on intestinal permeability. In a double-blind trial 20 preterm infants ( gestational age 27 ( 24 - 31 ) weeks, postnatal age 42 ( 11 - 84 ) days ), and weight at study entry 1570 ( 1080 - 2300 ) g were enrolled. The infants were randomized to receive either a formula with 8 g/L of either GOS/LCFOS ( 1.5 or 3.0 g/kg ). The stool specimens were quantitatively cultured weekly for the number of bifidobacteria, gastric residue, bowel habits, and feeding tolerance.  Clinical examination including anthropometric measurements, microbiological analysis of fecal sample s, and blood leukocyte population analysis were performed at birth and 6 and 10 weeks... The results showed that the incidence of NEC, the group fed the oligosaccharide supplemented formula increased to the upper range of infant growth, and the placebo group. The incidence of > or = 1 serious infection, as measured by extrusion force ( P=0.006 ), was not significantly different in the supplemented group ( P = 0.056 )..  The intestinal microbiota of infants who received a st and ard formula seems to resemble a more mature gut flora, while the 0.8 g/dL group, 9.7 - 14 % of these neonates.. Conclusion : Neonatal enteric NEC. The intestinal flora of preterm neonates was not different between the 2 groups. and/.}).\",\n",
        "    \"18842808\": \"Retrieve concise conclusion without background: BACKGROUND The purpose of this study was to evaluate the effectiveness of the hydrosoluble fiber glucomannan to a Step-One-Diet in mildly hypercholesterolemic type II diabetic and non-diabetic subjects and to compare the response of these two subject groups to the treatments. MATERIAL / METHODS One hundred and seventy six men and women were included to receive either active fiber substance or placebo in r and omized placebo-controlled studies. The subjects were encouraged not to change their ordinary diets or general lifestyle during the investigation. RESULTS : After a three-days food recall, a balanced diet with adequate caloric intake was provided to all obese children. In all patients before and 2 - 4 months after the intervention, the plasma lipids ( weight, height, weight excess ) and laboratory data ( serum levels of cholesterol, HDL, triglycerides, glucose, fructosamine, glycosylated hemoglobin, RBC, WBC, hemoglobin, iron, calcium, Cu and Zn ) have been determined. Excess weight and triglycerids levels were significantly decreased in treated obese patients than in obese controls 4 months later. Both groups experienced decreases in ( P < 0.01 ) body weight, percent body fat, systolic blood pressure, waist circumference, and plasma glucose levels. After 12 weeks, HDL-C and TAG improved significantly in the fiber ( 10 % and -34 % ) and placebo ( 14 %, -43 % ) groups. The results of lipid profiles did not differ between subject groups. Overall plasma lathosterol concentrations, as well as FBG, and other lipids were lowered ( P<0.05 ). The study to perform a meta- analysis of r, omized controlled trials of glucarannan on plasma lipid, FBG.. and).-..\",\n",
        "}\n",
        "\n",
        "rouge_results = calculate_mid_rouge(targets, generated)\n",
        "display(rouge_results)\n",
        "\n",
        "bertscore_results = calculate_mean_bertscore(targets, generated, \"microsoft/deberta-xlarge-mnli\")\n",
        "display(bertscore_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating on model outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5db8868a54a7467bbb78a5b72a6246f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/7.09k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d20731c59d0640fd99fc07aa7381d175",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/5.76k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c745d8b936504438bc89bd8023588755",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90f9ac06f50948c6b9ae979d6e9bb9e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/264M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56360babb1bb49aaaef97c47b892c611",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/14188 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2d270a2f85c440cbd3daed0205ae9c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/1667 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbe3035ce79640c5aa048e01797f418c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/2021 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['review_id', 'pmid', 'title', 'abstract', 'target', 'background'],\n",
              "    num_rows: 2021\n",
              "})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pull in original validation data from huggingface datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"allenai/mslr2022\", \"ms2\", split=\"validation\")  # takes 9 mins\n",
        "display(dataset)\n",
        "\n",
        "# organize target data into dicts for rouge and bertscore\n",
        "targets = {row['review_id']: row for row in dataset}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_id</th>\n",
              "      <th>candidate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26762372</td>\n",
              "      <td>Background : The aim of the present study was...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>28723736</td>\n",
              "      <td>BACKGROUND Surgical site infections ( SSIs ) ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22002191</td>\n",
              "      <td>BACKGROUND AND PURPOSE : The purpose of this ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31921439</td>\n",
              "      <td>Background : Several established tools are av...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>30301735</td>\n",
              "      <td>Background Topical glyceryl trinitrate ( GTN ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  review_id                                          candidate\n",
              "0  26762372   Background : The aim of the present study was...\n",
              "1  28723736   BACKGROUND Surgical site infections ( SSIs ) ...\n",
              "2  22002191   BACKGROUND AND PURPOSE : The purpose of this ...\n",
              "3  31921439   Background : Several established tools are av...\n",
              "4  30301735   Background Topical glyceryl trinitrate ( GTN ..."
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Open results from BioBART on validation set\n",
        "df_biobart = pd.read_csv(\"pretrained_no_finetune/biobart/biobart_validation.csv\")\n",
        "# review_id is the docid and should be a string\n",
        "df_biobart['review_id'] = df_biobart['review_id'].astype(str)\n",
        "display(df_biobart.head())\n",
        "\n",
        "# organize into dicts for rouge and bertscore\n",
        "generated = {row['review_id']: row['candidate'] for _, row in df_biobart.iterrows()}\n",
        "\n",
        "# check to see if all docids are present and unique\n",
        "assert set(generated.keys()) == set(targets.keys())\n",
        "assert len(generated) == len(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing ROUGE scores...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge': {'rouge1': AggregateScore(low=Score(precision=0.4055802800888743, recall=0.09275304902853466, fmeasure=0.14602859806458054), mid=Score(precision=0.4684841170217311, recall=0.1368081397204562, fmeasure=0.19313987314858178), high=Score(precision=0.5292965416194026, recall=0.1846324503860281, fmeasure=0.24419925805297138)),\n",
              "  'rouge2': AggregateScore(low=Score(precision=0.07903111258537857, recall=0.019706828072382443, fmeasure=0.030622062392122968), mid=Score(precision=0.12393397334710443, recall=0.029514860093884453, fmeasure=0.043063559207044105), high=Score(precision=0.17885397842710346, recall=0.03957410485771586, fmeasure=0.055320315002104034)),\n",
              "  'rougeL': AggregateScore(low=Score(precision=0.21365770075391635, recall=0.053068956654908885, fmeasure=0.0843145250607575), mid=Score(precision=0.2803899119545282, recall=0.07259153613859604, fmeasure=0.10465551306960569), high=Score(precision=0.35017814201804887, recall=0.09427380657546322, fmeasure=0.12768413961078837)),\n",
              "  'rougeLsum': AggregateScore(low=Score(precision=0.2543677117982362, recall=0.0578045717470017, fmeasure=0.09296591748112877), mid=Score(precision=0.301864316206216, recall=0.08444372153070924, fmeasure=0.12018506454951568), high=Score(precision=0.3607594965048051, recall=0.11023842960453534, fmeasure=0.14646785989217245))},\n",
              " 'rouge1': 0.19313987314858178,\n",
              " 'rouge2': 0.043063559207044105,\n",
              " 'rougeL': 0.10465551306960569,\n",
              " 'rougeLsum': 0.12018506454951568}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing BERTscore...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'bertscore_avg_p': 0.47950369119644165,\n",
              " 'bertscore_avg_r': 0.5985714197158813,\n",
              " 'bertscore_avg_f': 0.5301606059074402,\n",
              " 'bertscore_std_p': 0.05009160935878754,\n",
              " 'bertscore_std_r': 0.039970237761735916,\n",
              " 'bertscore_std_f': 0.03192624822258949}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# SMALL SUBSET DATASET - calculate rouge and bertscore for only 10 docs\n",
        "\n",
        "subset_review_ids = list(generated.keys())[:10]\n",
        "targets_subset = {docid: targets[docid] for docid in subset_review_ids}\n",
        "generated_subset = {docid: generated[docid] for docid in subset_review_ids}\n",
        "\n",
        "rouge_results = calculate_mid_rouge(targets_subset, generated_subset)\n",
        "display(rouge_results)\n",
        "\n",
        "bertscore_results = calculate_mean_bertscore(targets_subset, generated_subset, \"microsoft/deberta-xlarge-mnli\")\n",
        "display(bertscore_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing ROUGE scores...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'rouge': {'rouge1': AggregateScore(low=Score(precision=0.4961293110596966, recall=0.09804610749650168, fmeasure=0.15496854682493477), mid=Score(precision=0.5010460912997867, recall=0.10033882683006921, fmeasure=0.15787649150257704), high=Score(precision=0.5061464436016391, recall=0.10296259138737313, fmeasure=0.16123738642284913)),\n",
              "  'rouge2': AggregateScore(low=Score(precision=0.12074097153798954, recall=0.021833711098443242, fmeasure=0.03501379734426587), mid=Score(precision=0.12477178207184145, recall=0.022547010164837327, fmeasure=0.03609727189353998), high=Score(precision=0.1284730654869281, recall=0.023239723047089753, fmeasure=0.03711046213471403)),\n",
              "  'rougeL': AggregateScore(low=Score(precision=0.3091211111639237, recall=0.05700992858853988, fmeasure=0.09137477431435435), mid=Score(precision=0.3138483862064446, recall=0.05815324451429994, fmeasure=0.09277901738343573), high=Score(precision=0.31780643509315887, recall=0.059300800373725426, fmeasure=0.09430320248381381)),\n",
              "  'rougeLsum': AggregateScore(low=Score(precision=0.34022809189644826, recall=0.06522637932743615, fmeasure=0.1037920479790383), mid=Score(precision=0.3443802745766378, recall=0.06691941306703642, fmeasure=0.10582250252061917), high=Score(precision=0.348589540310728, recall=0.06843215052089782, fmeasure=0.1077508540229986))},\n",
              " 'rouge1': 0.15787649150257704,\n",
              " 'rouge2': 0.03609727189353998,\n",
              " 'rougeL': 0.09277901738343573,\n",
              " 'rougeLsum': 0.10582250252061917}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing BERTscore...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'bertscore_avg_p': 0.4647563397884369,\n",
              " 'bertscore_avg_r': 0.6073237061500549,\n",
              " 'bertscore_avg_f': 0.524921178817749,\n",
              " 'bertscore_std_p': 0.04064429923892021,\n",
              " 'bertscore_std_r': 0.04167566075921059,\n",
              " 'bertscore_std_f': 0.030336380004882812}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# FULL DATASET - calculate rouge and bertscore\n",
        "rouge_results = calculate_mid_rouge(targets, generated)\n",
        "display(rouge_results)\n",
        "\n",
        "bertscore_results = calculate_mean_bertscore(targets, generated, \"microsoft/deberta-xlarge-mnli\")\n",
        "display(bertscore_results)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
